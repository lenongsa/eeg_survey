{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FingerMovements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T17:54:53.798158Z",
     "start_time": "2020-11-04T17:54:53.779689Z"
    }
   },
   "outputs": [],
   "source": [
    "###libraries\n",
    "\n",
    "#Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "import sys\n",
    "import os\n",
    "\n",
    "#Time series transformers\n",
    "from pyts.multivariate.classification import MultivariateClassifier\n",
    "from pyts.multivariate.transformation import WEASELMUSE\n",
    "from pyts.classification import KNeighborsClassifier\n",
    "from pywt import wavedec\n",
    "import pyeeg\n",
    "import scipy.stats\n",
    "\n",
    "#Classifiers\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "#Deep Learning\n",
    "import mcfly\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from keras import backend\n",
    "from numba import cuda \n",
    "\n",
    "#Random\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import gc\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T17:54:53.814894Z",
     "start_time": "2020-11-04T17:54:53.800173Z"
    }
   },
   "outputs": [],
   "source": [
    "###Read the data\n",
    "dataset = 'FingerMovements'\n",
    "\n",
    "X_train = np.load('Datasets_clean/{}/X_train.npy'.format(dataset))\n",
    "y_train = np.load('Datasets_clean/{}/y_train.npy'.format(dataset))\n",
    "X_test = np.load('Datasets_clean/{}/X_test.npy'.format(dataset))\n",
    "y_test = np.load('Datasets_clean/{}/y_test.npy'.format(dataset))\n",
    "\n",
    "X_full = np.vstack([X_train,X_test])\n",
    "y_full = np.hstack([y_train,y_test])\n",
    "\n",
    "#Create scores dict\n",
    "scores = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T17:54:54.028026Z",
     "start_time": "2020-11-04T17:54:54.015896Z"
    }
   },
   "outputs": [],
   "source": [
    "def ResampleLinear1D(original, targetLen = 40):\n",
    "    original = np.array(original, dtype=np.float)\n",
    "    index_arr = np.linspace(0, len(original)-1, num=targetLen, dtype=np.float)\n",
    "    index_floor = np.array(index_arr, dtype=np.int) #Round down\n",
    "    index_ceil = index_floor + 1\n",
    "    index_rem = index_arr - index_floor #Remain\n",
    "\n",
    "    val1 = original[index_floor]\n",
    "    val2 = original[index_ceil % len(original)]\n",
    "    interp = val1 * (1.0-index_rem) + val2 * index_rem\n",
    "    assert(len(interp) == targetLen)\n",
    "    return interp\n",
    "\n",
    "def calculate_entropy(list_values):\n",
    "    counter_values = Counter(list_values).most_common()\n",
    "    probabilities = [elem[1]/len(list_values) for elem in counter_values]\n",
    "    entropy=scipy.stats.entropy(probabilities)\n",
    "    return entropy\n",
    " \n",
    "def calculate_statistics(list_values):\n",
    "    n5 = np.nanpercentile(list_values, 5)\n",
    "    n25 = np.nanpercentile(list_values, 25)\n",
    "    n75 = np.nanpercentile(list_values, 75)\n",
    "    n95 = np.nanpercentile(list_values, 95)\n",
    "    median = np.nanpercentile(list_values, 50)\n",
    "    mean = np.nanmean(list_values)\n",
    "    std = np.nanstd(list_values)\n",
    "    var = np.nanvar(list_values)\n",
    "    rms = np.nanmean(np.sqrt(list_values**2))\n",
    "    return [n5, n25, n75, n95, median, mean, std, var, rms]\n",
    " \n",
    "def calculate_crossings(list_values):\n",
    "    zero_crossing_indices = np.nonzero(np.diff(np.array(list_values) > 0))[0]\n",
    "    no_zero_crossings = len(zero_crossing_indices)\n",
    "    mean_crossing_indices = np.nonzero(np.diff(np.array(list_values) > np.nanmean(list_values)))[0]\n",
    "    no_mean_crossings = len(mean_crossing_indices)\n",
    "    return [no_zero_crossings, no_mean_crossings]\n",
    " \n",
    "def get_features(list_values):\n",
    "    entropy = calculate_entropy(list_values)\n",
    "    crossings = calculate_crossings(list_values)\n",
    "    statistics = calculate_statistics(list_values)\n",
    "    return [entropy] + crossings + statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DTW + 1-Knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T18:12:11.680304Z",
     "start_time": "2020-11-04T17:55:00.823844Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8965a7b2b16b4c0e870241ef8e4d232c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dtw_acc = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "for train_index, test_index in tqdm(skf.split(X_full, y_full)):\n",
    "    \n",
    "    X_train = X_full[train_index]\n",
    "    y_train = y_full[train_index]\n",
    "    X_test = X_full[test_index]\n",
    "    y_test = y_full[test_index]\n",
    "    \n",
    "#     X_train = np.apply_along_axis(ResampleLinear1D, axis = 2, arr = X_train)\n",
    "#     X_test = np.apply_along_axis(ResampleLinear1D, axis = 2, arr = X_test)\n",
    "    \n",
    "    dtw_knn = MultivariateClassifier(KNeighborsClassifier(metric = 'dtw_itakura',\n",
    "                                                      n_jobs = -1,\n",
    "                                                      metric_params = {'max_slope' : 2}))\n",
    "    \n",
    "    dtw_knn.fit(X_train,y_train)\n",
    "    acc = dtw_knn.score(X_test,y_test)\n",
    "    dtw_acc.append(acc)\n",
    "    \n",
    "scores['dtw_knn'] = [np.mean(dtw_acc), np.std(dtw_acc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEASELMUSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T20:49:05.587745Z",
     "start_time": "2020-11-04T20:42:53.769866Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c93a236332fe4af48f5c687bff398d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "accuracies_lr = []\n",
    "accuracies_rf = []\n",
    "accuracies_svc = []\n",
    "accuracies_lgbm = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "for train_index, test_index in tqdm(skf.split(X_full, y_full)):\n",
    "    \n",
    "    X_train = X_full[train_index]\n",
    "    y_train = y_full[train_index]\n",
    "    X_test = X_full[test_index]\n",
    "    y_test = y_full[test_index]\n",
    "\n",
    "    #Optimal hyperparameters\n",
    "    hyperparameters = {}\n",
    "\n",
    "    #wm + lr    \n",
    "    hyperparameters['wm_lr'] = {}\n",
    "    hyperparameters['wm_lr']['word_size'] = 2\n",
    "    hyperparameters['wm_lr']['n_bins'] = 3\n",
    "    hyperparameters['wm_lr']['C'] = 0.5\n",
    "\n",
    "    wm = WEASELMUSE(word_size = hyperparameters['wm_lr']['word_size'], \n",
    "                    n_bins = hyperparameters['wm_lr']['n_bins'])\n",
    "\n",
    "    logistic = LogisticRegression(solver = 'lbfgs',\n",
    "                                  multi_class = 'auto',\n",
    "                                  max_iter=3000,\n",
    "                                  C = hyperparameters['wm_lr']['C'])\n",
    "\n",
    "    clf = make_pipeline(wm, logistic)\n",
    "    clf.fit(X_train,y_train)\n",
    "    acc = clf.score(X_test,y_test)\n",
    "    accuracies_lr.append(np.round(acc,2))\n",
    "\n",
    "    #wm + rf    \n",
    "    hyperparameters['wm_rf'] = {}\n",
    "    hyperparameters['wm_rf']['word_size'] = 2\n",
    "    hyperparameters['wm_rf']['n_bins'] = 3\n",
    "    hyperparameters['wm_rf']['n_estimators'] = 300\n",
    "    hyperparameters['wm_rf']['max_depth'] = 9\n",
    "\n",
    "    wm = WEASELMUSE(word_size = hyperparameters['wm_rf']['word_size'], \n",
    "                    n_bins = hyperparameters['wm_rf']['n_bins'])\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators = hyperparameters['wm_rf']['n_estimators'],\n",
    "                                max_depth = hyperparameters['wm_rf']['max_depth'])\n",
    "\n",
    "    clf = make_pipeline(wm, rf)\n",
    "    clf.fit(X_train,y_train)\n",
    "    acc = clf.score(X_test,y_test)\n",
    "    accuracies_rf.append(np.round(acc,2))\n",
    "    \n",
    "    #wm + svc\n",
    "    hyperparameters['wm_svc'] = {}\n",
    "    hyperparameters['wm_svc']['word_size'] = 2\n",
    "    hyperparameters['wm_svc']['n_bins'] = 3\n",
    "    hyperparameters['wm_svc']['C'] = 1\n",
    "    hyperparameters['wm_svc']['kernel'] = 'rbf'\n",
    "    hyperparameters['wm_svc']['degree'] = 3\n",
    "    hyperparameters['wm_svc']['gamma'] = 'scale'\n",
    "\n",
    "    wm = WEASELMUSE(word_size = hyperparameters['wm_svc']['word_size'], \n",
    "                    n_bins = hyperparameters['wm_svc']['n_bins'])\n",
    "\n",
    "    svc = SVC(C = hyperparameters['wm_svc']['C'],\n",
    "              kernel = hyperparameters['wm_svc']['kernel'],\n",
    "              degree = hyperparameters['wm_svc']['degree'],\n",
    "              gamma = hyperparameters['wm_svc']['gamma'])\n",
    "\n",
    "    clf = make_pipeline(wm, svc)\n",
    "    clf.fit(X_train,y_train)\n",
    "    acc = clf.score(X_test,y_test)\n",
    "    accuracies_svc.append(np.round(acc,2))\n",
    "\n",
    "    #wm + lgbm\n",
    "    hyperparameters['wm_lgbm'] = {}\n",
    "    hyperparameters['wm_lgbm']['word_size'] = 4\n",
    "    hyperparameters['wm_lgbm']['n_bins'] = 5\n",
    "    hyperparameters['wm_lgbm']['num_leaves'] = 92\n",
    "    hyperparameters['wm_lgbm']['max_depth'] = 3\n",
    "    hyperparameters['wm_lgbm']['learning_rate'] = 0.3\n",
    "    hyperparameters['wm_lgbm']['n_estimators'] = 250\n",
    "    hyperparameters['wm_lgbm']['min_split_gain'] = 0.2\n",
    "    hyperparameters['wm_lgbm']['min_child_samples'] = 20\n",
    "    hyperparameters['wm_lgbm']['colsample_bytree'] = 1\n",
    "    hyperparameters['wm_lgbm']['reg_alpha'] = 0.3\n",
    "    hyperparameters['wm_lgbm']['reg_lambda'] = 0.2\n",
    "\n",
    "    wm = WEASELMUSE(word_size = hyperparameters['wm_svc']['word_size'], \n",
    "                    n_bins = hyperparameters['wm_svc']['n_bins'])\n",
    "\n",
    "    def sparse_float(mat):\n",
    "        return mat.astype('float')\n",
    "\n",
    "    trans_sparse_float = FunctionTransformer(sparse_float, validate = False)\n",
    "\n",
    "    lgbm = LGBMClassifier(n_jobs = -1,\n",
    "                          num_leaves = hyperparameters['wm_lgbm']['num_leaves'],\n",
    "                          max_depth = hyperparameters['wm_lgbm']['max_depth'],\n",
    "                          learning_rate = hyperparameters['wm_lgbm']['learning_rate'],\n",
    "                          n_estimators = hyperparameters['wm_lgbm']['n_estimators'],\n",
    "                          min_split_gain = hyperparameters['wm_lgbm']['min_split_gain'],\n",
    "                          min_child_samples = hyperparameters['wm_lgbm']['min_child_samples'],\n",
    "                          colsample_by_tree = hyperparameters['wm_lgbm']['colsample_bytree'],\n",
    "                          reg_alpha = hyperparameters['wm_lgbm']['reg_alpha'],\n",
    "                          reg_lambda = hyperparameters['wm_lgbm']['reg_lambda'])\n",
    "\n",
    "    clf = make_pipeline(wm, trans_sparse_float, lgbm)\n",
    "    clf.fit(X_train,y_train)\n",
    "    acc = clf.score(X_test,y_test)\n",
    "    accuracies_lgbm.append(np.round(acc,2))\n",
    "    \n",
    "scores['wm_lr'] = [np.mean(accuracies_lr), np.std(accuracies_lr)]\n",
    "scores['wm_rf'] = [np.mean(accuracies_rf), np.std(accuracies_rf)]\n",
    "scores['wm_svc'] = [np.mean(accuracies_svc), np.std(accuracies_svc)]\n",
    "scores['wm_lgbm'] = [np.mean(accuracies_lgbm), np.std(accuracies_lgbm)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T18:15:00.220833Z",
     "start_time": "2020-11-04T18:15:00.210492Z"
    }
   },
   "outputs": [],
   "source": [
    "def dl_func(dl_type,\n",
    "            X_train_dl, \n",
    "            y_train_dl,\n",
    "            X_val_dl,\n",
    "            y_val_dl,\n",
    "            X_test_dl,\n",
    "            y_test_dl):\n",
    "    \n",
    "    #Validate diferent architectures\n",
    "    num_of_candidate_models = 8\n",
    "    random_search_epoches = 100\n",
    "    random_search_es = 30\n",
    "    best_model_epoches = 200\n",
    "    best_model_es = 30\n",
    "\n",
    "    for mod_type in [dl_type]:\n",
    "\n",
    "        #Create architectures\n",
    "        num_classes = y_train_dl.shape[1]\n",
    "        metric = 'accuracy'\n",
    "        models = mcfly.modelgen.generate_models(X_train_dl.shape,\n",
    "                                                number_of_classes=num_classes,\n",
    "                                                number_of_models = num_of_candidate_models,\n",
    "                                                model_types = [mod_type],\n",
    "                                                metrics=[metric])\n",
    "\n",
    "        #Save intermediate results\n",
    "        resultpath = 'temp'\n",
    "        outputfile = os.path.join(resultpath, 'modelcomparison_{}_{}.json'.format(mod_type,dataset))\n",
    "\n",
    "        #Find best architecture\n",
    "        histories, val_accuracies, val_losses = mcfly.find_architecture.train_models_on_samples(X_train_dl, y_train_dl,\n",
    "                                                                                                X_val_dl, y_val_dl,\n",
    "                                                                                                models,\n",
    "                                                                                                nr_epochs=random_search_epoches,\n",
    "                                                                                                subset_size=None,\n",
    "                                                                                                verbose=False,\n",
    "                                                                                                outputfile=outputfile,\n",
    "                                                                                                metric=metric,\n",
    "                                                                                                early_stopping_patience=random_search_es\n",
    "                                                                                                )\n",
    "\n",
    "        #Select and train best architecture\n",
    "        best_model_index = np.argmax(val_accuracies)\n",
    "        best_model, best_params, best_model_types = models[best_model_index]\n",
    "\n",
    "        es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=best_model_es)\n",
    "        mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "        history = best_model.fit(X_train_dl, y_train_dl,\n",
    "                                 epochs=best_model_epoches, validation_data=(X_val_dl, y_val_dl),\n",
    "                                 callbacks = [es,mc])\n",
    "\n",
    "        #Accuracy in test dataset\n",
    "        saved_model = load_model('best_model.h5')\n",
    "        return saved_model.evaluate(X_test_dl, y_test_dl, verbose = False)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-04T17:54:53.901Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bceaeb58a8e6417a949806913155a8fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of model_types is set from ['CNN', 'DeepConvLSTM', 'ResNet', 'InceptionTime'] (default) to ['CNN']\n",
      "The value of metrics is set from ['accuracy'] (default) to ['accuracy']\n",
      "Set maximum kernel size for InceptionTime models to number of timesteps.\n",
      "Train on 300 samples, validate on 74 samples\n",
      "Epoch 1/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1826 - accuracy: 0.9955\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.56757, saving model to best_model.h5\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.1804 - accuracy: 0.9967 - val_loss: 0.8881 - val_accuracy: 0.5676\n",
      "Epoch 2/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1735 - accuracy: 1.0000\n",
      "Epoch 00002: val_accuracy improved from 0.56757 to 0.59459, saving model to best_model.h5\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.1750 - accuracy: 1.0000 - val_loss: 0.9009 - val_accuracy: 0.5946\n",
      "Epoch 3/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.1742 - accuracy: 1.0000\n",
      "Epoch 00003: val_accuracy did not improve from 0.59459\n",
      "300/300 [==============================] - 0s 869us/sample - loss: 0.1751 - accuracy: 1.0000 - val_loss: 0.9179 - val_accuracy: 0.5811\n",
      "Epoch 4/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1733 - accuracy: 1.0000\n",
      "Epoch 00004: val_accuracy improved from 0.59459 to 0.60811, saving model to best_model.h5\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.1717 - accuracy: 1.0000 - val_loss: 0.8741 - val_accuracy: 0.6081\n",
      "Epoch 5/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1778 - accuracy: 1.0000\n",
      "Epoch 00005: val_accuracy did not improve from 0.60811\n",
      "300/300 [==============================] - 0s 834us/sample - loss: 0.1761 - accuracy: 1.0000 - val_loss: 0.8728 - val_accuracy: 0.5946\n",
      "Epoch 6/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1743 - accuracy: 1.0000\n",
      "Epoch 00006: val_accuracy did not improve from 0.60811\n",
      "300/300 [==============================] - 0s 832us/sample - loss: 0.1745 - accuracy: 1.0000 - val_loss: 0.8790 - val_accuracy: 0.6081\n",
      "Epoch 7/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1689 - accuracy: 1.0000\n",
      "Epoch 00007: val_accuracy did not improve from 0.60811\n",
      "300/300 [==============================] - 0s 849us/sample - loss: 0.1680 - accuracy: 1.0000 - val_loss: 0.8690 - val_accuracy: 0.5946\n",
      "Epoch 8/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1687 - accuracy: 1.0000\n",
      "Epoch 00008: val_accuracy did not improve from 0.60811\n",
      "300/300 [==============================] - 0s 832us/sample - loss: 0.1688 - accuracy: 1.0000 - val_loss: 0.8650 - val_accuracy: 0.5946\n",
      "Epoch 9/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1709 - accuracy: 1.0000\n",
      "Epoch 00009: val_accuracy did not improve from 0.60811\n",
      "300/300 [==============================] - 0s 819us/sample - loss: 0.1724 - accuracy: 1.0000 - val_loss: 0.8836 - val_accuracy: 0.5811\n",
      "Epoch 10/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1647 - accuracy: 1.0000\n",
      "Epoch 00010: val_accuracy did not improve from 0.60811\n",
      "300/300 [==============================] - 0s 854us/sample - loss: 0.1644 - accuracy: 1.0000 - val_loss: 0.8539 - val_accuracy: 0.6081\n",
      "Epoch 11/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1682 - accuracy: 1.0000\n",
      "Epoch 00011: val_accuracy did not improve from 0.60811\n",
      "300/300 [==============================] - 0s 858us/sample - loss: 0.1692 - accuracy: 1.0000 - val_loss: 0.8487 - val_accuracy: 0.5676\n",
      "Epoch 12/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1620 - accuracy: 1.0000\n",
      "Epoch 00012: val_accuracy did not improve from 0.60811\n",
      "300/300 [==============================] - 0s 839us/sample - loss: 0.1633 - accuracy: 1.0000 - val_loss: 0.8706 - val_accuracy: 0.5676\n",
      "Epoch 13/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1686 - accuracy: 1.0000\n",
      "Epoch 00013: val_accuracy did not improve from 0.60811\n",
      "300/300 [==============================] - 0s 849us/sample - loss: 0.1674 - accuracy: 1.0000 - val_loss: 0.8717 - val_accuracy: 0.5946\n",
      "Epoch 14/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1672 - accuracy: 1.0000\n",
      "Epoch 00014: val_accuracy did not improve from 0.60811\n",
      "300/300 [==============================] - 0s 832us/sample - loss: 0.1656 - accuracy: 1.0000 - val_loss: 0.8837 - val_accuracy: 0.5946\n",
      "Epoch 15/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1676 - accuracy: 1.0000\n",
      "Epoch 00015: val_accuracy did not improve from 0.60811\n",
      "300/300 [==============================] - 0s 804us/sample - loss: 0.1652 - accuracy: 1.0000 - val_loss: 0.8742 - val_accuracy: 0.5676\n",
      "Epoch 16/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1647 - accuracy: 1.0000\n",
      "Epoch 00016: val_accuracy did not improve from 0.60811\n",
      "300/300 [==============================] - 0s 813us/sample - loss: 0.1644 - accuracy: 1.0000 - val_loss: 0.8568 - val_accuracy: 0.5946\n",
      "Epoch 17/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1596 - accuracy: 1.0000\n",
      "Epoch 00017: val_accuracy did not improve from 0.60811\n",
      "300/300 [==============================] - 0s 810us/sample - loss: 0.1614 - accuracy: 1.0000 - val_loss: 0.8527 - val_accuracy: 0.6081\n",
      "Epoch 18/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1632 - accuracy: 1.0000\n",
      "Epoch 00018: val_accuracy did not improve from 0.60811\n",
      "300/300 [==============================] - 0s 817us/sample - loss: 0.1642 - accuracy: 1.0000 - val_loss: 0.8409 - val_accuracy: 0.5946\n",
      "Epoch 19/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1607 - accuracy: 1.0000\n",
      "Epoch 00019: val_accuracy improved from 0.60811 to 0.62162, saving model to best_model.h5\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.1589 - accuracy: 1.0000 - val_loss: 0.8433 - val_accuracy: 0.6216\n",
      "Epoch 20/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.1616 - accuracy: 1.0000\n",
      "Epoch 00020: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 865us/sample - loss: 0.1616 - accuracy: 1.0000 - val_loss: 0.9013 - val_accuracy: 0.5946\n",
      "Epoch 21/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1607 - accuracy: 1.0000\n",
      "Epoch 00021: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 833us/sample - loss: 0.1605 - accuracy: 1.0000 - val_loss: 0.8726 - val_accuracy: 0.5676\n",
      "Epoch 22/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1553 - accuracy: 1.0000\n",
      "Epoch 00022: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 814us/sample - loss: 0.1566 - accuracy: 1.0000 - val_loss: 0.8606 - val_accuracy: 0.5676\n",
      "Epoch 23/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1555 - accuracy: 1.0000\n",
      "Epoch 00023: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 818us/sample - loss: 0.1560 - accuracy: 1.0000 - val_loss: 0.9173 - val_accuracy: 0.5811\n",
      "Epoch 24/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1620 - accuracy: 1.0000\n",
      "Epoch 00024: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 839us/sample - loss: 0.1604 - accuracy: 1.0000 - val_loss: 0.8633 - val_accuracy: 0.5811\n",
      "Epoch 25/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1535 - accuracy: 1.0000\n",
      "Epoch 00025: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 839us/sample - loss: 0.1539 - accuracy: 1.0000 - val_loss: 0.8763 - val_accuracy: 0.5811\n",
      "Epoch 26/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.1574 - accuracy: 1.0000\n",
      "Epoch 00026: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 823us/sample - loss: 0.1570 - accuracy: 1.0000 - val_loss: 0.8724 - val_accuracy: 0.5811\n",
      "Epoch 27/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1501 - accuracy: 1.0000\n",
      "Epoch 00027: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 859us/sample - loss: 0.1517 - accuracy: 1.0000 - val_loss: 0.8791 - val_accuracy: 0.5811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1504 - accuracy: 1.0000\n",
      "Epoch 00028: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 821us/sample - loss: 0.1504 - accuracy: 1.0000 - val_loss: 0.8718 - val_accuracy: 0.5676\n",
      "Epoch 29/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1556 - accuracy: 1.0000\n",
      "Epoch 00029: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 808us/sample - loss: 0.1565 - accuracy: 1.0000 - val_loss: 0.8741 - val_accuracy: 0.5811\n",
      "Epoch 30/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1496 - accuracy: 1.0000\n",
      "Epoch 00030: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 824us/sample - loss: 0.1512 - accuracy: 1.0000 - val_loss: 0.8544 - val_accuracy: 0.5676\n",
      "Epoch 31/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1539 - accuracy: 1.0000\n",
      "Epoch 00031: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 822us/sample - loss: 0.1544 - accuracy: 1.0000 - val_loss: 0.8659 - val_accuracy: 0.5946\n",
      "Epoch 32/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1533 - accuracy: 1.0000\n",
      "Epoch 00032: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 833us/sample - loss: 0.1531 - accuracy: 1.0000 - val_loss: 0.8572 - val_accuracy: 0.5946\n",
      "Epoch 33/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1526 - accuracy: 1.0000\n",
      "Epoch 00033: val_accuracy improved from 0.62162 to 0.64865, saving model to best_model.h5\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.1524 - accuracy: 1.0000 - val_loss: 0.8381 - val_accuracy: 0.6486\n",
      "Epoch 34/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1506 - accuracy: 1.0000\n",
      "Epoch 00034: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 837us/sample - loss: 0.1502 - accuracy: 1.0000 - val_loss: 0.8827 - val_accuracy: 0.6486\n",
      "Epoch 35/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1470 - accuracy: 1.0000\n",
      "Epoch 00035: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 830us/sample - loss: 0.1485 - accuracy: 1.0000 - val_loss: 0.8721 - val_accuracy: 0.6081\n",
      "Epoch 36/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1494 - accuracy: 1.0000\n",
      "Epoch 00036: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 831us/sample - loss: 0.1521 - accuracy: 1.0000 - val_loss: 0.8708 - val_accuracy: 0.5405\n",
      "Epoch 37/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1591 - accuracy: 1.0000\n",
      "Epoch 00037: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 816us/sample - loss: 0.1603 - accuracy: 1.0000 - val_loss: 0.9038 - val_accuracy: 0.5946\n",
      "Epoch 38/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1489 - accuracy: 1.0000\n",
      "Epoch 00038: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 841us/sample - loss: 0.1507 - accuracy: 0.9967 - val_loss: 0.8926 - val_accuracy: 0.6081\n",
      "Epoch 39/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1534 - accuracy: 0.9955\n",
      "Epoch 00039: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 834us/sample - loss: 0.1536 - accuracy: 0.9967 - val_loss: 0.9350 - val_accuracy: 0.6216\n",
      "Epoch 40/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1563 - accuracy: 0.9955\n",
      "Epoch 00040: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 831us/sample - loss: 0.1559 - accuracy: 0.9967 - val_loss: 1.0614 - val_accuracy: 0.5270\n",
      "Epoch 41/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1565 - accuracy: 1.0000\n",
      "Epoch 00041: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 832us/sample - loss: 0.1574 - accuracy: 1.0000 - val_loss: 0.8937 - val_accuracy: 0.5946\n",
      "Epoch 42/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1585 - accuracy: 1.0000\n",
      "Epoch 00042: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 830us/sample - loss: 0.1575 - accuracy: 1.0000 - val_loss: 0.9193 - val_accuracy: 0.5541\n",
      "Epoch 43/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1486 - accuracy: 1.0000\n",
      "Epoch 00043: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 865us/sample - loss: 0.1534 - accuracy: 1.0000 - val_loss: 0.9259 - val_accuracy: 0.6081\n",
      "Epoch 44/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1607 - accuracy: 1.0000\n",
      "Epoch 00044: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 846us/sample - loss: 0.1579 - accuracy: 1.0000 - val_loss: 0.9807 - val_accuracy: 0.6081\n",
      "Epoch 45/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1462 - accuracy: 1.0000\n",
      "Epoch 00045: val_accuracy improved from 0.64865 to 0.67568, saving model to best_model.h5\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.1472 - accuracy: 1.0000 - val_loss: 0.9197 - val_accuracy: 0.6757\n",
      "Epoch 46/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1448 - accuracy: 1.0000\n",
      "Epoch 00046: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 845us/sample - loss: 0.1457 - accuracy: 1.0000 - val_loss: 0.8892 - val_accuracy: 0.6486\n",
      "Epoch 47/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1452 - accuracy: 1.0000\n",
      "Epoch 00047: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 834us/sample - loss: 0.1455 - accuracy: 1.0000 - val_loss: 0.9243 - val_accuracy: 0.6081\n",
      "Epoch 48/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1477 - accuracy: 1.0000\n",
      "Epoch 00048: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 815us/sample - loss: 0.1469 - accuracy: 1.0000 - val_loss: 0.9359 - val_accuracy: 0.5676\n",
      "Epoch 49/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1456 - accuracy: 1.0000\n",
      "Epoch 00049: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 809us/sample - loss: 0.1435 - accuracy: 1.0000 - val_loss: 0.9042 - val_accuracy: 0.5811\n",
      "Epoch 50/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1439 - accuracy: 1.0000\n",
      "Epoch 00050: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 821us/sample - loss: 0.1434 - accuracy: 1.0000 - val_loss: 0.8945 - val_accuracy: 0.6216\n",
      "Epoch 51/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1419 - accuracy: 1.0000\n",
      "Epoch 00051: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 822us/sample - loss: 0.1414 - accuracy: 1.0000 - val_loss: 0.8687 - val_accuracy: 0.6351\n",
      "Epoch 52/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1384 - accuracy: 1.0000\n",
      "Epoch 00052: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 807us/sample - loss: 0.1393 - accuracy: 1.0000 - val_loss: 0.8583 - val_accuracy: 0.6622\n",
      "Epoch 53/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1411 - accuracy: 1.0000\n",
      "Epoch 00053: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 804us/sample - loss: 0.1426 - accuracy: 1.0000 - val_loss: 0.8483 - val_accuracy: 0.6351\n",
      "Epoch 54/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1517 - accuracy: 1.0000\n",
      "Epoch 00054: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 838us/sample - loss: 0.1498 - accuracy: 1.0000 - val_loss: 0.8178 - val_accuracy: 0.6351\n",
      "Epoch 55/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1594 - accuracy: 0.9955\n",
      "Epoch 00055: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 820us/sample - loss: 0.1563 - accuracy: 0.9967 - val_loss: 0.8636 - val_accuracy: 0.6081\n",
      "Epoch 56/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1523 - accuracy: 1.0000\n",
      "Epoch 00056: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 805us/sample - loss: 0.1522 - accuracy: 1.0000 - val_loss: 1.0639 - val_accuracy: 0.5405\n",
      "Epoch 57/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1581 - accuracy: 0.9955\n",
      "Epoch 00057: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 808us/sample - loss: 0.1718 - accuracy: 0.9967 - val_loss: 0.8337 - val_accuracy: 0.6486\n",
      "Epoch 58/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1643 - accuracy: 0.9955\n",
      "Epoch 00058: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 826us/sample - loss: 0.1808 - accuracy: 0.9867 - val_loss: 0.8605 - val_accuracy: 0.5946\n",
      "Epoch 59/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1728 - accuracy: 0.9955\n",
      "Epoch 00059: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 821us/sample - loss: 0.1896 - accuracy: 0.9867 - val_loss: 0.9205 - val_accuracy: 0.5405\n",
      "Epoch 60/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1776 - accuracy: 1.0000\n",
      "Epoch 00060: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 812us/sample - loss: 0.1755 - accuracy: 1.0000 - val_loss: 0.9585 - val_accuracy: 0.5946\n",
      "Epoch 61/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1686 - accuracy: 1.0000\n",
      "Epoch 00061: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 818us/sample - loss: 0.1695 - accuracy: 1.0000 - val_loss: 1.1880 - val_accuracy: 0.5811\n",
      "Epoch 62/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1621 - accuracy: 1.0000\n",
      "Epoch 00062: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 823us/sample - loss: 0.1629 - accuracy: 1.0000 - val_loss: 1.1764 - val_accuracy: 0.5811\n",
      "Epoch 63/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1576 - accuracy: 1.0000\n",
      "Epoch 00063: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 806us/sample - loss: 0.1569 - accuracy: 1.0000 - val_loss: 0.9574 - val_accuracy: 0.5676\n",
      "Epoch 64/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1507 - accuracy: 1.0000\n",
      "Epoch 00064: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 819us/sample - loss: 0.1496 - accuracy: 1.0000 - val_loss: 0.9817 - val_accuracy: 0.5270\n",
      "Epoch 65/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1504 - accuracy: 1.0000\n",
      "Epoch 00065: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 838us/sample - loss: 0.1495 - accuracy: 1.0000 - val_loss: 0.9543 - val_accuracy: 0.5405\n",
      "Epoch 66/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1406 - accuracy: 1.0000\n",
      "Epoch 00066: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 827us/sample - loss: 0.1410 - accuracy: 1.0000 - val_loss: 0.9398 - val_accuracy: 0.5405\n",
      "Epoch 67/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1430 - accuracy: 1.0000\n",
      "Epoch 00067: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 830us/sample - loss: 0.1443 - accuracy: 1.0000 - val_loss: 0.9489 - val_accuracy: 0.5405\n",
      "Epoch 68/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1417 - accuracy: 1.0000\n",
      "Epoch 00068: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 815us/sample - loss: 0.1442 - accuracy: 1.0000 - val_loss: 0.9511 - val_accuracy: 0.5405\n",
      "Epoch 69/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1395 - accuracy: 1.0000\n",
      "Epoch 00069: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 805us/sample - loss: 0.1402 - accuracy: 1.0000 - val_loss: 0.9304 - val_accuracy: 0.5676\n",
      "Epoch 70/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1388 - accuracy: 1.0000\n",
      "Epoch 00070: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 812us/sample - loss: 0.1403 - accuracy: 1.0000 - val_loss: 0.9905 - val_accuracy: 0.5676\n",
      "Epoch 71/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1371 - accuracy: 1.0000\n",
      "Epoch 00071: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 804us/sample - loss: 0.1385 - accuracy: 1.0000 - val_loss: 0.9607 - val_accuracy: 0.5946\n",
      "Epoch 72/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1394 - accuracy: 1.0000\n",
      "Epoch 00072: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 827us/sample - loss: 0.1390 - accuracy: 1.0000 - val_loss: 0.9582 - val_accuracy: 0.5676\n",
      "Epoch 73/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1375 - accuracy: 1.0000\n",
      "Epoch 00073: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 808us/sample - loss: 0.1359 - accuracy: 1.0000 - val_loss: 0.9362 - val_accuracy: 0.5676\n",
      "Epoch 74/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1340 - accuracy: 1.0000\n",
      "Epoch 00074: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 834us/sample - loss: 0.1392 - accuracy: 1.0000 - val_loss: 0.9214 - val_accuracy: 0.5676\n",
      "Epoch 75/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1335 - accuracy: 1.0000\n",
      "Epoch 00075: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 806us/sample - loss: 0.1343 - accuracy: 1.0000 - val_loss: 0.8832 - val_accuracy: 0.5541\n",
      "Epoch 00075: early stopping\n",
      "The value of model_types is set from ['CNN', 'DeepConvLSTM', 'ResNet', 'InceptionTime'] (default) to ['InceptionTime']\n",
      "The value of metrics is set from ['accuracy'] (default) to ['accuracy']\n",
      "Set maximum kernel size for InceptionTime models to number of timesteps.\n",
      "Train on 300 samples, validate on 74 samples\n",
      "Epoch 1/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.2057 - accuracy: 0.9236\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.62162, saving model to best_model.h5\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.1998 - accuracy: 0.9267 - val_loss: 1.0136 - val_accuracy: 0.6216\n",
      "Epoch 2/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.1110 - accuracy: 0.9722\n",
      "Epoch 00002: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.1101 - accuracy: 0.9733 - val_loss: 1.5380 - val_accuracy: 0.5405\n",
      "Epoch 3/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.1337 - accuracy: 0.9444\n",
      "Epoch 00003: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.1496 - accuracy: 0.9400 - val_loss: 1.2549 - val_accuracy: 0.5946\n",
      "Epoch 4/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0808 - accuracy: 0.9757\n",
      "Epoch 00004: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0873 - accuracy: 0.9733 - val_loss: 1.5834 - val_accuracy: 0.5405\n",
      "Epoch 5/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0756 - accuracy: 0.9757\n",
      "Epoch 00005: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0756 - accuracy: 0.9767 - val_loss: 1.4113 - val_accuracy: 0.5811\n",
      "Epoch 6/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0702 - accuracy: 0.9792\n",
      "Epoch 00006: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0724 - accuracy: 0.9767 - val_loss: 1.4181 - val_accuracy: 0.5946\n",
      "Epoch 7/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0702 - accuracy: 0.9757\n",
      "Epoch 00007: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0774 - accuracy: 0.9700 - val_loss: 1.8719 - val_accuracy: 0.5946\n",
      "Epoch 8/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288/300 [===========================>..] - ETA: 0s - loss: 0.1126 - accuracy: 0.9653\n",
      "Epoch 00008: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.1101 - accuracy: 0.9667 - val_loss: 1.5907 - val_accuracy: 0.6216\n",
      "Epoch 9/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0981 - accuracy: 0.9653\n",
      "Epoch 00009: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.1030 - accuracy: 0.9633 - val_loss: 1.8490 - val_accuracy: 0.6081\n",
      "Epoch 10/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0642 - accuracy: 0.9826\n",
      "Epoch 00010: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0682 - accuracy: 0.9800 - val_loss: 1.3999 - val_accuracy: 0.5946\n",
      "Epoch 11/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0642 - accuracy: 0.9826\n",
      "Epoch 00011: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0679 - accuracy: 0.9800 - val_loss: 1.6862 - val_accuracy: 0.5946\n",
      "Epoch 12/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0483 - accuracy: 0.9896\n",
      "Epoch 00012: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0532 - accuracy: 0.9867 - val_loss: 1.3417 - val_accuracy: 0.5676\n",
      "Epoch 13/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0445 - accuracy: 0.9896\n",
      "Epoch 00013: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0436 - accuracy: 0.9900 - val_loss: 1.5255 - val_accuracy: 0.5405\n",
      "Epoch 14/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0217 - accuracy: 0.9965\n",
      "Epoch 00014: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0213 - accuracy: 0.9967 - val_loss: 1.4404 - val_accuracy: 0.5541\n",
      "Epoch 15/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0405 - accuracy: 0.9896\n",
      "Epoch 00015: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0395 - accuracy: 0.9900 - val_loss: 1.4197 - val_accuracy: 0.6081\n",
      "Epoch 16/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0321 - accuracy: 0.9931\n",
      "Epoch 00016: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0309 - accuracy: 0.9933 - val_loss: 1.5384 - val_accuracy: 0.5946\n",
      "Epoch 17/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0300 - accuracy: 0.9931\n",
      "Epoch 00017: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0291 - accuracy: 0.9933 - val_loss: 1.6871 - val_accuracy: 0.5405\n",
      "Epoch 18/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0269 - accuracy: 0.9931\n",
      "Epoch 00018: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0266 - accuracy: 0.9933 - val_loss: 1.5723 - val_accuracy: 0.5811\n",
      "Epoch 19/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0251 - accuracy: 0.9931\n",
      "Epoch 00019: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0251 - accuracy: 0.9933 - val_loss: 1.6735 - val_accuracy: 0.6216\n",
      "Epoch 20/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 00020: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0080 - accuracy: 1.0000 - val_loss: 1.5812 - val_accuracy: 0.5676\n",
      "Epoch 21/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0101 - accuracy: 0.9965\n",
      "Epoch 00021: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0112 - accuracy: 0.9967 - val_loss: 1.5663 - val_accuracy: 0.5811\n",
      "Epoch 22/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0114 - accuracy: 0.9965\n",
      "Epoch 00022: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0110 - accuracy: 0.9967 - val_loss: 1.5380 - val_accuracy: 0.5811\n",
      "Epoch 23/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 00023: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0072 - accuracy: 1.0000 - val_loss: 1.6336 - val_accuracy: 0.5405\n",
      "Epoch 24/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 00024: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 2ms/sample - loss: 0.0050 - accuracy: 1.0000 - val_loss: 1.6687 - val_accuracy: 0.5135\n",
      "Epoch 25/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 00025: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0063 - accuracy: 1.0000 - val_loss: 1.5725 - val_accuracy: 0.5811\n",
      "Epoch 26/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0068 - accuracy: 0.9965\n",
      "Epoch 00026: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0309 - accuracy: 0.9933 - val_loss: 1.7209 - val_accuracy: 0.6081\n",
      "Epoch 27/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0779 - accuracy: 0.9722\n",
      "Epoch 00027: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0862 - accuracy: 0.9667 - val_loss: 1.7296 - val_accuracy: 0.5811\n",
      "Epoch 28/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.1526 - accuracy: 0.9306\n",
      "Epoch 00028: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.1493 - accuracy: 0.9333 - val_loss: 1.5968 - val_accuracy: 0.6081\n",
      "Epoch 29/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.1078 - accuracy: 0.9479\n",
      "Epoch 00029: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.1046 - accuracy: 0.9500 - val_loss: 1.6079 - val_accuracy: 0.5811\n",
      "Epoch 30/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.1265 - accuracy: 0.9514\n",
      "Epoch 00030: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.1266 - accuracy: 0.9500 - val_loss: 1.5845 - val_accuracy: 0.5946\n",
      "Epoch 31/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0787 - accuracy: 0.9722\n",
      "Epoch 00031: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0790 - accuracy: 0.9700 - val_loss: 1.4297 - val_accuracy: 0.5811\n",
      "Epoch 00031: early stopping\n",
      "The value of model_types is set from ['CNN', 'DeepConvLSTM', 'ResNet', 'InceptionTime'] (default) to ['CNN']\n",
      "The value of metrics is set from ['accuracy'] (default) to ['accuracy']\n",
      "Set maximum kernel size for InceptionTime models to number of timesteps.\n",
      "Train on 300 samples, validate on 74 samples\n",
      "Epoch 1/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.2296 - accuracy: 1.0000\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.56757, saving model to best_model.h5\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.2292 - accuracy: 1.0000 - val_loss: 1.0001 - val_accuracy: 0.5676\n",
      "Epoch 2/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.2156 - accuracy: 1.0000\n",
      "Epoch 00002: val_accuracy did not improve from 0.56757\n",
      "300/300 [==============================] - 0s 876us/sample - loss: 0.2162 - accuracy: 1.0000 - val_loss: 1.0001 - val_accuracy: 0.5676\n",
      "Epoch 3/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.2204 - accuracy: 1.0000\n",
      "Epoch 00003: val_accuracy improved from 0.56757 to 0.58108, saving model to best_model.h5\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.2194 - accuracy: 1.0000 - val_loss: 0.9891 - val_accuracy: 0.5811\n",
      "Epoch 4/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.2155 - accuracy: 1.0000\n",
      "Epoch 00004: val_accuracy did not improve from 0.58108\n",
      "300/300 [==============================] - 0s 884us/sample - loss: 0.2186 - accuracy: 1.0000 - val_loss: 0.9745 - val_accuracy: 0.5676\n",
      "Epoch 5/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.2130 - accuracy: 1.0000\n",
      "Epoch 00005: val_accuracy did not improve from 0.58108\n",
      "300/300 [==============================] - 0s 859us/sample - loss: 0.2130 - accuracy: 1.0000 - val_loss: 0.9468 - val_accuracy: 0.5811\n",
      "Epoch 6/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.2133 - accuracy: 1.0000\n",
      "Epoch 00006: val_accuracy did not improve from 0.58108\n",
      "300/300 [==============================] - 0s 874us/sample - loss: 0.2149 - accuracy: 1.0000 - val_loss: 0.9515 - val_accuracy: 0.5811\n",
      "Epoch 7/200\n",
      "256/300 [========================>.....] - ETA: 0s - loss: 0.2132 - accuracy: 1.0000\n",
      "Epoch 00007: val_accuracy did not improve from 0.58108\n",
      "300/300 [==============================] - 0s 891us/sample - loss: 0.2126 - accuracy: 1.0000 - val_loss: 0.9723 - val_accuracy: 0.5676\n",
      "Epoch 8/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.2096 - accuracy: 1.0000\n",
      "Epoch 00008: val_accuracy did not improve from 0.58108\n",
      "300/300 [==============================] - 0s 852us/sample - loss: 0.2081 - accuracy: 1.0000 - val_loss: 0.9731 - val_accuracy: 0.5405\n",
      "Epoch 9/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.2050 - accuracy: 1.0000\n",
      "Epoch 00009: val_accuracy did not improve from 0.58108\n",
      "300/300 [==============================] - 0s 850us/sample - loss: 0.2048 - accuracy: 1.0000 - val_loss: 0.9630 - val_accuracy: 0.5541\n",
      "Epoch 10/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.2043 - accuracy: 1.0000\n",
      "Epoch 00010: val_accuracy did not improve from 0.58108\n",
      "300/300 [==============================] - 0s 870us/sample - loss: 0.2053 - accuracy: 1.0000 - val_loss: 0.9520 - val_accuracy: 0.5541\n",
      "Epoch 11/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.2099 - accuracy: 1.0000\n",
      "Epoch 00011: val_accuracy did not improve from 0.58108\n",
      "300/300 [==============================] - 0s 880us/sample - loss: 0.2097 - accuracy: 1.0000 - val_loss: 0.9468 - val_accuracy: 0.5676\n",
      "Epoch 12/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.2005 - accuracy: 1.0000\n",
      "Epoch 00012: val_accuracy did not improve from 0.58108\n",
      "300/300 [==============================] - 0s 844us/sample - loss: 0.1998 - accuracy: 1.0000 - val_loss: 0.9404 - val_accuracy: 0.5811\n",
      "Epoch 13/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1988 - accuracy: 1.0000\n",
      "Epoch 00013: val_accuracy did not improve from 0.58108\n",
      "300/300 [==============================] - 0s 848us/sample - loss: 0.2044 - accuracy: 1.0000 - val_loss: 0.9371 - val_accuracy: 0.5541\n",
      "Epoch 14/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.2027 - accuracy: 1.0000\n",
      "Epoch 00014: val_accuracy improved from 0.58108 to 0.59459, saving model to best_model.h5\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.2020 - accuracy: 1.0000 - val_loss: 0.9657 - val_accuracy: 0.5946\n",
      "Epoch 15/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.2080 - accuracy: 0.9955\n",
      "Epoch 00015: val_accuracy did not improve from 0.59459\n",
      "300/300 [==============================] - 0s 836us/sample - loss: 0.2085 - accuracy: 0.9933 - val_loss: 0.9071 - val_accuracy: 0.5811\n",
      "Epoch 16/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.2156 - accuracy: 1.0000\n",
      "Epoch 00016: val_accuracy improved from 0.59459 to 0.62162, saving model to best_model.h5\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.2221 - accuracy: 1.0000 - val_loss: 0.9094 - val_accuracy: 0.6216\n",
      "Epoch 17/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.2130 - accuracy: 1.0000\n",
      "Epoch 00017: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 854us/sample - loss: 0.2120 - accuracy: 1.0000 - val_loss: 1.1075 - val_accuracy: 0.5000\n",
      "Epoch 18/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.2063 - accuracy: 1.0000\n",
      "Epoch 00018: val_accuracy improved from 0.62162 to 0.63514, saving model to best_model.h5\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.2083 - accuracy: 1.0000 - val_loss: 1.0574 - val_accuracy: 0.6351\n",
      "Epoch 19/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.2027 - accuracy: 1.0000\n",
      "Epoch 00019: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 863us/sample - loss: 0.2037 - accuracy: 1.0000 - val_loss: 1.0754 - val_accuracy: 0.6216\n",
      "Epoch 20/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1993 - accuracy: 1.0000\n",
      "Epoch 00020: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 844us/sample - loss: 0.2018 - accuracy: 1.0000 - val_loss: 1.0979 - val_accuracy: 0.5405\n",
      "Epoch 21/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1958 - accuracy: 1.0000\n",
      "Epoch 00021: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 855us/sample - loss: 0.1997 - accuracy: 1.0000 - val_loss: 1.0590 - val_accuracy: 0.5811\n",
      "Epoch 22/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.2004 - accuracy: 1.0000\n",
      "Epoch 00022: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 847us/sample - loss: 0.1982 - accuracy: 1.0000 - val_loss: 1.0554 - val_accuracy: 0.5946\n",
      "Epoch 23/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.1927 - accuracy: 1.0000\n",
      "Epoch 00023: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 871us/sample - loss: 0.1925 - accuracy: 1.0000 - val_loss: 1.0769 - val_accuracy: 0.5676\n",
      "Epoch 24/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.1964 - accuracy: 1.0000\n",
      "Epoch 00024: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 869us/sample - loss: 0.1975 - accuracy: 1.0000 - val_loss: 1.0902 - val_accuracy: 0.5541\n",
      "Epoch 25/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.1902 - accuracy: 1.0000\n",
      "Epoch 00025: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 884us/sample - loss: 0.1903 - accuracy: 1.0000 - val_loss: 1.0706 - val_accuracy: 0.5811\n",
      "Epoch 26/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.1886 - accuracy: 1.0000\n",
      "Epoch 00026: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 919us/sample - loss: 0.1885 - accuracy: 1.0000 - val_loss: 1.0586 - val_accuracy: 0.5811\n",
      "Epoch 27/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1918 - accuracy: 1.0000\n",
      "Epoch 00027: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 850us/sample - loss: 0.1923 - accuracy: 1.0000 - val_loss: 1.0316 - val_accuracy: 0.5405\n",
      "Epoch 28/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1849 - accuracy: 1.0000\n",
      "Epoch 00028: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 847us/sample - loss: 0.1849 - accuracy: 1.0000 - val_loss: 1.0129 - val_accuracy: 0.5676\n",
      "Epoch 29/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.1883 - accuracy: 1.0000\n",
      "Epoch 00029: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 868us/sample - loss: 0.1890 - accuracy: 1.0000 - val_loss: 1.0337 - val_accuracy: 0.5676\n",
      "Epoch 30/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1880 - accuracy: 1.0000\n",
      "Epoch 00030: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 858us/sample - loss: 0.1871 - accuracy: 1.0000 - val_loss: 1.0414 - val_accuracy: 0.5676\n",
      "Epoch 31/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1866 - accuracy: 1.0000\n",
      "Epoch 00031: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 842us/sample - loss: 0.1854 - accuracy: 1.0000 - val_loss: 1.0374 - val_accuracy: 0.5676\n",
      "Epoch 32/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1832 - accuracy: 1.0000\n",
      "Epoch 00032: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 854us/sample - loss: 0.1877 - accuracy: 1.0000 - val_loss: 1.0159 - val_accuracy: 0.5676\n",
      "Epoch 33/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1863 - accuracy: 1.0000\n",
      "Epoch 00033: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 845us/sample - loss: 0.1856 - accuracy: 1.0000 - val_loss: 1.0093 - val_accuracy: 0.5676\n",
      "Epoch 34/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.1884 - accuracy: 1.0000\n",
      "Epoch 00034: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 855us/sample - loss: 0.1881 - accuracy: 1.0000 - val_loss: 1.0102 - val_accuracy: 0.5405\n",
      "Epoch 35/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1779 - accuracy: 1.0000\n",
      "Epoch 00035: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 860us/sample - loss: 0.1790 - accuracy: 1.0000 - val_loss: 1.0038 - val_accuracy: 0.5676\n",
      "Epoch 36/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.1784 - accuracy: 1.0000\n",
      "Epoch 00036: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 854us/sample - loss: 0.1785 - accuracy: 1.0000 - val_loss: 0.9867 - val_accuracy: 0.5676\n",
      "Epoch 37/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1783 - accuracy: 1.0000\n",
      "Epoch 00037: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 851us/sample - loss: 0.1801 - accuracy: 1.0000 - val_loss: 0.9720 - val_accuracy: 0.6081\n",
      "Epoch 38/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1869 - accuracy: 0.9955\n",
      "Epoch 00038: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 857us/sample - loss: 0.1852 - accuracy: 0.9967 - val_loss: 0.9398 - val_accuracy: 0.5270\n",
      "Epoch 39/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1784 - accuracy: 1.0000\n",
      "Epoch 00039: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 851us/sample - loss: 0.1827 - accuracy: 1.0000 - val_loss: 0.9552 - val_accuracy: 0.5676\n",
      "Epoch 40/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1788 - accuracy: 1.0000\n",
      "Epoch 00040: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 831us/sample - loss: 0.1793 - accuracy: 1.0000 - val_loss: 0.9782 - val_accuracy: 0.5541\n",
      "Epoch 41/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1806 - accuracy: 1.0000\n",
      "Epoch 00041: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 851us/sample - loss: 0.1820 - accuracy: 1.0000 - val_loss: 1.0172 - val_accuracy: 0.5270\n",
      "Epoch 42/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1831 - accuracy: 1.0000\n",
      "Epoch 00042: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 832us/sample - loss: 0.1816 - accuracy: 1.0000 - val_loss: 1.0653 - val_accuracy: 0.5811\n",
      "Epoch 43/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1765 - accuracy: 1.0000\n",
      "Epoch 00043: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 850us/sample - loss: 0.1764 - accuracy: 1.0000 - val_loss: 1.0645 - val_accuracy: 0.5405\n",
      "Epoch 44/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1730 - accuracy: 1.0000\n",
      "Epoch 00044: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 858us/sample - loss: 0.1735 - accuracy: 1.0000 - val_loss: 1.0317 - val_accuracy: 0.5676\n",
      "Epoch 45/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.1744 - accuracy: 1.0000\n",
      "Epoch 00045: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 880us/sample - loss: 0.1753 - accuracy: 1.0000 - val_loss: 1.0211 - val_accuracy: 0.5541\n",
      "Epoch 46/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1743 - accuracy: 1.0000\n",
      "Epoch 00046: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 865us/sample - loss: 0.1740 - accuracy: 1.0000 - val_loss: 1.0779 - val_accuracy: 0.5676\n",
      "Epoch 47/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1697 - accuracy: 1.0000\n",
      "Epoch 00047: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 853us/sample - loss: 0.1742 - accuracy: 1.0000 - val_loss: 1.0630 - val_accuracy: 0.5541\n",
      "Epoch 48/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1725 - accuracy: 1.0000\n",
      "Epoch 00048: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 842us/sample - loss: 0.1745 - accuracy: 1.0000 - val_loss: 1.0624 - val_accuracy: 0.5135\n",
      "Epoch 00048: early stopping\n",
      "The value of model_types is set from ['CNN', 'DeepConvLSTM', 'ResNet', 'InceptionTime'] (default) to ['InceptionTime']\n",
      "The value of metrics is set from ['accuracy'] (default) to ['accuracy']\n",
      "Set maximum kernel size for InceptionTime models to number of timesteps.\n",
      "Train on 300 samples, validate on 74 samples\n",
      "Epoch 1/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0443 - accuracy: 0.9931\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.59459, saving model to best_model.h5\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0430 - accuracy: 0.9933 - val_loss: 1.4821 - val_accuracy: 0.5946\n",
      "Epoch 2/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0397 - accuracy: 0.9861\n",
      "Epoch 00002: val_accuracy did not improve from 0.59459\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0420 - accuracy: 0.9833 - val_loss: 1.5671 - val_accuracy: 0.5676\n",
      "Epoch 3/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0513 - accuracy: 0.9722\n",
      "Epoch 00003: val_accuracy did not improve from 0.59459\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0528 - accuracy: 0.9700 - val_loss: 1.5141 - val_accuracy: 0.5541\n",
      "Epoch 4/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0152 - accuracy: 1.0000\n",
      "Epoch 00004: val_accuracy did not improve from 0.59459\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0295 - accuracy: 0.9933 - val_loss: 1.5775 - val_accuracy: 0.5946\n",
      "Epoch 5/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0403 - accuracy: 0.9861\n",
      "Epoch 00005: val_accuracy did not improve from 0.59459\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0391 - accuracy: 0.9867 - val_loss: 1.6040 - val_accuracy: 0.5541\n",
      "Epoch 6/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0478 - accuracy: 0.9861\n",
      "Epoch 00006: val_accuracy did not improve from 0.59459\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0603 - accuracy: 0.9833 - val_loss: 1.7210 - val_accuracy: 0.5541\n",
      "Epoch 7/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.1015 - accuracy: 0.9653\n",
      "Epoch 00007: val_accuracy improved from 0.59459 to 0.64865, saving model to best_model.h5\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0984 - accuracy: 0.9667 - val_loss: 1.2919 - val_accuracy: 0.6486\n",
      "Epoch 8/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0450 - accuracy: 0.9861\n",
      "Epoch 00008: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0444 - accuracy: 0.9867 - val_loss: 1.2504 - val_accuracy: 0.6216\n",
      "Epoch 9/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0297 - accuracy: 0.9965\n",
      "Epoch 00009: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0302 - accuracy: 0.9967 - val_loss: 1.3900 - val_accuracy: 0.5270\n",
      "Epoch 10/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0179 - accuracy: 1.0000\n",
      "Epoch 00010: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0234 - accuracy: 0.9967 - val_loss: 1.4863 - val_accuracy: 0.5270\n",
      "Epoch 11/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0308 - accuracy: 0.9896\n",
      "Epoch 00011: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0297 - accuracy: 0.9900 - val_loss: 1.2961 - val_accuracy: 0.5811\n",
      "Epoch 12/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0240 - accuracy: 0.9931\n",
      "Epoch 00012: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0273 - accuracy: 0.9933 - val_loss: 1.3684 - val_accuracy: 0.5676\n",
      "Epoch 13/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0356 - accuracy: 0.9931\n",
      "Epoch 00013: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0435 - accuracy: 0.9900 - val_loss: 1.3888 - val_accuracy: 0.5270\n",
      "Epoch 14/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0330 - accuracy: 0.9861\n",
      "Epoch 00014: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0320 - accuracy: 0.9867 - val_loss: 1.3474 - val_accuracy: 0.5541\n",
      "Epoch 15/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0216 - accuracy: 0.9965\n",
      "Epoch 00015: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0211 - accuracy: 0.9967 - val_loss: 1.4109 - val_accuracy: 0.5811\n",
      "Epoch 16/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0128 - accuracy: 1.0000\n",
      "Epoch 00016: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0159 - accuracy: 0.9967 - val_loss: 1.6073 - val_accuracy: 0.5541\n",
      "Epoch 17/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0117 - accuracy: 1.0000\n",
      "Epoch 00017: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0115 - accuracy: 1.0000 - val_loss: 1.6593 - val_accuracy: 0.5541\n",
      "Epoch 18/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0201 - accuracy: 0.9896\n",
      "Epoch 00018: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0221 - accuracy: 0.9900 - val_loss: 1.7676 - val_accuracy: 0.5135\n",
      "Epoch 19/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0264 - accuracy: 0.9965\n",
      "Epoch 00019: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0257 - accuracy: 0.9967 - val_loss: 1.7415 - val_accuracy: 0.5541\n",
      "Epoch 20/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0172 - accuracy: 1.0000\n",
      "Epoch 00020: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0192 - accuracy: 1.0000 - val_loss: 1.9981 - val_accuracy: 0.5000\n",
      "Epoch 21/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0119 - accuracy: 1.0000\n",
      "Epoch 00021: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0119 - accuracy: 1.0000 - val_loss: 1.8349 - val_accuracy: 0.5000\n",
      "Epoch 22/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0100 - accuracy: 0.9965\n",
      "Epoch 00022: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0097 - accuracy: 0.9967 - val_loss: 1.7381 - val_accuracy: 0.4730\n",
      "Epoch 23/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0100 - accuracy: 0.9965\n",
      "Epoch 00023: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0129 - accuracy: 0.9933 - val_loss: 1.7318 - val_accuracy: 0.4865\n",
      "Epoch 24/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0227 - accuracy: 0.9896\n",
      "Epoch 00024: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0219 - accuracy: 0.9900 - val_loss: 1.6957 - val_accuracy: 0.5270\n",
      "Epoch 25/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0212 - accuracy: 0.9931\n",
      "Epoch 00025: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0233 - accuracy: 0.9933 - val_loss: 1.6777 - val_accuracy: 0.5270\n",
      "Epoch 26/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0109 - accuracy: 1.0000\n",
      "Epoch 00026: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0208 - accuracy: 0.9933 - val_loss: 1.7604 - val_accuracy: 0.5541\n",
      "Epoch 27/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0413 - accuracy: 0.9792\n",
      "Epoch 00027: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0421 - accuracy: 0.9800 - val_loss: 1.8459 - val_accuracy: 0.5405\n",
      "Epoch 28/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0552 - accuracy: 0.9722\n",
      "Epoch 00028: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0580 - accuracy: 0.9700 - val_loss: 1.6041 - val_accuracy: 0.5676\n",
      "Epoch 29/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0588 - accuracy: 0.9896\n",
      "Epoch 00029: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0580 - accuracy: 0.9900 - val_loss: 1.7967 - val_accuracy: 0.5270\n",
      "Epoch 30/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0449 - accuracy: 0.9896\n",
      "Epoch 00030: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0437 - accuracy: 0.9900 - val_loss: 1.9296 - val_accuracy: 0.5135\n",
      "Epoch 31/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0468 - accuracy: 0.9826\n",
      "Epoch 00031: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0497 - accuracy: 0.9800 - val_loss: 1.7557 - val_accuracy: 0.6081\n",
      "Epoch 32/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0205 - accuracy: 0.9965\n",
      "Epoch 00032: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0200 - accuracy: 0.9967 - val_loss: 2.1921 - val_accuracy: 0.5000\n",
      "Epoch 33/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0167 - accuracy: 1.0000\n",
      "Epoch 00033: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0169 - accuracy: 1.0000 - val_loss: 2.0379 - val_accuracy: 0.5000\n",
      "Epoch 34/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0136 - accuracy: 0.9965\n",
      "Epoch 00034: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0131 - accuracy: 0.9967 - val_loss: 1.8236 - val_accuracy: 0.4865\n",
      "Epoch 35/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0111 - accuracy: 0.9965\n",
      "Epoch 00035: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0108 - accuracy: 0.9967 - val_loss: 1.8322 - val_accuracy: 0.5135\n",
      "Epoch 36/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0098 - accuracy: 0.9965\n",
      "Epoch 00036: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0096 - accuracy: 0.9967 - val_loss: 1.8454 - val_accuracy: 0.5135\n",
      "Epoch 37/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 00037: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0201 - accuracy: 0.9933 - val_loss: 1.7531 - val_accuracy: 0.5270\n",
      "Epoch 00037: early stopping\n",
      "The value of model_types is set from ['CNN', 'DeepConvLSTM', 'ResNet', 'InceptionTime'] (default) to ['CNN']\n",
      "The value of metrics is set from ['accuracy'] (default) to ['accuracy']\n",
      "Set maximum kernel size for InceptionTime models to number of timesteps.\n",
      "Train on 300 samples, validate on 74 samples\n",
      "Epoch 1/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.3317 - accuracy: 0.9792\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.55405, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.3354 - accuracy: 0.9767 - val_loss: 1.4736 - val_accuracy: 0.5541\n",
      "Epoch 2/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.3233 - accuracy: 0.9861\n",
      "Epoch 00002: val_accuracy improved from 0.55405 to 0.62162, saving model to best_model.h5\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.3273 - accuracy: 0.9833 - val_loss: 1.3292 - val_accuracy: 0.6216\n",
      "Epoch 3/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.3138 - accuracy: 0.9861\n",
      "Epoch 00003: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 925us/sample - loss: 0.3142 - accuracy: 0.9867 - val_loss: 1.3663 - val_accuracy: 0.5000\n",
      "Epoch 4/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.3083 - accuracy: 0.9826\n",
      "Epoch 00004: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 950us/sample - loss: 0.3071 - accuracy: 0.9833 - val_loss: 1.3215 - val_accuracy: 0.5541\n",
      "Epoch 5/200\n",
      "256/300 [========================>.....] - ETA: 0s - loss: 0.2826 - accuracy: 0.9961\n",
      "Epoch 00005: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 915us/sample - loss: 0.2817 - accuracy: 0.9967 - val_loss: 1.2933 - val_accuracy: 0.5270\n",
      "Epoch 6/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.2659 - accuracy: 1.0000\n",
      "Epoch 00006: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 923us/sample - loss: 0.2656 - accuracy: 1.0000 - val_loss: 1.2342 - val_accuracy: 0.5946\n",
      "Epoch 7/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.2534 - accuracy: 1.0000\n",
      "Epoch 00007: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 925us/sample - loss: 0.2538 - accuracy: 1.0000 - val_loss: 1.2229 - val_accuracy: 0.5405\n",
      "Epoch 8/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.2491 - accuracy: 1.0000\n",
      "Epoch 00008: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 918us/sample - loss: 0.2473 - accuracy: 1.0000 - val_loss: 1.1853 - val_accuracy: 0.6081\n",
      "Epoch 9/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.2427 - accuracy: 1.0000\n",
      "Epoch 00009: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 938us/sample - loss: 0.2449 - accuracy: 1.0000 - val_loss: 1.1534 - val_accuracy: 0.6081\n",
      "Epoch 10/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.2408 - accuracy: 1.0000\n",
      "Epoch 00010: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 924us/sample - loss: 0.2416 - accuracy: 1.0000 - val_loss: 1.1789 - val_accuracy: 0.5000\n",
      "Epoch 11/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.2330 - accuracy: 1.0000\n",
      "Epoch 00011: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 896us/sample - loss: 0.2367 - accuracy: 0.9967 - val_loss: 1.1312 - val_accuracy: 0.5135\n",
      "Epoch 12/200\n",
      "256/300 [========================>.....] - ETA: 0s - loss: 0.2366 - accuracy: 0.9961\n",
      "Epoch 00012: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 908us/sample - loss: 0.2365 - accuracy: 0.9967 - val_loss: 1.1324 - val_accuracy: 0.5811\n",
      "Epoch 13/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.2351 - accuracy: 1.0000\n",
      "Epoch 00013: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 918us/sample - loss: 0.2355 - accuracy: 1.0000 - val_loss: 1.1957 - val_accuracy: 0.5405\n",
      "Epoch 14/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.2270 - accuracy: 1.0000\n",
      "Epoch 00014: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 929us/sample - loss: 0.2273 - accuracy: 1.0000 - val_loss: 1.1234 - val_accuracy: 0.5676\n",
      "Epoch 15/200\n",
      "256/300 [========================>.....] - ETA: 0s - loss: 0.2173 - accuracy: 1.0000\n",
      "Epoch 00015: val_accuracy improved from 0.62162 to 0.63514, saving model to best_model.h5\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.2183 - accuracy: 1.0000 - val_loss: 1.0617 - val_accuracy: 0.6351\n",
      "Epoch 16/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.2149 - accuracy: 1.0000\n",
      "Epoch 00016: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 934us/sample - loss: 0.2146 - accuracy: 1.0000 - val_loss: 1.0725 - val_accuracy: 0.5811\n",
      "Epoch 17/200\n",
      "256/300 [========================>.....] - ETA: 0s - loss: 0.2102 - accuracy: 1.0000\n",
      "Epoch 00017: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 908us/sample - loss: 0.2136 - accuracy: 1.0000 - val_loss: 1.0777 - val_accuracy: 0.6216\n",
      "Epoch 18/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.2128 - accuracy: 1.0000\n",
      "Epoch 00018: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 922us/sample - loss: 0.2121 - accuracy: 1.0000 - val_loss: 1.0176 - val_accuracy: 0.5541\n",
      "Epoch 19/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.2127 - accuracy: 0.9965\n",
      "Epoch 00019: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 917us/sample - loss: 0.2120 - accuracy: 0.9967 - val_loss: 1.0003 - val_accuracy: 0.6081\n",
      "Epoch 20/200\n",
      "256/300 [========================>.....] - ETA: 0s - loss: 0.1923 - accuracy: 1.0000\n",
      "Epoch 00020: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 901us/sample - loss: 0.1928 - accuracy: 1.0000 - val_loss: 1.0090 - val_accuracy: 0.6216\n",
      "Epoch 21/200\n",
      "256/300 [========================>.....] - ETA: 0s - loss: 0.1888 - accuracy: 1.0000\n",
      "Epoch 00021: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 911us/sample - loss: 0.1886 - accuracy: 1.0000 - val_loss: 0.9737 - val_accuracy: 0.5811\n",
      "Epoch 22/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.1855 - accuracy: 1.0000\n",
      "Epoch 00022: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 924us/sample - loss: 0.1863 - accuracy: 1.0000 - val_loss: 0.9749 - val_accuracy: 0.5946\n",
      "Epoch 23/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.1852 - accuracy: 1.0000\n",
      "Epoch 00023: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 929us/sample - loss: 0.1848 - accuracy: 1.0000 - val_loss: 0.9783 - val_accuracy: 0.6351\n",
      "Epoch 24/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1828 - accuracy: 1.0000\n",
      "Epoch 00024: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 915us/sample - loss: 0.1807 - accuracy: 1.0000 - val_loss: 0.9773 - val_accuracy: 0.6216\n",
      "Epoch 25/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.1802 - accuracy: 1.0000\n",
      "Epoch 00025: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 925us/sample - loss: 0.1814 - accuracy: 1.0000 - val_loss: 0.9247 - val_accuracy: 0.6081\n",
      "Epoch 26/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1745 - accuracy: 1.0000\n",
      "Epoch 00026: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 933us/sample - loss: 0.1748 - accuracy: 1.0000 - val_loss: 0.9885 - val_accuracy: 0.6081\n",
      "Epoch 27/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.1755 - accuracy: 1.0000\n",
      "Epoch 00027: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 925us/sample - loss: 0.1754 - accuracy: 1.0000 - val_loss: 1.0664 - val_accuracy: 0.5676\n",
      "Epoch 28/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.1711 - accuracy: 1.0000\n",
      "Epoch 00028: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 905us/sample - loss: 0.1737 - accuracy: 1.0000 - val_loss: 1.0481 - val_accuracy: 0.5676\n",
      "Epoch 29/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.1835 - accuracy: 0.9861\n",
      "Epoch 00029: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 925us/sample - loss: 0.1836 - accuracy: 0.9867 - val_loss: 0.9511 - val_accuracy: 0.6081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1769 - accuracy: 1.0000\n",
      "Epoch 00030: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 899us/sample - loss: 0.1755 - accuracy: 1.0000 - val_loss: 1.0580 - val_accuracy: 0.5270\n",
      "Epoch 31/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1666 - accuracy: 1.0000\n",
      "Epoch 00031: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 914us/sample - loss: 0.1696 - accuracy: 0.9967 - val_loss: 1.0739 - val_accuracy: 0.5135\n",
      "Epoch 32/200\n",
      "256/300 [========================>.....] - ETA: 0s - loss: 0.1670 - accuracy: 1.0000 ETA: 0s - loss: 0.1653 - accuracy: 1.\n",
      "Epoch 00032: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 932us/sample - loss: 0.1772 - accuracy: 0.9933 - val_loss: 1.0692 - val_accuracy: 0.5946\n",
      "Epoch 33/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.1997 - accuracy: 0.9826\n",
      "Epoch 00033: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 946us/sample - loss: 0.1984 - accuracy: 0.9833 - val_loss: 1.1057 - val_accuracy: 0.5811\n",
      "Epoch 34/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.1875 - accuracy: 0.9931\n",
      "Epoch 00034: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 937us/sample - loss: 0.1936 - accuracy: 0.9900 - val_loss: 1.2129 - val_accuracy: 0.5946\n",
      "Epoch 35/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.2247 - accuracy: 0.9821\n",
      "Epoch 00035: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 901us/sample - loss: 0.2332 - accuracy: 0.9767 - val_loss: 1.2690 - val_accuracy: 0.5946\n",
      "Epoch 36/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.3042 - accuracy: 0.9340\n",
      "Epoch 00036: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 930us/sample - loss: 0.2994 - accuracy: 0.9367 - val_loss: 1.2770 - val_accuracy: 0.5811\n",
      "Epoch 37/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.2702 - accuracy: 0.9688\n",
      "Epoch 00037: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 929us/sample - loss: 0.2755 - accuracy: 0.9633 - val_loss: 1.4789 - val_accuracy: 0.6351\n",
      "Epoch 38/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.3466 - accuracy: 0.9306\n",
      "Epoch 00038: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 926us/sample - loss: 0.3468 - accuracy: 0.9300 - val_loss: 2.4835 - val_accuracy: 0.5135\n",
      "Epoch 39/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.3492 - accuracy: 0.9444\n",
      "Epoch 00039: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 927us/sample - loss: 0.3590 - accuracy: 0.9400 - val_loss: 3.4720 - val_accuracy: 0.5270\n",
      "Epoch 40/200\n",
      "256/300 [========================>.....] - ETA: 0s - loss: 0.4842 - accuracy: 0.8867\n",
      "Epoch 00040: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 922us/sample - loss: 0.4703 - accuracy: 0.8933 - val_loss: 1.5053 - val_accuracy: 0.5405\n",
      "Epoch 41/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.4087 - accuracy: 0.9167\n",
      "Epoch 00041: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 927us/sample - loss: 0.4072 - accuracy: 0.9167 - val_loss: 2.7610 - val_accuracy: 0.5135\n",
      "Epoch 42/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.4054 - accuracy: 0.9097\n",
      "Epoch 00042: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 931us/sample - loss: 0.4049 - accuracy: 0.9100 - val_loss: 3.2162 - val_accuracy: 0.5811\n",
      "Epoch 43/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.3639 - accuracy: 0.9509\n",
      "Epoch 00043: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 907us/sample - loss: 0.3919 - accuracy: 0.9400 - val_loss: 1.9141 - val_accuracy: 0.6216\n",
      "Epoch 44/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.3598 - accuracy: 0.9583\n",
      "Epoch 00044: val_accuracy improved from 0.63514 to 0.66216, saving model to best_model.h5\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.3558 - accuracy: 0.9600 - val_loss: 2.2333 - val_accuracy: 0.6622\n",
      "Epoch 45/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.4125 - accuracy: 0.9107\n",
      "Epoch 00045: val_accuracy did not improve from 0.66216\n",
      "300/300 [==============================] - 0s 913us/sample - loss: 0.4188 - accuracy: 0.9067 - val_loss: 3.1073 - val_accuracy: 0.5946\n",
      "Epoch 46/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.3260 - accuracy: 0.9688\n",
      "Epoch 00046: val_accuracy did not improve from 0.66216\n",
      "300/300 [==============================] - 0s 938us/sample - loss: 0.3369 - accuracy: 0.9667 - val_loss: 2.7290 - val_accuracy: 0.6351\n",
      "Epoch 47/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.3509 - accuracy: 0.9583\n",
      "Epoch 00047: val_accuracy did not improve from 0.66216\n",
      "300/300 [==============================] - 0s 944us/sample - loss: 0.3550 - accuracy: 0.9567 - val_loss: 2.3981 - val_accuracy: 0.6216\n",
      "Epoch 48/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.3620 - accuracy: 0.9688\n",
      "Epoch 00048: val_accuracy did not improve from 0.66216\n",
      "300/300 [==============================] - 0s 924us/sample - loss: 0.3620 - accuracy: 0.9667 - val_loss: 3.0590 - val_accuracy: 0.5541\n",
      "Epoch 49/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.3450 - accuracy: 0.9549\n",
      "Epoch 00049: val_accuracy did not improve from 0.66216\n",
      "300/300 [==============================] - 0s 940us/sample - loss: 0.3424 - accuracy: 0.9567 - val_loss: 2.5214 - val_accuracy: 0.5811\n",
      "Epoch 50/200\n",
      "256/300 [========================>.....] - ETA: 0s - loss: 0.3155 - accuracy: 0.9766\n",
      "Epoch 00050: val_accuracy did not improve from 0.66216\n",
      "300/300 [==============================] - 0s 923us/sample - loss: 0.3092 - accuracy: 0.9800 - val_loss: 2.0624 - val_accuracy: 0.5676\n",
      "Epoch 51/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.2777 - accuracy: 0.9965\n",
      "Epoch 00051: val_accuracy did not improve from 0.66216\n",
      "300/300 [==============================] - 0s 932us/sample - loss: 0.2812 - accuracy: 0.9933 - val_loss: 1.8764 - val_accuracy: 0.5541\n",
      "Epoch 52/200\n",
      "256/300 [========================>.....] - ETA: 0s - loss: 0.2837 - accuracy: 0.9922\n",
      "Epoch 00052: val_accuracy did not improve from 0.66216\n",
      "300/300 [==============================] - 0s 909us/sample - loss: 0.2894 - accuracy: 0.9867 - val_loss: 1.6644 - val_accuracy: 0.6216\n",
      "Epoch 53/200\n",
      "256/300 [========================>.....] - ETA: 0s - loss: 0.2706 - accuracy: 0.9883\n",
      "Epoch 00053: val_accuracy did not improve from 0.66216\n",
      "300/300 [==============================] - 0s 928us/sample - loss: 0.2925 - accuracy: 0.9833 - val_loss: 1.7880 - val_accuracy: 0.5811\n",
      "Epoch 54/200\n",
      "256/300 [========================>.....] - ETA: 0s - loss: 0.3144 - accuracy: 0.9727\n",
      "Epoch 00054: val_accuracy did not improve from 0.66216\n",
      "300/300 [==============================] - 0s 928us/sample - loss: 0.3194 - accuracy: 0.9733 - val_loss: 1.6271 - val_accuracy: 0.5811\n",
      "Epoch 55/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.3340 - accuracy: 0.9643\n",
      "Epoch 00055: val_accuracy did not improve from 0.66216\n",
      "300/300 [==============================] - 0s 936us/sample - loss: 0.3656 - accuracy: 0.9500 - val_loss: 1.8378 - val_accuracy: 0.5135\n",
      "Epoch 56/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.3968 - accuracy: 0.9375\n",
      "Epoch 00056: val_accuracy did not improve from 0.66216\n",
      "300/300 [==============================] - 0s 910us/sample - loss: 0.4057 - accuracy: 0.9267 - val_loss: 2.3409 - val_accuracy: 0.5405\n",
      "Epoch 57/200\n",
      "256/300 [========================>.....] - ETA: 0s - loss: 0.3958 - accuracy: 0.9414\n",
      "Epoch 00057: val_accuracy did not improve from 0.66216\n",
      "300/300 [==============================] - 0s 901us/sample - loss: 0.3938 - accuracy: 0.9433 - val_loss: 1.9696 - val_accuracy: 0.5270\n",
      "Epoch 58/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.3713 - accuracy: 0.9514\n",
      "Epoch 00058: val_accuracy did not improve from 0.66216\n",
      "300/300 [==============================] - 0s 942us/sample - loss: 0.3676 - accuracy: 0.9533 - val_loss: 2.0682 - val_accuracy: 0.5541\n",
      "Epoch 59/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.3321 - accuracy: 0.9688\n",
      "Epoch 00059: val_accuracy did not improve from 0.66216\n",
      "300/300 [==============================] - 0s 910us/sample - loss: 0.3317 - accuracy: 0.9700 - val_loss: 2.0200 - val_accuracy: 0.5811\n",
      "Epoch 60/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.3070 - accuracy: 0.9931\n",
      "Epoch 00060: val_accuracy did not improve from 0.66216\n",
      "300/300 [==============================] - 0s 930us/sample - loss: 0.3066 - accuracy: 0.9933 - val_loss: 1.8924 - val_accuracy: 0.5541\n",
      "Epoch 61/200\n",
      "256/300 [========================>.....] - ETA: 0s - loss: 0.2726 - accuracy: 1.0000\n",
      "Epoch 00061: val_accuracy did not improve from 0.66216\n",
      "300/300 [==============================] - 0s 922us/sample - loss: 0.2748 - accuracy: 1.0000 - val_loss: 1.7403 - val_accuracy: 0.5811\n",
      "Epoch 62/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.2694 - accuracy: 0.9965\n",
      "Epoch 00062: val_accuracy did not improve from 0.66216\n",
      "300/300 [==============================] - 0s 956us/sample - loss: 0.2686 - accuracy: 0.9967 - val_loss: 1.6249 - val_accuracy: 0.5811\n",
      "Epoch 63/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.2550 - accuracy: 1.0000\n",
      "Epoch 00063: val_accuracy did not improve from 0.66216\n",
      "300/300 [==============================] - 0s 975us/sample - loss: 0.2603 - accuracy: 0.9967 - val_loss: 1.6136 - val_accuracy: 0.5405\n",
      "Epoch 64/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.2533 - accuracy: 0.9965\n",
      "Epoch 00064: val_accuracy did not improve from 0.66216\n",
      "300/300 [==============================] - 0s 981us/sample - loss: 0.2840 - accuracy: 0.9867 - val_loss: 1.6990 - val_accuracy: 0.5676\n",
      "Epoch 65/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.3227 - accuracy: 0.9688\n",
      "Epoch 00065: val_accuracy did not improve from 0.66216\n",
      "300/300 [==============================] - 0s 957us/sample - loss: 0.3196 - accuracy: 0.9700 - val_loss: 1.5338 - val_accuracy: 0.5676\n",
      "Epoch 66/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.2950 - accuracy: 0.9722\n",
      "Epoch 00066: val_accuracy did not improve from 0.66216\n",
      "300/300 [==============================] - 0s 935us/sample - loss: 0.3154 - accuracy: 0.9633 - val_loss: 1.3728 - val_accuracy: 0.5541\n",
      "Epoch 67/200\n",
      "256/300 [========================>.....] - ETA: 0s - loss: 0.3230 - accuracy: 0.9727\n",
      "Epoch 00067: val_accuracy did not improve from 0.66216\n",
      "300/300 [==============================] - 0s 922us/sample - loss: 0.3616 - accuracy: 0.9533 - val_loss: 1.4868 - val_accuracy: 0.5135\n",
      "Epoch 68/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.4598 - accuracy: 0.9236\n",
      "Epoch 00068: val_accuracy did not improve from 0.66216\n",
      "300/300 [==============================] - 0s 956us/sample - loss: 0.4557 - accuracy: 0.9233 - val_loss: 2.1273 - val_accuracy: 0.4324\n",
      "Epoch 69/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.4301 - accuracy: 0.9201\n",
      "Epoch 00069: val_accuracy did not improve from 0.66216\n",
      "300/300 [==============================] - 0s 942us/sample - loss: 0.4432 - accuracy: 0.9167 - val_loss: 2.1380 - val_accuracy: 0.5946\n",
      "Epoch 70/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.4437 - accuracy: 0.9375\n",
      "Epoch 00070: val_accuracy did not improve from 0.66216\n",
      "300/300 [==============================] - 0s 919us/sample - loss: 0.4297 - accuracy: 0.9433 - val_loss: 3.1161 - val_accuracy: 0.5270\n",
      "Epoch 71/200\n",
      "256/300 [========================>.....] - ETA: 0s - loss: 0.4124 - accuracy: 0.9414\n",
      "Epoch 00071: val_accuracy did not improve from 0.66216\n",
      "300/300 [==============================] - 0s 907us/sample - loss: 0.4038 - accuracy: 0.9467 - val_loss: 2.8726 - val_accuracy: 0.5811\n",
      "Epoch 72/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.3503 - accuracy: 0.9653\n",
      "Epoch 00072: val_accuracy did not improve from 0.66216\n",
      "300/300 [==============================] - 0s 930us/sample - loss: 0.3491 - accuracy: 0.9667 - val_loss: 2.1946 - val_accuracy: 0.5676\n",
      "Epoch 73/200\n",
      "256/300 [========================>.....] - ETA: 0s - loss: 0.3023 - accuracy: 0.9922\n",
      "Epoch 00073: val_accuracy did not improve from 0.66216\n",
      "300/300 [==============================] - 0s 926us/sample - loss: 0.3042 - accuracy: 0.9900 - val_loss: 2.0698 - val_accuracy: 0.5676\n",
      "Epoch 74/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.2967 - accuracy: 0.9896\n",
      "Epoch 00074: val_accuracy did not improve from 0.66216\n",
      "300/300 [==============================] - 0s 946us/sample - loss: 0.2988 - accuracy: 0.9900 - val_loss: 2.1099 - val_accuracy: 0.5135\n",
      "Epoch 00074: early stopping\n",
      "The value of model_types is set from ['CNN', 'DeepConvLSTM', 'ResNet', 'InceptionTime'] (default) to ['InceptionTime']\n",
      "The value of metrics is set from ['accuracy'] (default) to ['accuracy']\n",
      "Set maximum kernel size for InceptionTime models to number of timesteps.\n",
      "Train on 300 samples, validate on 74 samples\n",
      "Epoch 1/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5953 - accuracy: 0.6944\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.64865, saving model to best_model.h5\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6010 - accuracy: 0.6867 - val_loss: 0.6415 - val_accuracy: 0.6486\n",
      "Epoch 2/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5998 - accuracy: 0.7049\n",
      "Epoch 00002: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.5957 - accuracy: 0.7067 - val_loss: 0.6317 - val_accuracy: 0.6216\n",
      "Epoch 3/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6123 - accuracy: 0.6701\n",
      "Epoch 00003: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6179 - accuracy: 0.6600 - val_loss: 0.6552 - val_accuracy: 0.6351\n",
      "Epoch 4/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6053 - accuracy: 0.6597\n",
      "Epoch 00004: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 990us/sample - loss: 0.6027 - accuracy: 0.6633 - val_loss: 0.6258 - val_accuracy: 0.6486\n",
      "Epoch 5/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6106 - accuracy: 0.7083\n",
      "Epoch 00005: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6104 - accuracy: 0.7033 - val_loss: 0.6644 - val_accuracy: 0.6351\n",
      "Epoch 6/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6034 - accuracy: 0.6806\n",
      "Epoch 00006: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.5960 - accuracy: 0.6833 - val_loss: 0.6330 - val_accuracy: 0.6351\n",
      "Epoch 7/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6179 - accuracy: 0.6632\n",
      "Epoch 00007: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6178 - accuracy: 0.6667 - val_loss: 0.6701 - val_accuracy: 0.6351\n",
      "Epoch 8/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6208 - accuracy: 0.6771\n",
      "Epoch 00008: val_accuracy improved from 0.64865 to 0.67568, saving model to best_model.h5\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6159 - accuracy: 0.6833 - val_loss: 0.6359 - val_accuracy: 0.6757\n",
      "Epoch 9/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6331 - accuracy: 0.6806\n",
      "Epoch 00009: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 995us/sample - loss: 0.6324 - accuracy: 0.6800 - val_loss: 0.6544 - val_accuracy: 0.6216\n",
      "Epoch 10/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5978 - accuracy: 0.6736\n",
      "Epoch 00010: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.5984 - accuracy: 0.6733 - val_loss: 0.6273 - val_accuracy: 0.6351\n",
      "Epoch 11/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6426 - accuracy: 0.6632\n",
      "Epoch 00011: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6400 - accuracy: 0.6633 - val_loss: 0.6743 - val_accuracy: 0.5946\n",
      "Epoch 12/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6424 - accuracy: 0.6354\n",
      "Epoch 00012: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 999us/sample - loss: 0.6467 - accuracy: 0.6367 - val_loss: 0.6441 - val_accuracy: 0.5811\n",
      "Epoch 13/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6042 - accuracy: 0.6771\n",
      "Epoch 00013: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 997us/sample - loss: 0.6012 - accuracy: 0.6800 - val_loss: 0.6470 - val_accuracy: 0.6486\n",
      "Epoch 14/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5992 - accuracy: 0.6840\n",
      "Epoch 00014: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 945us/sample - loss: 0.5957 - accuracy: 0.6833 - val_loss: 0.6440 - val_accuracy: 0.5946\n",
      "Epoch 15/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6173 - accuracy: 0.6736\n",
      "Epoch 00015: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 997us/sample - loss: 0.6111 - accuracy: 0.6800 - val_loss: 0.6323 - val_accuracy: 0.6081\n",
      "Epoch 16/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6092 - accuracy: 0.6632\n",
      "Epoch 00016: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6066 - accuracy: 0.6700 - val_loss: 0.6289 - val_accuracy: 0.6351\n",
      "Epoch 17/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6168 - accuracy: 0.6597\n",
      "Epoch 00017: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6221 - accuracy: 0.6600 - val_loss: 0.6174 - val_accuracy: 0.6351\n",
      "Epoch 18/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6199 - accuracy: 0.6667\n",
      "Epoch 00018: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6255 - accuracy: 0.6533 - val_loss: 0.6382 - val_accuracy: 0.5811\n",
      "Epoch 19/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6002 - accuracy: 0.6875\n",
      "Epoch 00019: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6000 - accuracy: 0.6833 - val_loss: 0.6803 - val_accuracy: 0.6216\n",
      "Epoch 20/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6190 - accuracy: 0.6840\n",
      "Epoch 00020: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6117 - accuracy: 0.6900 - val_loss: 0.6458 - val_accuracy: 0.5946\n",
      "Epoch 21/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6256 - accuracy: 0.6319\n",
      "Epoch 00021: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6194 - accuracy: 0.6400 - val_loss: 0.6123 - val_accuracy: 0.6757\n",
      "Epoch 22/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6281 - accuracy: 0.6840\n",
      "Epoch 00022: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 999us/sample - loss: 0.6258 - accuracy: 0.6867 - val_loss: 0.6699 - val_accuracy: 0.6216\n",
      "Epoch 23/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6124 - accuracy: 0.6389\n",
      "Epoch 00023: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 997us/sample - loss: 0.6198 - accuracy: 0.6267 - val_loss: 0.6352 - val_accuracy: 0.6486\n",
      "Epoch 24/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6069 - accuracy: 0.6701\n",
      "Epoch 00024: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 965us/sample - loss: 0.6042 - accuracy: 0.6767 - val_loss: 0.6375 - val_accuracy: 0.6622\n",
      "Epoch 25/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5966 - accuracy: 0.6701\n",
      "Epoch 00025: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 971us/sample - loss: 0.6050 - accuracy: 0.6700 - val_loss: 0.6303 - val_accuracy: 0.6351\n",
      "Epoch 26/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6412 - accuracy: 0.6424\n",
      "Epoch 00026: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 961us/sample - loss: 0.6438 - accuracy: 0.6367 - val_loss: 0.5906 - val_accuracy: 0.6486\n",
      "Epoch 27/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6139 - accuracy: 0.6285\n",
      "Epoch 00027: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6139 - accuracy: 0.6267 - val_loss: 0.6122 - val_accuracy: 0.6622\n",
      "Epoch 28/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6038 - accuracy: 0.6701\n",
      "Epoch 00028: val_accuracy improved from 0.67568 to 0.68919, saving model to best_model.h5\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6061 - accuracy: 0.6700 - val_loss: 0.6306 - val_accuracy: 0.6892\n",
      "Epoch 29/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6090 - accuracy: 0.6667\n",
      "Epoch 00029: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 972us/sample - loss: 0.6148 - accuracy: 0.6533 - val_loss: 0.6400 - val_accuracy: 0.6486\n",
      "Epoch 30/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6006 - accuracy: 0.6875\n",
      "Epoch 00030: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6024 - accuracy: 0.6833 - val_loss: 0.6298 - val_accuracy: 0.6081\n",
      "Epoch 31/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6056 - accuracy: 0.6806\n",
      "Epoch 00031: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6156 - accuracy: 0.6700 - val_loss: 0.6326 - val_accuracy: 0.6216\n",
      "Epoch 32/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6193 - accuracy: 0.6493\n",
      "Epoch 00032: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6140 - accuracy: 0.6567 - val_loss: 0.6687 - val_accuracy: 0.6216\n",
      "Epoch 33/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5980 - accuracy: 0.7083\n",
      "Epoch 00033: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.5982 - accuracy: 0.6967 - val_loss: 0.6276 - val_accuracy: 0.6351\n",
      "Epoch 34/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6035 - accuracy: 0.6806\n",
      "Epoch 00034: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6020 - accuracy: 0.6867 - val_loss: 0.6354 - val_accuracy: 0.6216\n",
      "Epoch 35/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6152 - accuracy: 0.6458\n",
      "Epoch 00035: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6175 - accuracy: 0.6400 - val_loss: 0.6303 - val_accuracy: 0.6486\n",
      "Epoch 36/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6123 - accuracy: 0.6701\n",
      "Epoch 00036: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6096 - accuracy: 0.6700 - val_loss: 0.6244 - val_accuracy: 0.6216\n",
      "Epoch 37/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5985 - accuracy: 0.6806\n",
      "Epoch 00037: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.5998 - accuracy: 0.6767 - val_loss: 0.6366 - val_accuracy: 0.6216\n",
      "Epoch 38/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6026 - accuracy: 0.6771\n",
      "Epoch 00038: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6081 - accuracy: 0.6700 - val_loss: 0.6552 - val_accuracy: 0.5946\n",
      "Epoch 39/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5987 - accuracy: 0.6806\n",
      "Epoch 00039: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6028 - accuracy: 0.6800 - val_loss: 0.6274 - val_accuracy: 0.5946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5989 - accuracy: 0.6806\n",
      "Epoch 00040: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.5945 - accuracy: 0.6867 - val_loss: 0.6570 - val_accuracy: 0.6216\n",
      "Epoch 41/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5954 - accuracy: 0.6979\n",
      "Epoch 00041: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.5953 - accuracy: 0.7000 - val_loss: 0.6369 - val_accuracy: 0.6081\n",
      "Epoch 42/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6020 - accuracy: 0.6562\n",
      "Epoch 00042: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6089 - accuracy: 0.6500 - val_loss: 0.6310 - val_accuracy: 0.6351\n",
      "Epoch 43/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6049 - accuracy: 0.6736\n",
      "Epoch 00043: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6108 - accuracy: 0.6633 - val_loss: 0.6548 - val_accuracy: 0.5946\n",
      "Epoch 44/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6151 - accuracy: 0.6840\n",
      "Epoch 00044: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6162 - accuracy: 0.6833 - val_loss: 0.6258 - val_accuracy: 0.6216\n",
      "Epoch 45/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6254 - accuracy: 0.6424\n",
      "Epoch 00045: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 971us/sample - loss: 0.6239 - accuracy: 0.6467 - val_loss: 0.6311 - val_accuracy: 0.6081\n",
      "Epoch 46/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6213 - accuracy: 0.6562\n",
      "Epoch 00046: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6190 - accuracy: 0.6600 - val_loss: 0.6609 - val_accuracy: 0.5811\n",
      "Epoch 47/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6096 - accuracy: 0.6528\n",
      "Epoch 00047: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 992us/sample - loss: 0.6086 - accuracy: 0.6533 - val_loss: 0.6164 - val_accuracy: 0.6216\n",
      "Epoch 48/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6032 - accuracy: 0.6632\n",
      "Epoch 00048: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 987us/sample - loss: 0.6042 - accuracy: 0.6667 - val_loss: 0.6277 - val_accuracy: 0.6486\n",
      "Epoch 49/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5962 - accuracy: 0.7014\n",
      "Epoch 00049: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 999us/sample - loss: 0.5967 - accuracy: 0.6933 - val_loss: 0.6660 - val_accuracy: 0.6081\n",
      "Epoch 50/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6093 - accuracy: 0.6597\n",
      "Epoch 00050: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 986us/sample - loss: 0.6105 - accuracy: 0.6633 - val_loss: 0.6285 - val_accuracy: 0.6081\n",
      "Epoch 51/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6054 - accuracy: 0.6667\n",
      "Epoch 00051: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 987us/sample - loss: 0.6056 - accuracy: 0.6667 - val_loss: 0.6341 - val_accuracy: 0.6486\n",
      "Epoch 52/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6213 - accuracy: 0.6944\n",
      "Epoch 00052: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6268 - accuracy: 0.6800 - val_loss: 0.6380 - val_accuracy: 0.5946\n",
      "Epoch 53/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6207 - accuracy: 0.6319\n",
      "Epoch 00053: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6415 - accuracy: 0.6267 - val_loss: 0.6179 - val_accuracy: 0.6622\n",
      "Epoch 54/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5978 - accuracy: 0.6806\n",
      "Epoch 00054: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 992us/sample - loss: 0.6001 - accuracy: 0.6767 - val_loss: 0.6518 - val_accuracy: 0.6216\n",
      "Epoch 55/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5994 - accuracy: 0.6944\n",
      "Epoch 00055: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.5969 - accuracy: 0.7000 - val_loss: 0.6201 - val_accuracy: 0.6351\n",
      "Epoch 56/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6446 - accuracy: 0.6042\n",
      "Epoch 00056: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6387 - accuracy: 0.6167 - val_loss: 0.6846 - val_accuracy: 0.6351\n",
      "Epoch 57/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6136 - accuracy: 0.6632\n",
      "Epoch 00057: val_accuracy improved from 0.68919 to 0.70270, saving model to best_model.h5\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6115 - accuracy: 0.6600 - val_loss: 0.6189 - val_accuracy: 0.7027\n",
      "Epoch 58/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5998 - accuracy: 0.6632\n",
      "Epoch 00058: val_accuracy did not improve from 0.70270\n",
      "300/300 [==============================] - 0s 989us/sample - loss: 0.6046 - accuracy: 0.6633 - val_loss: 0.6315 - val_accuracy: 0.6486\n",
      "Epoch 59/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6060 - accuracy: 0.6771\n",
      "Epoch 00059: val_accuracy did not improve from 0.70270\n",
      "300/300 [==============================] - 0s 998us/sample - loss: 0.6101 - accuracy: 0.6700 - val_loss: 0.6277 - val_accuracy: 0.6486\n",
      "Epoch 60/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6086 - accuracy: 0.6528\n",
      "Epoch 00060: val_accuracy did not improve from 0.70270\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6096 - accuracy: 0.6567 - val_loss: 0.6221 - val_accuracy: 0.6486\n",
      "Epoch 61/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6163 - accuracy: 0.6632\n",
      "Epoch 00061: val_accuracy did not improve from 0.70270\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6138 - accuracy: 0.6633 - val_loss: 0.6329 - val_accuracy: 0.6081\n",
      "Epoch 62/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6070 - accuracy: 0.6632\n",
      "Epoch 00062: val_accuracy did not improve from 0.70270\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6024 - accuracy: 0.6733 - val_loss: 0.6209 - val_accuracy: 0.6757\n",
      "Epoch 63/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5972 - accuracy: 0.6979\n",
      "Epoch 00063: val_accuracy did not improve from 0.70270\n",
      "300/300 [==============================] - 0s 995us/sample - loss: 0.5977 - accuracy: 0.7000 - val_loss: 0.6192 - val_accuracy: 0.6081\n",
      "Epoch 64/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6151 - accuracy: 0.6771\n",
      "Epoch 00064: val_accuracy did not improve from 0.70270\n",
      "300/300 [==============================] - 0s 995us/sample - loss: 0.6127 - accuracy: 0.6800 - val_loss: 0.6699 - val_accuracy: 0.6081\n",
      "Epoch 65/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6699 - accuracy: 0.6493\n",
      "Epoch 00065: val_accuracy did not improve from 0.70270\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6645 - accuracy: 0.6567 - val_loss: 0.6845 - val_accuracy: 0.6351\n",
      "Epoch 66/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6204 - accuracy: 0.6215\n",
      "Epoch 00066: val_accuracy did not improve from 0.70270\n",
      "300/300 [==============================] - 0s 992us/sample - loss: 0.6269 - accuracy: 0.6200 - val_loss: 0.6224 - val_accuracy: 0.6486\n",
      "Epoch 67/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6078 - accuracy: 0.6667\n",
      "Epoch 00067: val_accuracy did not improve from 0.70270\n",
      "300/300 [==============================] - 0s 986us/sample - loss: 0.6035 - accuracy: 0.6700 - val_loss: 0.6442 - val_accuracy: 0.6216\n",
      "Epoch 68/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5978 - accuracy: 0.7049\n",
      "Epoch 00068: val_accuracy did not improve from 0.70270\n",
      "300/300 [==============================] - 0s 998us/sample - loss: 0.6020 - accuracy: 0.6967 - val_loss: 0.6290 - val_accuracy: 0.5946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6035 - accuracy: 0.6771\n",
      "Epoch 00069: val_accuracy did not improve from 0.70270\n",
      "300/300 [==============================] - 0s 991us/sample - loss: 0.6010 - accuracy: 0.6800 - val_loss: 0.6391 - val_accuracy: 0.5676\n",
      "Epoch 70/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5980 - accuracy: 0.6701\n",
      "Epoch 00070: val_accuracy did not improve from 0.70270\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6036 - accuracy: 0.6667 - val_loss: 0.6194 - val_accuracy: 0.6081\n",
      "Epoch 71/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5925 - accuracy: 0.7049\n",
      "Epoch 00071: val_accuracy did not improve from 0.70270\n",
      "300/300 [==============================] - 0s 990us/sample - loss: 0.5934 - accuracy: 0.7067 - val_loss: 0.6540 - val_accuracy: 0.6351\n",
      "Epoch 72/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6260 - accuracy: 0.6701\n",
      "Epoch 00072: val_accuracy did not improve from 0.70270\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6229 - accuracy: 0.6667 - val_loss: 0.6237 - val_accuracy: 0.6216\n",
      "Epoch 73/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6082 - accuracy: 0.6771\n",
      "Epoch 00073: val_accuracy did not improve from 0.70270\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6084 - accuracy: 0.6767 - val_loss: 0.6289 - val_accuracy: 0.5946\n",
      "Epoch 74/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6009 - accuracy: 0.6667\n",
      "Epoch 00074: val_accuracy did not improve from 0.70270\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6057 - accuracy: 0.6567 - val_loss: 0.6277 - val_accuracy: 0.6351\n",
      "Epoch 75/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6111 - accuracy: 0.6910\n",
      "Epoch 00075: val_accuracy did not improve from 0.70270\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6077 - accuracy: 0.6933 - val_loss: 0.6510 - val_accuracy: 0.6081\n",
      "Epoch 76/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5955 - accuracy: 0.6806\n",
      "Epoch 00076: val_accuracy did not improve from 0.70270\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.5938 - accuracy: 0.6800 - val_loss: 0.6106 - val_accuracy: 0.6486\n",
      "Epoch 77/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6225 - accuracy: 0.6667\n",
      "Epoch 00077: val_accuracy did not improve from 0.70270\n",
      "300/300 [==============================] - 0s 991us/sample - loss: 0.6183 - accuracy: 0.6667 - val_loss: 0.6254 - val_accuracy: 0.6351\n",
      "Epoch 78/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6218 - accuracy: 0.6736\n",
      "Epoch 00078: val_accuracy did not improve from 0.70270\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6206 - accuracy: 0.6700 - val_loss: 0.6652 - val_accuracy: 0.6216\n",
      "Epoch 79/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6435 - accuracy: 0.6250\n",
      "Epoch 00079: val_accuracy did not improve from 0.70270\n",
      "300/300 [==============================] - 0s 996us/sample - loss: 0.6388 - accuracy: 0.6300 - val_loss: 0.6163 - val_accuracy: 0.6486\n",
      "Epoch 80/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6303 - accuracy: 0.6771\n",
      "Epoch 00080: val_accuracy did not improve from 0.70270\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6245 - accuracy: 0.6767 - val_loss: 0.6489 - val_accuracy: 0.6351\n",
      "Epoch 81/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6128 - accuracy: 0.6458\n",
      "Epoch 00081: val_accuracy did not improve from 0.70270\n",
      "300/300 [==============================] - 0s 975us/sample - loss: 0.6118 - accuracy: 0.6467 - val_loss: 0.6121 - val_accuracy: 0.7027\n",
      "Epoch 82/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6048 - accuracy: 0.6806\n",
      "Epoch 00082: val_accuracy did not improve from 0.70270\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6005 - accuracy: 0.6833 - val_loss: 0.6902 - val_accuracy: 0.6081\n",
      "Epoch 83/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6290 - accuracy: 0.6771\n",
      "Epoch 00083: val_accuracy did not improve from 0.70270\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6264 - accuracy: 0.6767 - val_loss: 0.6118 - val_accuracy: 0.6486\n",
      "Epoch 84/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5913 - accuracy: 0.7188\n",
      "Epoch 00084: val_accuracy did not improve from 0.70270\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.5903 - accuracy: 0.7133 - val_loss: 0.6421 - val_accuracy: 0.6081\n",
      "Epoch 85/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6117 - accuracy: 0.6597\n",
      "Epoch 00085: val_accuracy did not improve from 0.70270\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6080 - accuracy: 0.6633 - val_loss: 0.6433 - val_accuracy: 0.6351\n",
      "Epoch 86/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6396 - accuracy: 0.6181\n",
      "Epoch 00086: val_accuracy did not improve from 0.70270\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6367 - accuracy: 0.6200 - val_loss: 0.6203 - val_accuracy: 0.6216\n",
      "Epoch 87/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6454 - accuracy: 0.6007\n",
      "Epoch 00087: val_accuracy did not improve from 0.70270\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6420 - accuracy: 0.6067 - val_loss: 0.6128 - val_accuracy: 0.6757\n",
      "Epoch 00087: early stopping\n",
      "The value of model_types is set from ['CNN', 'DeepConvLSTM', 'ResNet', 'InceptionTime'] (default) to ['CNN']\n",
      "The value of metrics is set from ['accuracy'] (default) to ['accuracy']\n",
      "Set maximum kernel size for InceptionTime models to number of timesteps.\n",
      "Train on 300 samples, validate on 74 samples\n",
      "Epoch 1/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.7236 - accuracy: 0.6771\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.60811, saving model to best_model.h5\n",
      "300/300 [==============================] - 0s 2ms/sample - loss: 0.7267 - accuracy: 0.6733 - val_loss: 0.9713 - val_accuracy: 0.6081\n",
      "Epoch 2/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6359 - accuracy: 0.7222\n",
      "Epoch 00002: val_accuracy did not improve from 0.60811\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6429 - accuracy: 0.7133 - val_loss: 0.7483 - val_accuracy: 0.5270\n",
      "Epoch 3/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6038 - accuracy: 0.7535\n",
      "Epoch 00003: val_accuracy did not improve from 0.60811\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6076 - accuracy: 0.7500 - val_loss: 0.7323 - val_accuracy: 0.5676\n",
      "Epoch 4/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6237 - accuracy: 0.7188\n",
      "Epoch 00004: val_accuracy did not improve from 0.60811\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6204 - accuracy: 0.7167 - val_loss: 0.7638 - val_accuracy: 0.5135\n",
      "Epoch 5/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5654 - accuracy: 0.7917\n",
      "Epoch 00005: val_accuracy did not improve from 0.60811\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.5668 - accuracy: 0.7933 - val_loss: 0.9326 - val_accuracy: 0.5135\n",
      "Epoch 6/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5529 - accuracy: 0.7951\n",
      "Epoch 00006: val_accuracy did not improve from 0.60811\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.5518 - accuracy: 0.8000 - val_loss: 0.8975 - val_accuracy: 0.4730\n",
      "Epoch 7/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5329 - accuracy: 0.7882\n",
      "Epoch 00007: val_accuracy did not improve from 0.60811\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.5459 - accuracy: 0.7800 - val_loss: 0.8924 - val_accuracy: 0.5000\n",
      "Epoch 8/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5335 - accuracy: 0.7951\n",
      "Epoch 00008: val_accuracy did not improve from 0.60811\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.5335 - accuracy: 0.7967 - val_loss: 0.8785 - val_accuracy: 0.5135\n",
      "Epoch 9/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5971 - accuracy: 0.7222\n",
      "Epoch 00009: val_accuracy did not improve from 0.60811\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.5967 - accuracy: 0.7200 - val_loss: 0.8855 - val_accuracy: 0.5135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5803 - accuracy: 0.7431\n",
      "Epoch 00010: val_accuracy did not improve from 0.60811\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.5968 - accuracy: 0.7400 - val_loss: 0.7785 - val_accuracy: 0.5135\n",
      "Epoch 11/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5158 - accuracy: 0.7986\n",
      "Epoch 00011: val_accuracy did not improve from 0.60811\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.5114 - accuracy: 0.8067 - val_loss: 0.7547 - val_accuracy: 0.5541\n",
      "Epoch 12/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.4870 - accuracy: 0.8576\n",
      "Epoch 00012: val_accuracy did not improve from 0.60811\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.4850 - accuracy: 0.8600 - val_loss: 0.8087 - val_accuracy: 0.5676\n",
      "Epoch 13/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.4686 - accuracy: 0.8264\n",
      "Epoch 00013: val_accuracy did not improve from 0.60811\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.4713 - accuracy: 0.8167 - val_loss: 0.8567 - val_accuracy: 0.5405\n",
      "Epoch 14/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.4739 - accuracy: 0.8125\n",
      "Epoch 00014: val_accuracy did not improve from 0.60811\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.4760 - accuracy: 0.8133 - val_loss: 0.7944 - val_accuracy: 0.5405\n",
      "Epoch 15/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5142 - accuracy: 0.7986\n",
      "Epoch 00015: val_accuracy did not improve from 0.60811\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.5073 - accuracy: 0.8033 - val_loss: 1.0599 - val_accuracy: 0.5000\n",
      "Epoch 16/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5327 - accuracy: 0.7847\n",
      "Epoch 00016: val_accuracy improved from 0.60811 to 0.63514, saving model to best_model.h5\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.5357 - accuracy: 0.7800 - val_loss: 0.7989 - val_accuracy: 0.6351\n",
      "Epoch 17/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5075 - accuracy: 0.8194\n",
      "Epoch 00017: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.5045 - accuracy: 0.8233 - val_loss: 1.1172 - val_accuracy: 0.5135\n",
      "Epoch 18/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5455 - accuracy: 0.7778\n",
      "Epoch 00018: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.5485 - accuracy: 0.7733 - val_loss: 0.9413 - val_accuracy: 0.5270\n",
      "Epoch 19/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.4797 - accuracy: 0.8125\n",
      "Epoch 00019: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.4857 - accuracy: 0.8067 - val_loss: 0.8495 - val_accuracy: 0.5405\n",
      "Epoch 20/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6338 - accuracy: 0.7535\n",
      "Epoch 00020: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6271 - accuracy: 0.7600 - val_loss: 1.1806 - val_accuracy: 0.5000\n",
      "Epoch 21/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5490 - accuracy: 0.7778\n",
      "Epoch 00021: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.5501 - accuracy: 0.7800 - val_loss: 0.8348 - val_accuracy: 0.5405\n",
      "Epoch 22/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5200 - accuracy: 0.8021\n",
      "Epoch 00022: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.5169 - accuracy: 0.7967 - val_loss: 0.9334 - val_accuracy: 0.5405\n",
      "Epoch 23/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5567 - accuracy: 0.7604\n",
      "Epoch 00023: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.5555 - accuracy: 0.7633 - val_loss: 1.0306 - val_accuracy: 0.4730\n",
      "Epoch 24/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5473 - accuracy: 0.7951\n",
      "Epoch 00024: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.5447 - accuracy: 0.7933 - val_loss: 1.0398 - val_accuracy: 0.6216\n",
      "Epoch 25/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.4834 - accuracy: 0.8229\n",
      "Epoch 00025: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.4924 - accuracy: 0.8200 - val_loss: 1.0266 - val_accuracy: 0.5405\n",
      "Epoch 26/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.4528 - accuracy: 0.8542\n",
      "Epoch 00026: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.4486 - accuracy: 0.8600 - val_loss: 0.8899 - val_accuracy: 0.5541\n",
      "Epoch 27/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.4325 - accuracy: 0.8611\n",
      "Epoch 00027: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.4340 - accuracy: 0.8567 - val_loss: 0.8983 - val_accuracy: 0.5270\n",
      "Epoch 28/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5304 - accuracy: 0.7986\n",
      "Epoch 00028: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.5288 - accuracy: 0.8000 - val_loss: 0.8497 - val_accuracy: 0.5946\n",
      "Epoch 29/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.4640 - accuracy: 0.8264\n",
      "Epoch 00029: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.4710 - accuracy: 0.8200 - val_loss: 1.0274 - val_accuracy: 0.5676\n",
      "Epoch 30/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5822 - accuracy: 0.7604\n",
      "Epoch 00030: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6008 - accuracy: 0.7533 - val_loss: 0.9416 - val_accuracy: 0.5676\n",
      "Epoch 31/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6182 - accuracy: 0.7465\n",
      "Epoch 00031: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6139 - accuracy: 0.7500 - val_loss: 1.2914 - val_accuracy: 0.4865\n",
      "Epoch 32/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6579 - accuracy: 0.7257\n",
      "Epoch 00032: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6618 - accuracy: 0.7267 - val_loss: 1.4207 - val_accuracy: 0.5135\n",
      "Epoch 33/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5848 - accuracy: 0.7951\n",
      "Epoch 00033: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.5838 - accuracy: 0.7967 - val_loss: 2.2967 - val_accuracy: 0.4865\n",
      "Epoch 34/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5082 - accuracy: 0.8194\n",
      "Epoch 00034: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.5084 - accuracy: 0.8200 - val_loss: 1.4565 - val_accuracy: 0.4730\n",
      "Epoch 35/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.4904 - accuracy: 0.8333\n",
      "Epoch 00035: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.4876 - accuracy: 0.8367 - val_loss: 1.3397 - val_accuracy: 0.4730\n",
      "Epoch 36/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.4837 - accuracy: 0.8403\n",
      "Epoch 00036: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.4912 - accuracy: 0.8300 - val_loss: 0.9103 - val_accuracy: 0.4865\n",
      "Epoch 37/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5030 - accuracy: 0.8194\n",
      "Epoch 00037: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.5052 - accuracy: 0.8167 - val_loss: 0.9118 - val_accuracy: 0.4595\n",
      "Epoch 38/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.4562 - accuracy: 0.8472\n",
      "Epoch 00038: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.4590 - accuracy: 0.8500 - val_loss: 0.8889 - val_accuracy: 0.5405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.4674 - accuracy: 0.8333\n",
      "Epoch 00039: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.4676 - accuracy: 0.8367 - val_loss: 0.8859 - val_accuracy: 0.5405\n",
      "Epoch 40/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.4680 - accuracy: 0.8194\n",
      "Epoch 00040: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.4686 - accuracy: 0.8200 - val_loss: 0.8711 - val_accuracy: 0.5405\n",
      "Epoch 41/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.4720 - accuracy: 0.8299\n",
      "Epoch 00041: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.4815 - accuracy: 0.8233 - val_loss: 0.8932 - val_accuracy: 0.5135\n",
      "Epoch 42/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6059 - accuracy: 0.7604\n",
      "Epoch 00042: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6184 - accuracy: 0.7567 - val_loss: 1.0124 - val_accuracy: 0.5811\n",
      "Epoch 43/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6427 - accuracy: 0.7431\n",
      "Epoch 00043: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6502 - accuracy: 0.7333 - val_loss: 2.3450 - val_accuracy: 0.5405\n",
      "Epoch 44/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6336 - accuracy: 0.7465\n",
      "Epoch 00044: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6301 - accuracy: 0.7500 - val_loss: 1.7562 - val_accuracy: 0.4730\n",
      "Epoch 45/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5526 - accuracy: 0.7708\n",
      "Epoch 00045: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.5442 - accuracy: 0.7767 - val_loss: 0.8770 - val_accuracy: 0.5405\n",
      "Epoch 46/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.4933 - accuracy: 0.8264\n",
      "Epoch 00046: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.4950 - accuracy: 0.8267 - val_loss: 1.1546 - val_accuracy: 0.4459\n",
      "Epoch 00046: early stopping\n",
      "The value of model_types is set from ['CNN', 'DeepConvLSTM', 'ResNet', 'InceptionTime'] (default) to ['InceptionTime']\n",
      "The value of metrics is set from ['accuracy'] (default) to ['accuracy']\n",
      "Set maximum kernel size for InceptionTime models to number of timesteps.\n",
      "Train on 300 samples, validate on 74 samples\n",
      "Epoch 1/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0959 - accuracy: 0.9653\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.62162, saving model to best_model.h5\n",
      "300/300 [==============================] - 1s 3ms/sample - loss: 0.0947 - accuracy: 0.9667 - val_loss: 1.1482 - val_accuracy: 0.6216\n",
      "Epoch 2/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0595 - accuracy: 0.9792\n",
      "Epoch 00002: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0576 - accuracy: 0.9800 - val_loss: 1.1748 - val_accuracy: 0.5541\n",
      "Epoch 3/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0509 - accuracy: 0.9896\n",
      "Epoch 00003: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0500 - accuracy: 0.9900 - val_loss: 1.2347 - val_accuracy: 0.5270\n",
      "Epoch 4/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0210 - accuracy: 0.9965\n",
      "Epoch 00004: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0208 - accuracy: 0.9967 - val_loss: 1.2502 - val_accuracy: 0.5405\n",
      "Epoch 5/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0161 - accuracy: 1.0000\n",
      "Epoch 00005: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0158 - accuracy: 1.0000 - val_loss: 1.2326 - val_accuracy: 0.5946\n",
      "Epoch 6/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0126 - accuracy: 1.0000\n",
      "Epoch 00006: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0130 - accuracy: 1.0000 - val_loss: 1.4284 - val_accuracy: 0.5135\n",
      "Epoch 7/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0175 - accuracy: 0.9931\n",
      "Epoch 00007: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0178 - accuracy: 0.9933 - val_loss: 1.2229 - val_accuracy: 0.6081\n",
      "Epoch 8/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0144 - accuracy: 0.9965\n",
      "Epoch 00008: val_accuracy improved from 0.62162 to 0.63514, saving model to best_model.h5\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0141 - accuracy: 0.9967 - val_loss: 1.3748 - val_accuracy: 0.6351\n",
      "Epoch 9/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0060 - accuracy: 1.0000\n",
      "Epoch 00009: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0098 - accuracy: 0.9967 - val_loss: 1.5874 - val_accuracy: 0.5811\n",
      "Epoch 10/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0149 - accuracy: 0.9965\n",
      "Epoch 00010: val_accuracy did not improve from 0.63514\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0145 - accuracy: 0.9967 - val_loss: 1.4477 - val_accuracy: 0.5946\n",
      "Epoch 11/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0150 - accuracy: 0.9965\n",
      "Epoch 00011: val_accuracy improved from 0.63514 to 0.67568, saving model to best_model.h5\n",
      "300/300 [==============================] - 1s 3ms/sample - loss: 0.0148 - accuracy: 0.9967 - val_loss: 1.2430 - val_accuracy: 0.6757\n",
      "Epoch 12/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 00012: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0050 - accuracy: 1.0000 - val_loss: 1.7617 - val_accuracy: 0.5405\n",
      "Epoch 13/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0105 - accuracy: 0.9965\n",
      "Epoch 00013: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0107 - accuracy: 0.9967 - val_loss: 1.4145 - val_accuracy: 0.6216\n",
      "Epoch 14/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 00014: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0062 - accuracy: 1.0000 - val_loss: 1.5256 - val_accuracy: 0.6622\n",
      "Epoch 15/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0068 - accuracy: 0.9965\n",
      "Epoch 00015: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0066 - accuracy: 0.9967 - val_loss: 1.7024 - val_accuracy: 0.5541\n",
      "Epoch 16/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0083 - accuracy: 0.9965\n",
      "Epoch 00016: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0081 - accuracy: 0.9967 - val_loss: 1.4623 - val_accuracy: 0.5676\n",
      "Epoch 17/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 00017: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0059 - accuracy: 1.0000 - val_loss: 1.3176 - val_accuracy: 0.5811\n",
      "Epoch 18/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 00018: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0106 - accuracy: 0.9967 - val_loss: 1.9237 - val_accuracy: 0.5811\n",
      "Epoch 19/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.1213 - accuracy: 0.9757\n",
      "Epoch 00019: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.1271 - accuracy: 0.9700 - val_loss: 2.0417 - val_accuracy: 0.5676\n",
      "Epoch 20/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.1614 - accuracy: 0.9410\n",
      "Epoch 00020: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.2010 - accuracy: 0.9300 - val_loss: 3.1599 - val_accuracy: 0.5135\n",
      "Epoch 21/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.1641 - accuracy: 0.9479\n",
      "Epoch 00021: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.1630 - accuracy: 0.9467 - val_loss: 2.4911 - val_accuracy: 0.5676\n",
      "Epoch 22/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.1276 - accuracy: 0.9444\n",
      "Epoch 00022: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.1352 - accuracy: 0.9433 - val_loss: 1.7706 - val_accuracy: 0.6216\n",
      "Epoch 23/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0660 - accuracy: 0.9896\n",
      "Epoch 00023: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0651 - accuracy: 0.9900 - val_loss: 1.9908 - val_accuracy: 0.4865\n",
      "Epoch 24/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0483 - accuracy: 0.9896\n",
      "Epoch 00024: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0471 - accuracy: 0.9900 - val_loss: 1.6160 - val_accuracy: 0.5676\n",
      "Epoch 25/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0313 - accuracy: 0.9965\n",
      "Epoch 00025: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0367 - accuracy: 0.9933 - val_loss: 1.6438 - val_accuracy: 0.5405\n",
      "Epoch 26/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0236 - accuracy: 0.9896\n",
      "Epoch 00026: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0233 - accuracy: 0.9900 - val_loss: 1.5610 - val_accuracy: 0.5676\n",
      "Epoch 27/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0216 - accuracy: 0.9965\n",
      "Epoch 00027: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0267 - accuracy: 0.9933 - val_loss: 1.4855 - val_accuracy: 0.5946\n",
      "Epoch 28/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0556 - accuracy: 0.9826\n",
      "Epoch 00028: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0551 - accuracy: 0.9833 - val_loss: 1.3788 - val_accuracy: 0.6081\n",
      "Epoch 29/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0263 - accuracy: 0.9896\n",
      "Epoch 00029: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0268 - accuracy: 0.9900 - val_loss: 1.4104 - val_accuracy: 0.5946\n",
      "Epoch 30/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0171 - accuracy: 1.0000\n",
      "Epoch 00030: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0271 - accuracy: 0.9967 - val_loss: 1.5317 - val_accuracy: 0.5676\n",
      "Epoch 31/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0375 - accuracy: 0.9861\n",
      "Epoch 00031: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0370 - accuracy: 0.9867 - val_loss: 1.8318 - val_accuracy: 0.5676\n",
      "Epoch 32/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0218 - accuracy: 0.9931\n",
      "Epoch 00032: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0212 - accuracy: 0.9933 - val_loss: 1.7782 - val_accuracy: 0.5135\n",
      "Epoch 33/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0209 - accuracy: 0.9931\n",
      "Epoch 00033: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0202 - accuracy: 0.9933 - val_loss: 1.8468 - val_accuracy: 0.5676\n",
      "Epoch 34/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 00034: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0187 - accuracy: 0.9967 - val_loss: 2.1072 - val_accuracy: 0.5405\n",
      "Epoch 35/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0285 - accuracy: 0.9896\n",
      "Epoch 00035: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0275 - accuracy: 0.9900 - val_loss: 1.9514 - val_accuracy: 0.6216\n",
      "Epoch 36/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0205 - accuracy: 0.9931\n",
      "Epoch 00036: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0202 - accuracy: 0.9933 - val_loss: 1.9291 - val_accuracy: 0.5541\n",
      "Epoch 37/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0313 - accuracy: 0.9896\n",
      "Epoch 00037: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0305 - accuracy: 0.9900 - val_loss: 1.5970 - val_accuracy: 0.5541\n",
      "Epoch 38/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0432 - accuracy: 0.9861\n",
      "Epoch 00038: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0426 - accuracy: 0.9867 - val_loss: 1.8601 - val_accuracy: 0.4865\n",
      "Epoch 39/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0484 - accuracy: 0.9826\n",
      "Epoch 00039: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0466 - accuracy: 0.9833 - val_loss: 1.7814 - val_accuracy: 0.5811\n",
      "Epoch 40/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0425 - accuracy: 0.9792\n",
      "Epoch 00040: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0414 - accuracy: 0.9800 - val_loss: 1.8289 - val_accuracy: 0.5000\n",
      "Epoch 41/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0599 - accuracy: 0.9757\n",
      "Epoch 00041: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0607 - accuracy: 0.9767 - val_loss: 2.0285 - val_accuracy: 0.5811\n",
      "Epoch 00041: early stopping\n",
      "The value of model_types is set from ['CNN', 'DeepConvLSTM', 'ResNet', 'InceptionTime'] (default) to ['CNN']\n",
      "The value of metrics is set from ['accuracy'] (default) to ['accuracy']\n",
      "Set maximum kernel size for InceptionTime models to number of timesteps.\n",
      "Train on 300 samples, validate on 74 samples\n",
      "Epoch 1/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1536 - accuracy: 0.9955\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.60811, saving model to best_model.h5\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.1524 - accuracy: 0.9967 - val_loss: 0.9647 - val_accuracy: 0.6081\n",
      "Epoch 2/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1709 - accuracy: 0.9955\n",
      "Epoch 00002: val_accuracy did not improve from 0.60811\n",
      "300/300 [==============================] - 0s 811us/sample - loss: 0.1735 - accuracy: 0.9933 - val_loss: 1.0171 - val_accuracy: 0.5811\n",
      "Epoch 3/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1543 - accuracy: 0.9911\n",
      "Epoch 00003: val_accuracy improved from 0.60811 to 0.64865, saving model to best_model.h5\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.1717 - accuracy: 0.9900 - val_loss: 0.9067 - val_accuracy: 0.6486\n",
      "Epoch 4/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1693 - accuracy: 0.9911\n",
      "Epoch 00004: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 832us/sample - loss: 0.1774 - accuracy: 0.9867 - val_loss: 0.9347 - val_accuracy: 0.6486\n",
      "Epoch 5/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1540 - accuracy: 1.0000\n",
      "Epoch 00005: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 838us/sample - loss: 0.1593 - accuracy: 1.0000 - val_loss: 0.9917 - val_accuracy: 0.6486\n",
      "Epoch 6/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1423 - accuracy: 1.0000\n",
      "Epoch 00006: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 815us/sample - loss: 0.1684 - accuracy: 0.9900 - val_loss: 0.9670 - val_accuracy: 0.5676\n",
      "Epoch 7/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1673 - accuracy: 0.9866\n",
      "Epoch 00007: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 814us/sample - loss: 0.1678 - accuracy: 0.9867 - val_loss: 1.0548 - val_accuracy: 0.5811\n",
      "Epoch 8/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1557 - accuracy: 1.0000\n",
      "Epoch 00008: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 809us/sample - loss: 0.1535 - accuracy: 1.0000 - val_loss: 1.0278 - val_accuracy: 0.6351\n",
      "Epoch 9/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1499 - accuracy: 1.0000\n",
      "Epoch 00009: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 810us/sample - loss: 0.1676 - accuracy: 0.9900 - val_loss: 0.9900 - val_accuracy: 0.6351\n",
      "Epoch 10/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1453 - accuracy: 1.0000\n",
      "Epoch 00010: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 799us/sample - loss: 0.1503 - accuracy: 0.9967 - val_loss: 1.0429 - val_accuracy: 0.6081\n",
      "Epoch 11/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1415 - accuracy: 1.0000\n",
      "Epoch 00011: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 818us/sample - loss: 0.1549 - accuracy: 0.9967 - val_loss: 1.0076 - val_accuracy: 0.5946\n",
      "Epoch 12/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1518 - accuracy: 0.9866\n",
      "Epoch 00012: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 864us/sample - loss: 0.1618 - accuracy: 0.9833 - val_loss: 0.9612 - val_accuracy: 0.6216\n",
      "Epoch 13/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1493 - accuracy: 0.9955\n",
      "Epoch 00013: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 850us/sample - loss: 0.1542 - accuracy: 0.9933 - val_loss: 1.1075 - val_accuracy: 0.6216\n",
      "Epoch 14/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1676 - accuracy: 0.9911\n",
      "Epoch 00014: val_accuracy did not improve from 0.64865\n",
      "300/300 [==============================] - 0s 819us/sample - loss: 0.1792 - accuracy: 0.9800 - val_loss: 1.1204 - val_accuracy: 0.5676\n",
      "Epoch 15/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1826 - accuracy: 0.9821\n",
      "Epoch 00015: val_accuracy improved from 0.64865 to 0.66216, saving model to best_model.h5\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.1827 - accuracy: 0.9867 - val_loss: 1.0820 - val_accuracy: 0.6622\n",
      "Epoch 16/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1810 - accuracy: 0.9821\n",
      "Epoch 00016: val_accuracy improved from 0.66216 to 0.67568, saving model to best_model.h5\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.1942 - accuracy: 0.9800 - val_loss: 0.9474 - val_accuracy: 0.6757\n",
      "Epoch 17/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1740 - accuracy: 0.9866\n",
      "Epoch 00017: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 852us/sample - loss: 0.1904 - accuracy: 0.9733 - val_loss: 0.9804 - val_accuracy: 0.6351\n",
      "Epoch 18/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1648 - accuracy: 0.9911\n",
      "Epoch 00018: val_accuracy did not improve from 0.67568\n",
      "300/300 [==============================] - 0s 855us/sample - loss: 0.1747 - accuracy: 0.9833 - val_loss: 1.0859 - val_accuracy: 0.6622\n",
      "Epoch 19/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1927 - accuracy: 0.9821\n",
      "Epoch 00019: val_accuracy improved from 0.67568 to 0.68919, saving model to best_model.h5\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.1885 - accuracy: 0.9800 - val_loss: 1.0796 - val_accuracy: 0.6892\n",
      "Epoch 20/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1634 - accuracy: 0.9955\n",
      "Epoch 00020: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 800us/sample - loss: 0.1718 - accuracy: 0.9900 - val_loss: 0.9720 - val_accuracy: 0.6757\n",
      "Epoch 21/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1736 - accuracy: 0.9777\n",
      "Epoch 00021: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 852us/sample - loss: 0.1755 - accuracy: 0.9800 - val_loss: 1.1465 - val_accuracy: 0.6351\n",
      "Epoch 22/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.1632 - accuracy: 0.9931\n",
      "Epoch 00022: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 870us/sample - loss: 0.1647 - accuracy: 0.9933 - val_loss: 1.1933 - val_accuracy: 0.6216\n",
      "Epoch 23/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1498 - accuracy: 0.9955\n",
      "Epoch 00023: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 830us/sample - loss: 0.1708 - accuracy: 0.9833 - val_loss: 1.1743 - val_accuracy: 0.6081\n",
      "Epoch 24/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1506 - accuracy: 0.9955\n",
      "Epoch 00024: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 816us/sample - loss: 0.1585 - accuracy: 0.9900 - val_loss: 1.1475 - val_accuracy: 0.6081\n",
      "Epoch 25/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1551 - accuracy: 1.0000\n",
      "Epoch 00025: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 820us/sample - loss: 0.1714 - accuracy: 0.9900 - val_loss: 1.2319 - val_accuracy: 0.5946\n",
      "Epoch 26/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1833 - accuracy: 0.9821\n",
      "Epoch 00026: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 823us/sample - loss: 0.1824 - accuracy: 0.9833 - val_loss: 1.1644 - val_accuracy: 0.5946\n",
      "Epoch 27/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1516 - accuracy: 0.9955\n",
      "Epoch 00027: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 787us/sample - loss: 0.1526 - accuracy: 0.9967 - val_loss: 1.0994 - val_accuracy: 0.6216\n",
      "Epoch 28/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1514 - accuracy: 0.9911\n",
      "Epoch 00028: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 786us/sample - loss: 0.1479 - accuracy: 0.9933 - val_loss: 1.0985 - val_accuracy: 0.6216\n",
      "Epoch 29/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1324 - accuracy: 1.0000\n",
      "Epoch 00029: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 783us/sample - loss: 0.1391 - accuracy: 0.9933 - val_loss: 1.1138 - val_accuracy: 0.5946\n",
      "Epoch 30/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1328 - accuracy: 1.0000\n",
      "Epoch 00030: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 799us/sample - loss: 0.1313 - accuracy: 1.0000 - val_loss: 1.1312 - val_accuracy: 0.6757\n",
      "Epoch 31/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1343 - accuracy: 0.9955\n",
      "Epoch 00031: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 819us/sample - loss: 0.1405 - accuracy: 0.9933 - val_loss: 1.1198 - val_accuracy: 0.6892\n",
      "Epoch 32/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1382 - accuracy: 0.9955\n",
      "Epoch 00032: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 832us/sample - loss: 0.1418 - accuracy: 0.9967 - val_loss: 1.1561 - val_accuracy: 0.5676\n",
      "Epoch 33/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1444 - accuracy: 0.9955\n",
      "Epoch 00033: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 809us/sample - loss: 0.1453 - accuracy: 0.9900 - val_loss: 1.1653 - val_accuracy: 0.5541\n",
      "Epoch 34/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1351 - accuracy: 1.0000\n",
      "Epoch 00034: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 806us/sample - loss: 0.1325 - accuracy: 1.0000 - val_loss: 0.9993 - val_accuracy: 0.6351\n",
      "Epoch 35/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1299 - accuracy: 1.0000\n",
      "Epoch 00035: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 813us/sample - loss: 0.1339 - accuracy: 1.0000 - val_loss: 1.0155 - val_accuracy: 0.6622\n",
      "Epoch 36/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1307 - accuracy: 0.9955\n",
      "Epoch 00036: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 812us/sample - loss: 0.1331 - accuracy: 0.9933 - val_loss: 1.0525 - val_accuracy: 0.6351\n",
      "Epoch 37/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1272 - accuracy: 1.0000\n",
      "Epoch 00037: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 820us/sample - loss: 0.1251 - accuracy: 1.0000 - val_loss: 1.1105 - val_accuracy: 0.6081\n",
      "Epoch 38/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1246 - accuracy: 1.0000\n",
      "Epoch 00038: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 816us/sample - loss: 0.1242 - accuracy: 1.0000 - val_loss: 1.1077 - val_accuracy: 0.6081\n",
      "Epoch 39/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1300 - accuracy: 1.0000\n",
      "Epoch 00039: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 819us/sample - loss: 0.1347 - accuracy: 0.9967 - val_loss: 1.0907 - val_accuracy: 0.6216\n",
      "Epoch 40/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1160 - accuracy: 1.0000\n",
      "Epoch 00040: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 828us/sample - loss: 0.1198 - accuracy: 1.0000 - val_loss: 1.1144 - val_accuracy: 0.6622\n",
      "Epoch 41/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1258 - accuracy: 1.0000\n",
      "Epoch 00041: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 836us/sample - loss: 0.1249 - accuracy: 1.0000 - val_loss: 1.0908 - val_accuracy: 0.6351\n",
      "Epoch 42/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1206 - accuracy: 1.0000\n",
      "Epoch 00042: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 812us/sample - loss: 0.1224 - accuracy: 1.0000 - val_loss: 1.0839 - val_accuracy: 0.6216\n",
      "Epoch 43/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1179 - accuracy: 1.0000\n",
      "Epoch 00043: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 832us/sample - loss: 0.1185 - accuracy: 1.0000 - val_loss: 1.1151 - val_accuracy: 0.5676\n",
      "Epoch 44/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1178 - accuracy: 1.0000\n",
      "Epoch 00044: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 836us/sample - loss: 0.1176 - accuracy: 1.0000 - val_loss: 1.0953 - val_accuracy: 0.5676\n",
      "Epoch 45/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.1171 - accuracy: 1.0000\n",
      "Epoch 00045: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 848us/sample - loss: 0.1167 - accuracy: 1.0000 - val_loss: 1.0560 - val_accuracy: 0.6081\n",
      "Epoch 46/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1134 - accuracy: 1.0000\n",
      "Epoch 00046: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 818us/sample - loss: 0.1167 - accuracy: 1.0000 - val_loss: 1.0445 - val_accuracy: 0.6216\n",
      "Epoch 47/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1110 - accuracy: 1.0000\n",
      "Epoch 00047: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 842us/sample - loss: 0.1147 - accuracy: 0.9967 - val_loss: 1.0655 - val_accuracy: 0.6216\n",
      "Epoch 48/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1156 - accuracy: 1.0000\n",
      "Epoch 00048: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 859us/sample - loss: 0.1220 - accuracy: 0.9967 - val_loss: 1.1341 - val_accuracy: 0.5135\n",
      "Epoch 49/200\n",
      "224/300 [=====================>........] - ETA: 0s - loss: 0.1258 - accuracy: 1.0000\n",
      "Epoch 00049: val_accuracy did not improve from 0.68919\n",
      "300/300 [==============================] - 0s 849us/sample - loss: 0.1217 - accuracy: 1.0000 - val_loss: 1.1475 - val_accuracy: 0.5541\n",
      "Epoch 00049: early stopping\n",
      "The value of model_types is set from ['CNN', 'DeepConvLSTM', 'ResNet', 'InceptionTime'] (default) to ['InceptionTime']\n",
      "The value of metrics is set from ['accuracy'] (default) to ['accuracy']\n",
      "Set maximum kernel size for InceptionTime models to number of timesteps.\n",
      "Train on 300 samples, validate on 74 samples\n",
      "Epoch 1/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.1494 - accuracy: 0.9444\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.52703, saving model to best_model.h5\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.1618 - accuracy: 0.9333 - val_loss: 1.3416 - val_accuracy: 0.5270\n",
      "Epoch 2/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.1884 - accuracy: 0.9340\n",
      "Epoch 00002: val_accuracy improved from 0.52703 to 0.54054, saving model to best_model.h5\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.1888 - accuracy: 0.9333 - val_loss: 1.3675 - val_accuracy: 0.5405\n",
      "Epoch 3/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0674 - accuracy: 0.9861\n",
      "Epoch 00003: val_accuracy improved from 0.54054 to 0.56757, saving model to best_model.h5\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0711 - accuracy: 0.9800 - val_loss: 1.2016 - val_accuracy: 0.5676\n",
      "Epoch 4/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0541 - accuracy: 0.9861\n",
      "Epoch 00004: val_accuracy did not improve from 0.56757\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0556 - accuracy: 0.9867 - val_loss: 1.2939 - val_accuracy: 0.5676\n",
      "Epoch 5/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0471 - accuracy: 0.9861\n",
      "Epoch 00005: val_accuracy did not improve from 0.56757\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0502 - accuracy: 0.9867 - val_loss: 1.3956 - val_accuracy: 0.4730\n",
      "Epoch 6/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0404 - accuracy: 0.9896\n",
      "Epoch 00006: val_accuracy did not improve from 0.56757\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0408 - accuracy: 0.9900 - val_loss: 1.3572 - val_accuracy: 0.5405\n",
      "Epoch 7/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0327 - accuracy: 0.9931\n",
      "Epoch 00007: val_accuracy did not improve from 0.56757\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0319 - accuracy: 0.9933 - val_loss: 1.4204 - val_accuracy: 0.5405\n",
      "Epoch 8/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0165 - accuracy: 0.9965\n",
      "Epoch 00008: val_accuracy did not improve from 0.56757\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0183 - accuracy: 0.9967 - val_loss: 1.4825 - val_accuracy: 0.5270\n",
      "Epoch 9/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0271 - accuracy: 0.9931\n",
      "Epoch 00009: val_accuracy did not improve from 0.56757\n",
      "300/300 [==============================] - 0s 983us/sample - loss: 0.0283 - accuracy: 0.9933 - val_loss: 1.5496 - val_accuracy: 0.5270\n",
      "Epoch 10/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0209 - accuracy: 0.9965\n",
      "Epoch 00010: val_accuracy did not improve from 0.56757\n",
      "300/300 [==============================] - 0s 993us/sample - loss: 0.0203 - accuracy: 0.9967 - val_loss: 1.6065 - val_accuracy: 0.5000\n",
      "Epoch 11/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0195 - accuracy: 0.9965\n",
      "Epoch 00011: val_accuracy did not improve from 0.56757\n",
      "300/300 [==============================] - 0s 983us/sample - loss: 0.0285 - accuracy: 0.9933 - val_loss: 1.4710 - val_accuracy: 0.5541\n",
      "Epoch 12/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0424 - accuracy: 0.9896\n",
      "Epoch 00012: val_accuracy did not improve from 0.56757\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0429 - accuracy: 0.9900 - val_loss: 1.7763 - val_accuracy: 0.5135\n",
      "Epoch 13/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0505 - accuracy: 0.9792\n",
      "Epoch 00013: val_accuracy did not improve from 0.56757\n",
      "300/300 [==============================] - 0s 984us/sample - loss: 0.0532 - accuracy: 0.9767 - val_loss: 1.6738 - val_accuracy: 0.5270\n",
      "Epoch 14/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0505 - accuracy: 0.9826\n",
      "Epoch 00014: val_accuracy did not improve from 0.56757\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0565 - accuracy: 0.9800 - val_loss: 1.7005 - val_accuracy: 0.5135\n",
      "Epoch 15/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0487 - accuracy: 0.9861\n",
      "Epoch 00015: val_accuracy improved from 0.56757 to 0.59459, saving model to best_model.h5\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0494 - accuracy: 0.9867 - val_loss: 1.8199 - val_accuracy: 0.5946\n",
      "Epoch 16/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0334 - accuracy: 0.9931\n",
      "Epoch 00016: val_accuracy did not improve from 0.59459\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0326 - accuracy: 0.9933 - val_loss: 1.7955 - val_accuracy: 0.5811\n",
      "Epoch 17/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0221 - accuracy: 0.9931\n",
      "Epoch 00017: val_accuracy did not improve from 0.59459\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0254 - accuracy: 0.9900 - val_loss: 1.7089 - val_accuracy: 0.5676\n",
      "Epoch 18/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0342 - accuracy: 0.9931\n",
      "Epoch 00018: val_accuracy did not improve from 0.59459\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0377 - accuracy: 0.9900 - val_loss: 1.7578 - val_accuracy: 0.4595\n",
      "Epoch 19/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0532 - accuracy: 0.9826\n",
      "Epoch 00019: val_accuracy did not improve from 0.59459\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0515 - accuracy: 0.9833 - val_loss: 1.9105 - val_accuracy: 0.4730\n",
      "Epoch 20/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0477 - accuracy: 0.9861\n",
      "Epoch 00020: val_accuracy did not improve from 0.59459\n",
      "300/300 [==============================] - 0s 990us/sample - loss: 0.0527 - accuracy: 0.9800 - val_loss: 1.9984 - val_accuracy: 0.5135\n",
      "Epoch 21/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0506 - accuracy: 0.9792\n",
      "Epoch 00021: val_accuracy did not improve from 0.59459\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0488 - accuracy: 0.9800 - val_loss: 2.0736 - val_accuracy: 0.5541\n",
      "Epoch 22/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0475 - accuracy: 0.9861\n",
      "Epoch 00022: val_accuracy did not improve from 0.59459\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0518 - accuracy: 0.9833 - val_loss: 1.9177 - val_accuracy: 0.5135\n",
      "Epoch 23/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0378 - accuracy: 0.9931\n",
      "Epoch 00023: val_accuracy did not improve from 0.59459\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0374 - accuracy: 0.9933 - val_loss: 1.7489 - val_accuracy: 0.5946\n",
      "Epoch 24/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0230 - accuracy: 0.9931\n",
      "Epoch 00024: val_accuracy improved from 0.59459 to 0.60811, saving model to best_model.h5\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0239 - accuracy: 0.9933 - val_loss: 1.5668 - val_accuracy: 0.6081\n",
      "Epoch 25/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0218 - accuracy: 0.9931\n",
      "Epoch 00025: val_accuracy improved from 0.60811 to 0.62162, saving model to best_model.h5\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0328 - accuracy: 0.9867 - val_loss: 1.6991 - val_accuracy: 0.6216\n",
      "Epoch 26/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0442 - accuracy: 0.9896\n",
      "Epoch 00026: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0432 - accuracy: 0.9900 - val_loss: 1.8909 - val_accuracy: 0.6081\n",
      "Epoch 27/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0276 - accuracy: 0.9931\n",
      "Epoch 00027: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0267 - accuracy: 0.9933 - val_loss: 1.6126 - val_accuracy: 0.5541\n",
      "Epoch 28/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0350 - accuracy: 0.9896\n",
      "Epoch 00028: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0373 - accuracy: 0.9900 - val_loss: 1.9397 - val_accuracy: 0.5135\n",
      "Epoch 29/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0575 - accuracy: 0.9688\n",
      "Epoch 00029: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0560 - accuracy: 0.9700 - val_loss: 2.1901 - val_accuracy: 0.6081\n",
      "Epoch 30/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0941 - accuracy: 0.9618\n",
      "Epoch 00030: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 998us/sample - loss: 0.1046 - accuracy: 0.9600 - val_loss: 1.9064 - val_accuracy: 0.6081\n",
      "Epoch 31/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.1591 - accuracy: 0.9410\n",
      "Epoch 00031: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 998us/sample - loss: 0.1535 - accuracy: 0.9433 - val_loss: 1.8786 - val_accuracy: 0.6216\n",
      "Epoch 32/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.1428 - accuracy: 0.9514\n",
      "Epoch 00032: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.1395 - accuracy: 0.9533 - val_loss: 1.8850 - val_accuracy: 0.6081\n",
      "Epoch 33/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.1206 - accuracy: 0.9722\n",
      "Epoch 00033: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.1220 - accuracy: 0.9733 - val_loss: 1.7683 - val_accuracy: 0.5270\n",
      "Epoch 34/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0874 - accuracy: 0.9618\n",
      "Epoch 00034: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0851 - accuracy: 0.9633 - val_loss: 1.7086 - val_accuracy: 0.5405\n",
      "Epoch 35/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0284 - accuracy: 0.9965\n",
      "Epoch 00035: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0294 - accuracy: 0.9967 - val_loss: 1.9625 - val_accuracy: 0.5270\n",
      "Epoch 36/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0249 - accuracy: 0.9931\n",
      "Epoch 00036: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0243 - accuracy: 0.9933 - val_loss: 1.8293 - val_accuracy: 0.5676\n",
      "Epoch 37/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0143 - accuracy: 0.9965\n",
      "Epoch 00037: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0140 - accuracy: 0.9967 - val_loss: 1.8557 - val_accuracy: 0.5405\n",
      "Epoch 38/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0087 - accuracy: 1.0000\n",
      "Epoch 00038: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0088 - accuracy: 1.0000 - val_loss: 1.6960 - val_accuracy: 0.5946\n",
      "Epoch 39/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0072 - accuracy: 1.0000\n",
      "Epoch 00039: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0077 - accuracy: 1.0000 - val_loss: 1.6617 - val_accuracy: 0.5811\n",
      "Epoch 40/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 00040: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 997us/sample - loss: 0.0055 - accuracy: 1.0000 - val_loss: 1.7471 - val_accuracy: 0.5405\n",
      "Epoch 41/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 00041: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0087 - accuracy: 1.0000 - val_loss: 1.7266 - val_accuracy: 0.5811\n",
      "Epoch 42/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0105 - accuracy: 1.0000\n",
      "Epoch 00042: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 981us/sample - loss: 0.0101 - accuracy: 1.0000 - val_loss: 1.9782 - val_accuracy: 0.5135\n",
      "Epoch 43/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 00043: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0098 - accuracy: 1.0000 - val_loss: 1.7407 - val_accuracy: 0.5541\n",
      "Epoch 44/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 00044: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1000us/sample - loss: 0.0086 - accuracy: 1.0000 - val_loss: 1.6522 - val_accuracy: 0.5676\n",
      "Epoch 45/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 00045: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0078 - accuracy: 0.9967 - val_loss: 1.7172 - val_accuracy: 0.5676\n",
      "Epoch 46/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0108 - accuracy: 1.0000\n",
      "Epoch 00046: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0106 - accuracy: 1.0000 - val_loss: 2.3615 - val_accuracy: 0.5811\n",
      "Epoch 47/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0141 - accuracy: 0.9965\n",
      "Epoch 00047: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0169 - accuracy: 0.9967 - val_loss: 2.3339 - val_accuracy: 0.5405\n",
      "Epoch 48/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0259 - accuracy: 0.9931\n",
      "Epoch 00048: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0253 - accuracy: 0.9933 - val_loss: 2.2534 - val_accuracy: 0.5000\n",
      "Epoch 49/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0364 - accuracy: 0.9896\n",
      "Epoch 00049: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0354 - accuracy: 0.9900 - val_loss: 2.8339 - val_accuracy: 0.5135\n",
      "Epoch 50/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0396 - accuracy: 0.9792\n",
      "Epoch 00050: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0398 - accuracy: 0.9800 - val_loss: 2.0484 - val_accuracy: 0.5405\n",
      "Epoch 51/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0157 - accuracy: 1.0000\n",
      "Epoch 00051: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0246 - accuracy: 0.9967 - val_loss: 2.0311 - val_accuracy: 0.5811\n",
      "Epoch 52/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0758 - accuracy: 0.9653\n",
      "Epoch 00052: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0758 - accuracy: 0.9633 - val_loss: 2.1039 - val_accuracy: 0.5676\n",
      "Epoch 53/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.1073 - accuracy: 0.9583\n",
      "Epoch 00053: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.1076 - accuracy: 0.9567 - val_loss: 2.0737 - val_accuracy: 0.5541\n",
      "Epoch 54/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.1188 - accuracy: 0.9583\n",
      "Epoch 00054: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.1168 - accuracy: 0.9600 - val_loss: 1.8588 - val_accuracy: 0.5405\n",
      "Epoch 55/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.0768 - accuracy: 0.9688\n",
      "Epoch 00055: val_accuracy did not improve from 0.62162\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0770 - accuracy: 0.9700 - val_loss: 2.2399 - val_accuracy: 0.5405\n",
      "Epoch 00055: early stopping\n",
      "The value of model_types is set from ['CNN', 'DeepConvLSTM', 'ResNet', 'InceptionTime'] (default) to ['CNN']\n",
      "The value of metrics is set from ['accuracy'] (default) to ['accuracy']\n",
      "Set maximum kernel size for InceptionTime models to number of timesteps.\n",
      "Train on 300 samples, validate on 74 samples\n",
      "Epoch 1/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6967 - accuracy: 0.9965\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.59459, saving model to best_model.h5\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.6976 - accuracy: 0.9967 - val_loss: 1.2413 - val_accuracy: 0.5946\n",
      "Epoch 2/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6969 - accuracy: 0.9965\n",
      "Epoch 00002: val_accuracy did not improve from 0.59459\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.7003 - accuracy: 0.9967 - val_loss: 1.2731 - val_accuracy: 0.5676\n",
      "Epoch 3/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6896 - accuracy: 1.0000\n",
      "Epoch 00003: val_accuracy did not improve from 0.59459\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.6893 - accuracy: 1.0000 - val_loss: 1.2571 - val_accuracy: 0.5676\n",
      "Epoch 4/200\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6687 - accuracy: 1.0000"
     ]
    }
   ],
   "source": [
    "accuracies_cnn = []\n",
    "accuracies_it = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "for train_index, test_index in tqdm(skf.split(X_full, y_full)):\n",
    "    \n",
    "    X_train = X_full[train_index]\n",
    "    y_train = y_full[train_index]\n",
    "    X_test = X_full[test_index]\n",
    "    y_test = y_full[test_index]\n",
    "\n",
    "    #Create train, validation and test sets for DL\n",
    "    LE = LabelEncoder()\n",
    "    skf_train_val = StratifiedKFold(n_splits=5)\n",
    "\n",
    "    for train_index, val_index in skf_train_val.split(X_train, y_train):\n",
    "        train_index = train_index\n",
    "        val_index = val_index\n",
    "\n",
    "    X_train_val = X_train.copy()\n",
    "\n",
    "    X_train_dl = X_train_val[train_index].copy()\n",
    "    X_train_dl = np.transpose(X_train_dl, (0,2,1))\n",
    "    y_train_dl = y_train[train_index].copy()\n",
    "    y_train_dl = to_categorical(LE.fit_transform(y_train_dl))\n",
    "\n",
    "    X_val_dl = X_train_val[val_index].copy()\n",
    "    X_val_dl = np.transpose(X_val_dl, (0,2,1))\n",
    "    y_val_dl = y_train[val_index].copy()\n",
    "    y_val_dl = to_categorical(LE.transform(y_val_dl))\n",
    "\n",
    "    X_test_dl = np.transpose(X_test, (0,2,1))\n",
    "    y_test_dl = to_categorical(LE.transform(y_test))\n",
    "    \n",
    "    acc_cnn = dl_func('CNN',\n",
    "                     X_train_dl, \n",
    "                     y_train_dl,\n",
    "                     X_val_dl,\n",
    "                     y_val_dl,\n",
    "                     X_test_dl,\n",
    "                     y_test_dl)\n",
    "    \n",
    "    acc_it = dl_func('InceptionTime',\n",
    "                     X_train_dl, \n",
    "                     y_train_dl,\n",
    "                     X_val_dl,\n",
    "                     y_val_dl,\n",
    "                     X_test_dl,\n",
    "                     y_test_dl)\n",
    "\n",
    "    accuracies_cnn.append(acc_cnn.copy())\n",
    "    accuracies_it.append(acc_it.copy())\n",
    "\n",
    "scores['dl_cnn'] = [np.mean(accuracies_cnn), np.std(accuracies_cnn)]\n",
    "scores['dl_it'] = [np.mean(accuracies_it), np.std(accuracies_it)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyEEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T21:08:50.508842Z",
     "start_time": "2020-11-04T20:49:30.145194Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e028c7d3794904a5029c8dfa1f990e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/prs/workspace/tf_2/lib/python3.6/site-packages/pywt/_multilevel.py:45: UserWarning: Level value of 4 is too high: all coefficients will experience boundary effects.\n",
      "  \"boundary effects.\").format(level))\n",
      "/jet/prs/workspace/tf_2/lib/python3.6/site-packages/pyeeg-0.4.4-py3.6.egg/pyeeg/entropy.py:235: RuntimeWarning: divide by zero encountered in log\n",
      "/jet/prs/workspace/tf_2/lib/python3.6/site-packages/pyeeg-0.4.4-py3.6.egg/pyeeg/entropy.py:235: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/jet/prs/workspace/tf_2/lib/python3.6/site-packages/pyeeg-0.4.4-py3.6.egg/pyeeg/fractal_dimension.py:22: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "/jet/prs/workspace/tf_2/lib/python3.6/site-packages/pywt/_multilevel.py:45: UserWarning: Level value of 4 is too high: all coefficients will experience boundary effects.\n",
      "  \"boundary effects.\").format(level))\n",
      "/jet/prs/workspace/tf_2/lib/python3.6/site-packages/pyeeg-0.4.4-py3.6.egg/pyeeg/entropy.py:235: RuntimeWarning: divide by zero encountered in log\n",
      "/jet/prs/workspace/tf_2/lib/python3.6/site-packages/pyeeg-0.4.4-py3.6.egg/pyeeg/entropy.py:235: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/jet/prs/workspace/tf_2/lib/python3.6/site-packages/pyeeg-0.4.4-py3.6.egg/pyeeg/fractal_dimension.py:22: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "/jet/prs/workspace/tf_2/lib/python3.6/site-packages/pywt/_multilevel.py:45: UserWarning: Level value of 4 is too high: all coefficients will experience boundary effects.\n",
      "  \"boundary effects.\").format(level))\n",
      "/jet/prs/workspace/tf_2/lib/python3.6/site-packages/pyeeg-0.4.4-py3.6.egg/pyeeg/entropy.py:235: RuntimeWarning: divide by zero encountered in log\n",
      "/jet/prs/workspace/tf_2/lib/python3.6/site-packages/pyeeg-0.4.4-py3.6.egg/pyeeg/entropy.py:235: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/jet/prs/workspace/tf_2/lib/python3.6/site-packages/pyeeg-0.4.4-py3.6.egg/pyeeg/fractal_dimension.py:22: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "/jet/prs/workspace/tf_2/lib/python3.6/site-packages/pywt/_multilevel.py:45: UserWarning: Level value of 4 is too high: all coefficients will experience boundary effects.\n",
      "  \"boundary effects.\").format(level))\n",
      "/jet/prs/workspace/tf_2/lib/python3.6/site-packages/pyeeg-0.4.4-py3.6.egg/pyeeg/entropy.py:235: RuntimeWarning: divide by zero encountered in log\n",
      "/jet/prs/workspace/tf_2/lib/python3.6/site-packages/pyeeg-0.4.4-py3.6.egg/pyeeg/entropy.py:235: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/jet/prs/workspace/tf_2/lib/python3.6/site-packages/pyeeg-0.4.4-py3.6.egg/pyeeg/fractal_dimension.py:22: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "/jet/prs/workspace/tf_2/lib/python3.6/site-packages/pywt/_multilevel.py:45: UserWarning: Level value of 4 is too high: all coefficients will experience boundary effects.\n",
      "  \"boundary effects.\").format(level))\n",
      "/jet/prs/workspace/tf_2/lib/python3.6/site-packages/pyeeg-0.4.4-py3.6.egg/pyeeg/entropy.py:235: RuntimeWarning: divide by zero encountered in log\n",
      "/jet/prs/workspace/tf_2/lib/python3.6/site-packages/pyeeg-0.4.4-py3.6.egg/pyeeg/entropy.py:235: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/jet/prs/workspace/tf_2/lib/python3.6/site-packages/pyeeg-0.4.4-py3.6.egg/pyeeg/fractal_dimension.py:22: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "/jet/prs/workspace/tf_2/lib/python3.6/site-packages/pywt/_multilevel.py:45: UserWarning: Level value of 4 is too high: all coefficients will experience boundary effects.\n",
      "  \"boundary effects.\").format(level))\n",
      "/jet/prs/workspace/tf_2/lib/python3.6/site-packages/pyeeg-0.4.4-py3.6.egg/pyeeg/entropy.py:235: RuntimeWarning: divide by zero encountered in log\n",
      "/jet/prs/workspace/tf_2/lib/python3.6/site-packages/pyeeg-0.4.4-py3.6.egg/pyeeg/entropy.py:235: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/jet/prs/workspace/tf_2/lib/python3.6/site-packages/pyeeg-0.4.4-py3.6.egg/pyeeg/fractal_dimension.py:22: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "/jet/prs/workspace/tf_2/lib/python3.6/site-packages/pywt/_multilevel.py:45: UserWarning: Level value of 4 is too high: all coefficients will experience boundary effects.\n",
      "  \"boundary effects.\").format(level))\n",
      "/jet/prs/workspace/tf_2/lib/python3.6/site-packages/pyeeg-0.4.4-py3.6.egg/pyeeg/entropy.py:235: RuntimeWarning: divide by zero encountered in log\n",
      "/jet/prs/workspace/tf_2/lib/python3.6/site-packages/pyeeg-0.4.4-py3.6.egg/pyeeg/entropy.py:235: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/jet/prs/workspace/tf_2/lib/python3.6/site-packages/pyeeg-0.4.4-py3.6.egg/pyeeg/fractal_dimension.py:22: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "/jet/prs/workspace/tf_2/lib/python3.6/site-packages/pywt/_multilevel.py:45: UserWarning: Level value of 4 is too high: all coefficients will experience boundary effects.\n",
      "  \"boundary effects.\").format(level))\n",
      "/jet/prs/workspace/tf_2/lib/python3.6/site-packages/pyeeg-0.4.4-py3.6.egg/pyeeg/entropy.py:235: RuntimeWarning: divide by zero encountered in log\n",
      "/jet/prs/workspace/tf_2/lib/python3.6/site-packages/pyeeg-0.4.4-py3.6.egg/pyeeg/entropy.py:235: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/jet/prs/workspace/tf_2/lib/python3.6/site-packages/pyeeg-0.4.4-py3.6.egg/pyeeg/fractal_dimension.py:22: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "/jet/prs/workspace/tf_2/lib/python3.6/site-packages/pywt/_multilevel.py:45: UserWarning: Level value of 4 is too high: all coefficients will experience boundary effects.\n",
      "  \"boundary effects.\").format(level))\n",
      "/jet/prs/workspace/tf_2/lib/python3.6/site-packages/pyeeg-0.4.4-py3.6.egg/pyeeg/entropy.py:235: RuntimeWarning: divide by zero encountered in log\n",
      "/jet/prs/workspace/tf_2/lib/python3.6/site-packages/pyeeg-0.4.4-py3.6.egg/pyeeg/entropy.py:235: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/jet/prs/workspace/tf_2/lib/python3.6/site-packages/pyeeg-0.4.4-py3.6.egg/pyeeg/fractal_dimension.py:22: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "/jet/prs/workspace/tf_2/lib/python3.6/site-packages/pywt/_multilevel.py:45: UserWarning: Level value of 4 is too high: all coefficients will experience boundary effects.\n",
      "  \"boundary effects.\").format(level))\n",
      "/jet/prs/workspace/tf_2/lib/python3.6/site-packages/pyeeg-0.4.4-py3.6.egg/pyeeg/entropy.py:235: RuntimeWarning: divide by zero encountered in log\n",
      "/jet/prs/workspace/tf_2/lib/python3.6/site-packages/pyeeg-0.4.4-py3.6.egg/pyeeg/entropy.py:235: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/jet/prs/workspace/tf_2/lib/python3.6/site-packages/pyeeg-0.4.4-py3.6.egg/pyeeg/fractal_dimension.py:22: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pyeeg_lr_acc = []\n",
    "pyeeg_rf_acc = []\n",
    "pyeeg_svc_acc = []\n",
    "pyeeg_lgbm_acc = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "for train_index, test_index in tqdm(skf.split(X_full, y_full)):\n",
    "    \n",
    "    X_train = X_full[train_index]\n",
    "    y_train = y_full[train_index]\n",
    "    X_test = X_full[test_index]\n",
    "    y_test = y_full[test_index]\n",
    "    \n",
    "    feat_dict = {}\n",
    "\n",
    "    for df,name in zip([X_train,X_test],['X_train_pyeeg','X_test_pyeeg']):\n",
    "        feat_dict[name] = np.array([]).reshape(-1,2100)\n",
    "\n",
    "        for ind in range(0,df.shape[0]):\n",
    "            features = np.array([])\n",
    "\n",
    "            for channel in range(0,df[ind].shape[0]):\n",
    "\n",
    "                eeg_series = df[ind][channel]\n",
    "\n",
    "                coefs = wavedec(eeg_series, 'db4', level=4)\n",
    "\n",
    "                for band in coefs:\n",
    "                    features = np.hstack([features,get_features(band)])\n",
    "\n",
    "                fft_power = pyeeg.bin_power(eeg_series, Band = [0.5,4,7,12,30,100], Fs = 256)\n",
    "                power_spectral_intensity = fft_power[0]\n",
    "                relative_intensity_ratio = fft_power[1]\n",
    "#                 fisher_info = pyeeg.fisher_info(eeg_series, Tau = 1, DE = 90)\n",
    "                pfd = pyeeg.pfd(eeg_series)\n",
    "                spectral_entropy = pyeeg.spectral_entropy(eeg_series, Band = [0.5,4,7,12,30,100], Fs = 256,\n",
    "                                                          Power_Ratio = relative_intensity_ratio)\n",
    "\n",
    "#                 eeg_series = ResampleLinear1D(eeg_series,200)\n",
    "\n",
    "#                 ap_entropy = pyeeg.ap_entropy(eeg_series, M = 3, R = 0.25*np.std(eeg_series))\n",
    "                hfd = pyeeg.hfd(eeg_series, Kmax = 10)\n",
    "                hjorth = pyeeg.hjorth(eeg_series)\n",
    "                hjorth_mob = hjorth[0]\n",
    "                hjorth_comp = hjorth[1]\n",
    "    #             hurst = pyeeg.hurst(eeg_series)\n",
    "\n",
    "\n",
    "                features = np.hstack([features, power_spectral_intensity, relative_intensity_ratio,\n",
    "                                      hfd, hjorth_mob, hjorth_comp, pfd, spectral_entropy])\n",
    "\n",
    "\n",
    "            feat_dict[name] = np.vstack([feat_dict[name],features])\n",
    "\n",
    "    X_train_pyeeg = feat_dict['X_train_pyeeg'].copy()\n",
    "    X_test_pyeeg = feat_dict['X_test_pyeeg'].copy()\n",
    "    \n",
    "    X_train_pyeeg = np.nan_to_num(X_train_pyeeg)\n",
    "    X_test_pyeeg = np.nan_to_num(X_test_pyeeg)\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train_pyeeg)\n",
    "    X_train_pyeeg = scaler.transform(X_train_pyeeg)\n",
    "    X_test_pyeeg = scaler.transform(X_test_pyeeg)\n",
    "    \n",
    "    #pyeeg + lr    \n",
    "    hyperparameters = {}\n",
    "    hyperparameters['pyeeg_lr'] = {}\n",
    "    hyperparameters['pyeeg_lr']['C'] = 1\n",
    "\n",
    "    logistic = LogisticRegression(solver = 'lbfgs',\n",
    "                                  multi_class = 'auto',\n",
    "                                  max_iter=3000,\n",
    "                                  C = hyperparameters['pyeeg_lr']['C'])\n",
    "\n",
    "    logistic.fit(X_train_pyeeg,y_train)\n",
    "    acc = logistic.score(X_test_pyeeg,y_test)\n",
    "    pyeeg_lr_acc.append(np.round(acc,2).copy())\n",
    "    \n",
    "    #pyeeg + rf    \n",
    "    hyperparameters = {}\n",
    "    hyperparameters['pyeeg_rf'] = {}\n",
    "    hyperparameters['pyeeg_rf']['n_estimators'] = 500\n",
    "    hyperparameters['pyeeg_rf']['max_depth'] = 3\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators = hyperparameters['pyeeg_rf']['n_estimators'],\n",
    "                                max_depth = hyperparameters['pyeeg_rf']['max_depth'])\n",
    "\n",
    "    rf.fit(X_train_pyeeg,y_train)\n",
    "    acc = rf.score(X_test_pyeeg,y_test)\n",
    "    pyeeg_rf_acc.append(np.round(acc,2).copy())\n",
    "    \n",
    "    #pyeeg + svc\n",
    "    hyperparameters = {}\n",
    "    hyperparameters['pyeeg_svc'] = {}\n",
    "    hyperparameters['pyeeg_svc']['C'] = 2\n",
    "    hyperparameters['pyeeg_svc']['kernel'] = 'rbf'\n",
    "    hyperparameters['pyeeg_svc']['degree'] = 3\n",
    "    hyperparameters['pyeeg_svc']['gamma'] = 'scale'\n",
    "\n",
    "    svc = SVC(C = hyperparameters['pyeeg_svc']['C'],\n",
    "              kernel = hyperparameters['pyeeg_svc']['kernel'],\n",
    "              degree = hyperparameters['pyeeg_svc']['degree'],\n",
    "              gamma = hyperparameters['pyeeg_svc']['gamma'])\n",
    "\n",
    "    clf = make_pipeline(svc)\n",
    "    clf.fit(X_train_pyeeg,y_train)\n",
    "    acc = clf.score(X_test_pyeeg,y_test)\n",
    "    pyeeg_svc_acc.append(np.round(acc,2).copy())\n",
    "\n",
    "    \n",
    "    #pyeeg + lgbm\n",
    "    hyperparameters = {}\n",
    "    hyperparameters['pyeeg_lgbm'] = {}\n",
    "    hyperparameters['pyeeg_lgbm']['num_leaves'] = 254\n",
    "    hyperparameters['pyeeg_lgbm']['max_depth'] = 2\n",
    "    hyperparameters['pyeeg_lgbm']['learning_rate'] = 0.1\n",
    "    hyperparameters['pyeeg_lgbm']['n_estimators'] = 500\n",
    "    hyperparameters['pyeeg_lgbm']['min_split_gain'] = 0.2\n",
    "    hyperparameters['pyeeg_lgbm']['min_child_samples'] = 20\n",
    "    hyperparameters['pyeeg_lgbm']['colsample_bytree'] = 1\n",
    "    hyperparameters['pyeeg_lgbm']['reg_alpha'] = 0.1\n",
    "    hyperparameters['pyeeg_lgbm']['reg_lambda'] = 0\n",
    "\n",
    "    def sparse_float(mat):\n",
    "        return mat.astype('float')\n",
    "\n",
    "    trans_sparse_float = FunctionTransformer(sparse_float, validate = False)\n",
    "\n",
    "    lgbm = LGBMClassifier(n_jobs = -1,\n",
    "                          num_leaves = hyperparameters['pyeeg_lgbm']['num_leaves'],\n",
    "                          max_depth = hyperparameters['pyeeg_lgbm']['max_depth'],\n",
    "                          learning_rate = hyperparameters['pyeeg_lgbm']['learning_rate'],\n",
    "                          n_estimators = hyperparameters['pyeeg_lgbm']['n_estimators'],\n",
    "                          min_split_gain = hyperparameters['pyeeg_lgbm']['min_split_gain'],\n",
    "                          min_child_samples = hyperparameters['pyeeg_lgbm']['min_child_samples'],\n",
    "                          colsample_by_tree = hyperparameters['pyeeg_lgbm']['colsample_bytree'],\n",
    "                          reg_alpha = hyperparameters['pyeeg_lgbm']['reg_alpha'],\n",
    "                          reg_lambda = hyperparameters['pyeeg_lgbm']['reg_lambda'])\n",
    "\n",
    "    clf = make_pipeline(trans_sparse_float, lgbm)\n",
    "    clf.fit(X_train_pyeeg,y_train)\n",
    "    acc = clf.score(X_test_pyeeg,y_test)\n",
    "    pyeeg_lgbm_acc.append(np.round(acc,2).copy())\n",
    "\n",
    "scores['pyeeg_lr'] = [np.mean(pyeeg_lr_acc), np.std(pyeeg_lr_acc)]\n",
    "scores['pyeeg_rf'] = [np.mean(pyeeg_rf_acc), np.std(pyeeg_rf_acc)]\n",
    "scores['pyeeg_svc'] = [np.mean(pyeeg_svc_acc), np.std(pyeeg_svc_acc)]\n",
    "scores['pyeeg_lgbm'] = [np.mean(pyeeg_lgbm_acc), np.std(pyeeg_lgbm_acc)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T21:50:19.144872Z",
     "start_time": "2020-11-04T21:50:19.100904Z"
    }
   },
   "outputs": [],
   "source": [
    "#Salva scores\n",
    "individual_results = pd.DataFrame({'dtw_acc' : dtw_acc,\n",
    "                                   'wm_lr_acc' : accuracies_lr,\n",
    "                                   'wm_rf_acc' : accuracies_rf,\n",
    "                                   'wm_svc_acc' : accuracies_svc,\n",
    "                                   'wm_lgbm_acc' : accuracies_lgbm,\n",
    "                                   'cnn_acc' : accuracies_cnn,\n",
    "                                   'it_acc' : accuracies_it,\n",
    "                                   'pyeeg_lr_acc' : pyeeg_lr_acc,\n",
    "                                   'pyeeg_rf_acc' : pyeeg_rf_acc,\n",
    "                                   'pyeeg_svc_acc' : pyeeg_svc_acc,\n",
    "                                   'pyeeg_lgbm_acc' : pyeeg_lgbm_acc})\n",
    "\n",
    "individual_results.to_csv('results/Individual_Scores_{}.csv'.format(dataset), index = False)\n",
    "\n",
    "results = pd.DataFrame(scores).T.reset_index().rename(columns={0 : 'mean',\n",
    "                                                               1 : 'std',\n",
    "                                                               'index' : 'method'}).sort_values('mean', ascending = False)\n",
    "\n",
    "results.to_csv('results/Scores_{}.csv'.format(dataset), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T21:50:19.161580Z",
     "start_time": "2020-11-04T21:50:19.147611Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pyeeg_svc</td>\n",
       "      <td>0.588000</td>\n",
       "      <td>0.058275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pyeeg_rf</td>\n",
       "      <td>0.581000</td>\n",
       "      <td>0.063632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pyeeg_lgbm</td>\n",
       "      <td>0.556000</td>\n",
       "      <td>0.089688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dl_it</td>\n",
       "      <td>0.555459</td>\n",
       "      <td>0.046151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pyeeg_lr</td>\n",
       "      <td>0.548000</td>\n",
       "      <td>0.058103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dtw_knn</td>\n",
       "      <td>0.540883</td>\n",
       "      <td>0.061287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wm_rf</td>\n",
       "      <td>0.535000</td>\n",
       "      <td>0.088459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wm_lr</td>\n",
       "      <td>0.524000</td>\n",
       "      <td>0.051029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dl_cnn</td>\n",
       "      <td>0.521777</td>\n",
       "      <td>0.061977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wm_lgbm</td>\n",
       "      <td>0.521000</td>\n",
       "      <td>0.073410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wm_svc</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>0.028018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        method      mean       std\n",
       "9    pyeeg_svc  0.588000  0.058275\n",
       "8     pyeeg_rf  0.581000  0.063632\n",
       "10  pyeeg_lgbm  0.556000  0.089688\n",
       "6        dl_it  0.555459  0.046151\n",
       "7     pyeeg_lr  0.548000  0.058103\n",
       "0      dtw_knn  0.540883  0.061287\n",
       "2        wm_rf  0.535000  0.088459\n",
       "1        wm_lr  0.524000  0.051029\n",
       "5       dl_cnn  0.521777  0.061977\n",
       "4      wm_lgbm  0.521000  0.073410\n",
       "3       wm_svc  0.515000  0.028018"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_2",
   "language": "python",
   "name": "tf_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
