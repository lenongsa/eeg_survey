{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the raw EEG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T13:18:37.492538Z",
     "start_time": "2020-11-26T13:18:37.488191Z"
    }
   },
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from aux_functions import timeseries_to_pandas\n",
    "from aux_functions import pandas_to_numpy\n",
    "\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alcoholism_S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-25T15:51:39.442533Z",
     "start_time": "2020-11-25T15:51:23.941458Z"
    }
   },
   "outputs": [],
   "source": [
    "#Read the csv files and concatenate them into a single dataframe\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for folder in ['SMNI_CMI_TRAIN', 'SMNI_CMI_TEST']:\n",
    "    files = os.listdir(os.path.join('Datasets_raw', 'Alcoholism_S1', folder))\n",
    "    files = [file for file in files if file.endswith('.csv')]\n",
    "    \n",
    "    for file in files:\n",
    "        data = pd.read_csv(os.path.join('Datasets_raw', 'Alcoholism_S1', folder, file))\n",
    "        df = pd.concat([df, data])\n",
    "        \n",
    "df = df.sort_values(['name', 'trial number', 'channel', 'sample num'])\n",
    "df = df[['name', 'subject identifier', 'matching condition', 'trial number',\n",
    "         'channel', 'sample num', 'sensor value']]\n",
    "\n",
    "#Limit the trials only to those where the matching condition is S1\n",
    "df = df[df['matching condition'] == 'S1 obj']\n",
    "\n",
    "#Create a dictionary to store, for each subject, a numpy array of all his trials \n",
    "#in the format (n_trials, n_channels, n_timesteps), and a numpy array with the \n",
    "#identifier of the subject ('c' for control, 'a' for alcoholic) for each\n",
    "#of his trials\n",
    "\n",
    "subjects = {}\n",
    "\n",
    "for subject in df.name.unique():\n",
    "    subjects[subject] = {}\n",
    "    trials = []\n",
    "    subject_data = df[df.name == subject]\n",
    "    \n",
    "    for trial in subject_data['trial number'].unique():\n",
    "        channels = []\n",
    "        trial_data = subject_data[subject_data['trial number'] == trial]\n",
    "    \n",
    "        for channel in trial_data['channel'].unique():\n",
    "            channel_data = list(trial_data.loc[trial_data['channel'] == channel, 'sensor value'])\n",
    "            channels.append(channel_data)\n",
    "            \n",
    "        trials.append(channels)\n",
    "        \n",
    "    subjects[subject]['X'] = np.array(trials)\n",
    "    subjects[subject]['y'] = np.array(subject_data[['subject identifier', 'trial number']]\\\n",
    "                                      .drop_duplicates()['subject identifier'])\n",
    "\n",
    "#Create folder structure and save the clean data\n",
    "if not os.path.exists(os.path.join('Datasets_clean', 'Alcoholism_S1')):\n",
    "    os.makedirs(os.path.join('Datasets_clean', 'Alcoholism_S1'))\n",
    "\n",
    "with open(os.path.join('Datasets_clean', 'Alcoholism_S1', 'clean_data.pkl'), 'wb') as handle:\n",
    "    pickle.dump(subjects, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FingerMovements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T13:20:25.809412Z",
     "start_time": "2020-11-26T13:20:18.888154Z"
    }
   },
   "outputs": [],
   "source": [
    "#Read data\n",
    "data_train = {}\n",
    "data_test = {}\n",
    "\n",
    "for i in range(1,29):\n",
    "    data_train[i] = arff.loadarff(os.path.join('Datasets_raw', \n",
    "                                               'FingerMovements',\n",
    "                                               f'FingerMovementsDimension{i}_TRAIN.arff'))[0]\n",
    "    data_test[i] = arff.loadarff(os.path.join('Datasets_raw', \n",
    "                                               'FingerMovements',\n",
    "                                               f'FingerMovementsDimension{i}_TEST.arff'))[0]\n",
    "\n",
    "#Convert to pandas dataframes\n",
    "df_train = timeseries_to_pandas(data_train)\n",
    "df_test = timeseries_to_pandas(data_test)\n",
    "\n",
    "#Convert to numpy arrays\n",
    "X_train, y_train = pandas_to_numpy(df_train)\n",
    "X_test, y_test = pandas_to_numpy(df_test)\n",
    "\n",
    "#Create folder structure and save the arrays\n",
    "if not os.path.exists(os.path.join('Datasets_clean', 'FingerMovements')):\n",
    "    os.makedirs(os.path.join('Datasets_clean', 'FingerMovements'))\n",
    "\n",
    "np.save('Datasets_clean/FingerMovements/X_train', X_train)\n",
    "np.save('Datasets_clean/FingerMovements/y_train', y_train)\n",
    "np.save('Datasets_clean/FingerMovements/X_test', X_test)\n",
    "np.save('Datasets_clean/FingerMovements/y_test', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SelfregulationSCP1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T13:22:16.954519Z",
     "start_time": "2020-11-26T13:22:05.553992Z"
    }
   },
   "outputs": [],
   "source": [
    "#Read data\n",
    "data_train = {}\n",
    "data_test = {}\n",
    "\n",
    "for i in range(1,7):\n",
    "    data_train[i] = arff.loadarff(os.path.join('Datasets_raw', \n",
    "                                               'SelfRegulationSCP1',\n",
    "                                               f'SelfRegulationSCP1Dimension{i}_TRAIN.arff'))[0]\n",
    "    data_test[i] = arff.loadarff(os.path.join('Datasets_raw', \n",
    "                                               'SelfRegulationSCP1',\n",
    "                                               f'SelfRegulationSCP1Dimension{i}_TEST.arff'))[0]\n",
    "    \n",
    "#Convert to pandas dataframes\n",
    "df_train = timeseries_to_pandas(data_train)\n",
    "df_test = timeseries_to_pandas(data_test)\n",
    "\n",
    "#Convert to numpy arrays\n",
    "X_train, y_train = pandas_to_numpy(df_train)\n",
    "X_test, y_test = pandas_to_numpy(df_test)\n",
    "\n",
    "#Create folder structure and save the arrays\n",
    "if not os.path.exists(os.path.join('Datasets_clean', 'SelfRegulationSCP1')):\n",
    "    os.makedirs(os.path.join('Datasets_clean', 'SelfRegulationSCP1'))\n",
    "\n",
    "np.save('Datasets_clean/SelfRegulationSCP1/X_train', X_train)\n",
    "np.save('Datasets_clean/SelfRegulationSCP1/y_train', y_train)\n",
    "np.save('Datasets_clean/SelfRegulationSCP1/X_test', X_test)\n",
    "np.save('Datasets_clean/SelfRegulationSCP1/y_test', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T13:33:51.286398Z",
     "start_time": "2020-11-26T13:32:28.757522Z"
    }
   },
   "outputs": [],
   "source": [
    "#Storing variables\n",
    "full_array = np.empty((0,33,8064))\n",
    "full_valence = np.empty(0)\n",
    "full_arousal = np.empty(0)\n",
    "full_liking = np.empty(0)\n",
    "\n",
    "#Read data\n",
    "for file in os.listdir(\"./Datasets_raw/DEAP/data_preprocessed_python\"):\n",
    "    if file.endswith(\".dat\"):\n",
    "        data = pickle.load(open('Datasets_raw/DEAP/data_preprocessed_python/{}'.format(file), 'rb'), encoding='latin1')\n",
    "        \n",
    "        #EEG timeseries\n",
    "        all_timeseries = data['data'].copy()\n",
    "        eeg_timeseries = all_timeseries[:,0:33,:]\n",
    "        full_array = np.vstack((full_array,eeg_timeseries))\n",
    "        \n",
    "        #Response variables\n",
    "        valence = data['labels'][:,0]\n",
    "        valence = np.digitize(valence,np.array([0,5]))\n",
    "        full_valence = np.hstack((full_valence,valence))\n",
    "\n",
    "        arousal = data['labels'][:,1]\n",
    "        arousal = np.digitize(arousal,np.array([0,5]))\n",
    "        full_arousal = np.hstack((full_arousal,arousal))\n",
    "        \n",
    "        liking = data['labels'][:,3]\n",
    "        liking = np.digitize(liking,np.array([0,5]))\n",
    "        full_liking = np.hstack((full_liking,valence))\n",
    "\n",
    "#0-1 response variables\n",
    "full_valence[full_valence == 1] = 0\n",
    "full_valence[full_valence == 2] = 1\n",
    "full_arousal[full_arousal == 1] = 0\n",
    "full_arousal[full_arousal == 2] = 1\n",
    "full_liking[full_liking == 1] = 0\n",
    "full_liking[full_liking == 2] = 1\n",
    "\n",
    "#Saving the numpy files\n",
    "X_train = full_array[0:960].copy()\n",
    "X_test = full_array[960:].copy()\n",
    "\n",
    "#Create folder structure and save the arrays\n",
    "if not os.path.exists(os.path.join('Datasets_clean', 'DEAP_Valence')):\n",
    "    os.makedirs(os.path.join('Datasets_clean', 'DEAP_Valence'))\n",
    "if not os.path.exists(os.path.join('Datasets_clean', 'DEAP_Arousal')):\n",
    "    os.makedirs(os.path.join('Datasets_clean', 'DEAP_Arousal'))\n",
    "if not os.path.exists(os.path.join('Datasets_clean', 'DEAP_Liking')):\n",
    "    os.makedirs(os.path.join('Datasets_clean', 'DEAP_Liking'))\n",
    "\n",
    "np.save('Datasets_clean/DEAP_Valence/X_train.npy', X_train)\n",
    "np.save('Datasets_clean/DEAP_Arousal/X_train.npy', X_train)\n",
    "np.save('Datasets_clean/DEAP_Liking/X_train.npy', X_train)\n",
    "\n",
    "np.save('Datasets_clean/DEAP_Valence/X_test.npy', X_test)\n",
    "np.save('Datasets_clean/DEAP_Arousal/X_test.npy', X_test)\n",
    "np.save('Datasets_clean/DEAP_Liking/X_test.npy', X_test)\n",
    "\n",
    "np.save('Datasets_clean/DEAP_Valence/y_train.npy', full_valence[0:960])\n",
    "np.save('Datasets_clean/DEAP_Arousal/y_train.npy', full_arousal[0:960])\n",
    "np.save('Datasets_clean/DEAP_Liking/y_train.npy', full_liking[0:960])\n",
    "\n",
    "np.save('Datasets_clean/DEAP_Valence/y_test.npy', full_valence[960:])\n",
    "np.save('Datasets_clean/DEAP_Arousal/y_test.npy', full_arousal[960:])\n",
    "np.save('Datasets_clean/DEAP_Liking/y_test.npy', full_liking[960:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_2",
   "language": "python",
   "name": "tf_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
