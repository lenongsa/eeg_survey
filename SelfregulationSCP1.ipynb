{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SelfregulationSCP1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T20:24:07.583915Z",
     "start_time": "2020-11-04T20:24:04.782721Z"
    }
   },
   "outputs": [],
   "source": [
    "###libraries\n",
    "\n",
    "#Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#Time series transformers\n",
    "from pyts.multivariate.classification import MultivariateClassifier\n",
    "from pyts.multivariate.transformation import WEASELMUSE\n",
    "from pyts.classification import KNeighborsClassifier\n",
    "from pywt import wavedec\n",
    "import pyeeg\n",
    "\n",
    "#Classifiers\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "#Deep Learning\n",
    "import mcfly\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "\n",
    "#Auxiliary Functions\n",
    "from aux_functions import ResampleLinear1D\n",
    "from aux_functions import get_features\n",
    "\n",
    "#System Settings\n",
    "import sys\n",
    "import os\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T20:24:07.614430Z",
     "start_time": "2020-11-04T20:24:07.586158Z"
    }
   },
   "outputs": [],
   "source": [
    "###Read the data\n",
    "dataset = 'SelfRegulationSCP1'\n",
    "\n",
    "#Train and testing datasets\n",
    "X_train = np.load('Datasets_clean/{}/X_train.npy'.format(dataset))\n",
    "y_train = np.load('Datasets_clean/{}/y_train.npy'.format(dataset))\n",
    "X_test = np.load('Datasets_clean/{}/X_test.npy'.format(dataset))\n",
    "y_test = np.load('Datasets_clean/{}/y_test.npy'.format(dataset))\n",
    "\n",
    "X_full = np.vstack([X_train,X_test])\n",
    "y_full = np.hstack([y_train,y_test])\n",
    "\n",
    "#Optimal Hyperparameters\n",
    "with open('hyperparameters.json') as json_file:\n",
    "    hyperparameters = json.load(json_file)[dataset]\n",
    "    \n",
    "#Scores dictionary and stratified k-fold\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "scores = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DTW + 1-Knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T23:07:07.066761Z",
     "start_time": "2020-11-04T20:24:07.630165Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10343969a82f4b15a8d23e451b2fe8e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dtw_acc = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "for train_index, test_index in tqdm(skf.split(X_full, y_full)):\n",
    "    \n",
    "    X_train = X_full[train_index]\n",
    "    y_train = y_full[train_index]\n",
    "    X_test = X_full[test_index]\n",
    "    y_test = y_full[test_index]\n",
    "    \n",
    "    X_train = np.apply_along_axis(ResampleLinear1D, axis = 2, arr = X_train)\n",
    "    X_test = np.apply_along_axis(ResampleLinear1D, axis = 2, arr = X_test)\n",
    "    \n",
    "    dtw_knn = MultivariateClassifier(KNeighborsClassifier(metric = 'dtw_itakura',\n",
    "                                                      n_jobs = -1,\n",
    "                                                      metric_params = {'max_slope' : 2}))\n",
    "    \n",
    "    dtw_knn.fit(X_train,y_train)\n",
    "    acc = dtw_knn.score(X_test,y_test)\n",
    "    dtw_acc.append(acc)\n",
    "    \n",
    "scores['dtw_knn'] = [np.mean(dtw_acc), np.std(dtw_acc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEASELMUSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T23:09:24.246558Z",
     "start_time": "2020-11-04T23:07:07.069827Z"
    }
   },
   "outputs": [],
   "source": [
    "#Accuracies\n",
    "accuracies_lr = []\n",
    "accuracies_rf = []\n",
    "accuracies_svc = []\n",
    "accuracies_lgbm = []\n",
    "\n",
    "#Train and evaluate the models over the stratified splits\n",
    "for train_index, test_index in skf.split(X_full, y_full):\n",
    "    \n",
    "    X_train = X_full[train_index]\n",
    "    y_train = y_full[train_index]\n",
    "    X_test = X_full[test_index]\n",
    "    y_test = y_full[test_index]\n",
    "\n",
    "    #wm + lr\n",
    "    wm = WEASELMUSE(word_size = hyperparameters['wm_lr']['word_size'], \n",
    "                    n_bins = hyperparameters['wm_lr']['n_bins'])\n",
    "\n",
    "    logistic = LogisticRegression(solver = 'lbfgs',\n",
    "                                  multi_class = 'auto',\n",
    "                                  max_iter=3000,\n",
    "                                  C = hyperparameters['wm_lr']['C'])\n",
    "\n",
    "    clf = make_pipeline(wm, logistic)\n",
    "    clf.fit(X_train,y_train)\n",
    "    acc = clf.score(X_test,y_test)\n",
    "    accuracies_lr.append(np.round(acc,2))\n",
    "    \n",
    "    #wm + rf\n",
    "    wm = WEASELMUSE(word_size = hyperparameters['wm_rf']['word_size'], \n",
    "                    n_bins = hyperparameters['wm_rf']['n_bins'])\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators = hyperparameters['wm_rf']['n_estimators'],\n",
    "                                max_depth = hyperparameters['wm_rf']['max_depth'])\n",
    "\n",
    "    clf = make_pipeline(wm, rf)\n",
    "    clf.fit(X_train,y_train)\n",
    "    acc = clf.score(X_test,y_test)\n",
    "    accuracies_rf.append(np.round(acc,2))\n",
    "\n",
    "    #wm + svc\n",
    "    wm = WEASELMUSE(word_size = hyperparameters['wm_svc']['word_size'], \n",
    "                    n_bins = hyperparameters['wm_svc']['n_bins'])\n",
    "\n",
    "    svc = SVC(C = hyperparameters['wm_svc']['C'],\n",
    "              kernel = hyperparameters['wm_svc']['kernel'],\n",
    "              degree = hyperparameters['wm_svc']['degree'],\n",
    "              gamma = hyperparameters['wm_svc']['gamma'])\n",
    "\n",
    "    clf = make_pipeline(wm, svc)\n",
    "    clf.fit(X_train,y_train)\n",
    "    acc = clf.score(X_test,y_test)\n",
    "    accuracies_svc.append(np.round(acc,2))\n",
    "\n",
    "    #wm + lgbm\n",
    "    wm = WEASELMUSE(word_size = hyperparameters['wm_svc']['word_size'], \n",
    "                    n_bins = hyperparameters['wm_svc']['n_bins'])\n",
    "\n",
    "    def sparse_float(mat):\n",
    "        return mat.astype('float')\n",
    "    trans_sparse_float = FunctionTransformer(sparse_float, validate = False)\n",
    "\n",
    "    lgbm = LGBMClassifier(n_jobs = -1,\n",
    "                          num_leaves = hyperparameters['wm_lgbm']['num_leaves'],\n",
    "                          max_depth = hyperparameters['wm_lgbm']['max_depth'],\n",
    "                          learning_rate = hyperparameters['wm_lgbm']['learning_rate'],\n",
    "                          n_estimators = hyperparameters['wm_lgbm']['n_estimators'],\n",
    "                          min_split_gain = hyperparameters['wm_lgbm']['min_split_gain'],\n",
    "                          min_child_samples = hyperparameters['wm_lgbm']['min_child_samples'],\n",
    "                          colsample_by_tree = hyperparameters['wm_lgbm']['colsample_bytree'],\n",
    "                          reg_alpha = hyperparameters['wm_lgbm']['reg_alpha'],\n",
    "                          reg_lambda = hyperparameters['wm_lgbm']['reg_lambda'])\n",
    "\n",
    "    clf = make_pipeline(wm, trans_sparse_float, lgbm)\n",
    "    clf.fit(X_train,y_train)\n",
    "    acc = clf.score(X_test,y_test)\n",
    "    accuracies_lgbm.append(np.round(acc,2))\n",
    "    \n",
    "#Save the mean and std of accuracies of the models over the splits\n",
    "scores['wm_lr'] = [np.mean(accuracies_lr), np.std(accuracies_lr)]\n",
    "scores['wm_rf'] = [np.mean(accuracies_rf), np.std(accuracies_rf)]\n",
    "scores['wm_svc'] = [np.mean(accuracies_svc), np.std(accuracies_svc)]\n",
    "scores['wm_lgbm'] = [np.mean(accuracies_lgbm), np.std(accuracies_lgbm)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T23:09:24.259356Z",
     "start_time": "2020-11-04T23:09:24.248839Z"
    }
   },
   "outputs": [],
   "source": [
    "#Function to train the Neural Networks\n",
    "def dl_func(dl_type,\n",
    "            X_train_dl, \n",
    "            y_train_dl,\n",
    "            X_val_dl,\n",
    "            y_val_dl,\n",
    "            X_test_dl,\n",
    "            y_test_dl):\n",
    "    \n",
    "    #Validate diferent architectures\n",
    "    num_of_candidate_models = 1#8\n",
    "    random_search_epoches = 1#100\n",
    "    random_search_es = 1#30\n",
    "    best_model_epoches = 200\n",
    "    best_model_es = 30\n",
    "    \n",
    "    for mod_type in [dl_type]:\n",
    "\n",
    "        #Create architectures\n",
    "        num_classes = y_train_dl.shape[1]\n",
    "        metric = 'accuracy'\n",
    "        models = mcfly.modelgen.generate_models(X_train_dl.shape,\n",
    "                                                number_of_classes=num_classes,\n",
    "                                                number_of_models = num_of_candidate_models,\n",
    "                                                model_types = [mod_type])\n",
    "\n",
    "        #Save intermediate results\n",
    "        resultpath = 'temp'\n",
    "        outputfile = os.path.join(resultpath, 'modelcomparison_{}_{}.json'.format(mod_type,dataset))\n",
    "\n",
    "        #Find best architecture\n",
    "        histories, val_accuracies, val_losses = mcfly.find_architecture.train_models_on_samples(X_train_dl, y_train_dl,\n",
    "                                                                                                X_val_dl, y_val_dl,\n",
    "                                                                                                models,\n",
    "                                                                                                nr_epochs=random_search_epoches,\n",
    "                                                                                                subset_size=None,\n",
    "                                                                                                verbose=False,\n",
    "                                                                                                outputfile=outputfile,\n",
    "                                                                                                early_stopping_patience=random_search_es\n",
    "                                                                                                )\n",
    "\n",
    "        #Select and train best architecture\n",
    "        best_model_index = np.argmax(val_accuracies)\n",
    "        best_model, best_params, best_model_types = models[best_model_index]\n",
    "\n",
    "        es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=best_model_es)\n",
    "        mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "        history = best_model.fit(X_train_dl, y_train_dl,\n",
    "                                 epochs=best_model_epoches, validation_data=(X_val_dl, y_val_dl),\n",
    "                                 callbacks = [es,mc])\n",
    "\n",
    "        #Accuracy in test dataset\n",
    "        saved_model = load_model('best_model.h5')\n",
    "        return saved_model.evaluate(X_test_dl, y_test_dl, verbose = False)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T20:36:03.071656Z",
     "start_time": "2020-11-04T23:09:24.260996Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #Train CNN and InceptionTime architectures\n",
    "# accuracies_cnn = []\n",
    "accuracies_it = []\n",
    "\n",
    "for train_index, test_index in skf.split(X_full, y_full):\n",
    "    \n",
    "    X_train = X_full[train_index]\n",
    "    y_train = y_full[train_index]\n",
    "    X_test = X_full[test_index]\n",
    "    y_test = y_full[test_index]\n",
    "\n",
    "    #Create train, validation and test sets for DL\n",
    "    LE = LabelEncoder()\n",
    "    skf_train_val = StratifiedKFold(n_splits=5)\n",
    "\n",
    "    for train_index, val_index in skf_train_val.split(X_train, y_train):\n",
    "        train_index = train_index\n",
    "        val_index = val_index\n",
    "\n",
    "    X_train_val = X_train.copy()\n",
    "\n",
    "    X_train_dl = X_train_val[train_index].copy()\n",
    "    X_train_dl = np.transpose(X_train_dl, (0,2,1))\n",
    "    y_train_dl = y_train[train_index].copy()\n",
    "    y_train_dl = to_categorical(LE.fit_transform(y_train_dl))\n",
    "\n",
    "    X_val_dl = X_train_val[val_index].copy()\n",
    "    X_val_dl = np.transpose(X_val_dl, (0,2,1))\n",
    "    y_val_dl = y_train[val_index].copy()\n",
    "    y_val_dl = to_categorical(LE.transform(y_val_dl))\n",
    "\n",
    "    X_test_dl = np.transpose(X_test, (0,2,1))\n",
    "    y_test_dl = to_categorical(LE.transform(y_test))\n",
    "    \n",
    "    #Define the networks\n",
    "    acc_cnn = dl_func('CNN',\n",
    "                     X_train_dl, \n",
    "                     y_train_dl,\n",
    "                     X_val_dl,\n",
    "                     y_val_dl,\n",
    "                     X_test_dl,\n",
    "                     y_test_dl)\n",
    "    \n",
    "    acc_it = dl_func('InceptionTime',\n",
    "                     X_train_dl, \n",
    "                     y_train_dl,\n",
    "                     X_val_dl,\n",
    "                     y_val_dl,\n",
    "                     X_test_dl,\n",
    "                     y_test_dl)\n",
    "\n",
    "    accuracies_cnn.append(acc_cnn)\n",
    "    accuracies_it.append(acc_it)\n",
    "\n",
    "#Save the mean and std of accuracies of the models over the splits\n",
    "scores['dl_cnn'] = [np.mean(accuracies_cnn), np.std(accuracies_cnn)]\n",
    "scores['dl_it'] = [np.mean(accuracies_it), np.std(accuracies_it)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyEEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T20:44:37.673600Z",
     "start_time": "2020-11-05T20:36:03.078530Z"
    }
   },
   "outputs": [],
   "source": [
    "#Accuracies\n",
    "pyeeg_lr_acc = []\n",
    "pyeeg_rf_acc = []\n",
    "pyeeg_svc_acc = []\n",
    "pyeeg_lgbm_acc = []\n",
    "\n",
    "#Train and evaluate the models over the stratified splits\n",
    "for train_index, test_index in skf.split(X_full, y_full):\n",
    "    \n",
    "    X_train = X_full[train_index]\n",
    "    y_train = y_full[train_index]\n",
    "    X_test = X_full[test_index]\n",
    "    y_test = y_full[test_index]\n",
    "    \n",
    "    #Create features out of the eeg channels\n",
    "    feat_dict = {}\n",
    "\n",
    "    for df,name in zip([X_train,X_test],['X_train_pyeeg','X_test_pyeeg']):\n",
    "        feat_dict[name] = np.array([]).reshape(-1,438)\n",
    "\n",
    "        for ind in range(0,df.shape[0]):\n",
    "            features = np.array([])\n",
    "\n",
    "            for channel in range(0,df[ind].shape[0]):\n",
    "                eeg_series = df[ind][channel]\n",
    "                \n",
    "                #Wavelet Coefficients features\n",
    "                coefs = wavedec(eeg_series, 'db4', level=4)\n",
    "                for band in coefs:\n",
    "                    features = np.hstack([features,get_features(band)])\n",
    "                \n",
    "                #Fourier Transform features\n",
    "                fft_power = pyeeg.bin_power(eeg_series, Band = [0.5,4,7,12,30,100], Fs = 256)\n",
    "                power_spectral_intensity = fft_power[0]\n",
    "                relative_intensity_ratio = fft_power[1]\n",
    "                spectral_entropy = pyeeg.spectral_entropy(eeg_series, Band = [0.5,4,7,12,30,100], Fs = 256,\n",
    "                                                          Power_Ratio = relative_intensity_ratio)\n",
    "\n",
    "                #Hjorth Parameters features\n",
    "                hjorth = pyeeg.hjorth(eeg_series)\n",
    "                hjorth_mob = hjorth[0]\n",
    "                hjorth_comp = hjorth[1]\n",
    "\n",
    "                features = np.hstack([features, power_spectral_intensity, relative_intensity_ratio,\n",
    "                                      hjorth_mob, hjorth_comp, spectral_entropy])\n",
    "\n",
    "\n",
    "            feat_dict[name] = np.vstack([feat_dict[name],features])\n",
    "\n",
    "    #Convert dictionary of features into numpy arrays\n",
    "    X_train_pyeeg = feat_dict['X_train_pyeeg'].copy()\n",
    "    X_test_pyeeg = feat_dict['X_test_pyeeg'].copy()\n",
    "    \n",
    "    X_train_pyeeg = np.nan_to_num(X_train_pyeeg)\n",
    "    X_test_pyeeg = np.nan_to_num(X_test_pyeeg)\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train_pyeeg)\n",
    "    X_train_pyeeg = scaler.transform(X_train_pyeeg)\n",
    "    X_test_pyeeg = scaler.transform(X_test_pyeeg)\n",
    "    \n",
    "    #pyeeg + lr\n",
    "    logistic = LogisticRegression(solver = 'lbfgs',\n",
    "                                  multi_class = 'auto',\n",
    "                                  max_iter=3000,\n",
    "                                  C = hyperparameters['pyeeg_lr']['C'])\n",
    "\n",
    "    logistic.fit(X_train_pyeeg,y_train)\n",
    "    acc = logistic.score(X_test_pyeeg,y_test)\n",
    "    pyeeg_lr_acc.append(np.round(acc,2).copy())\n",
    "    \n",
    "    #pyeeg + rf\n",
    "    rf = RandomForestClassifier(n_estimators = hyperparameters['pyeeg_rf']['n_estimators'],\n",
    "                                max_depth = hyperparameters['pyeeg_rf']['max_depth'])\n",
    "\n",
    "    rf.fit(X_train_pyeeg,y_train)\n",
    "    acc = rf.score(X_test_pyeeg,y_test)\n",
    "    pyeeg_rf_acc.append(np.round(acc,2).copy())\n",
    "    \n",
    "    #pyeeg + svc\n",
    "    svc = SVC(C = hyperparameters['pyeeg_svc']['C'],\n",
    "              kernel = hyperparameters['pyeeg_svc']['kernel'],\n",
    "              degree = hyperparameters['pyeeg_svc']['degree'],\n",
    "              gamma = hyperparameters['pyeeg_svc']['gamma'])\n",
    "\n",
    "    clf = make_pipeline(svc)\n",
    "    clf.fit(X_train_pyeeg,y_train)\n",
    "    acc = clf.score(X_test_pyeeg,y_test)\n",
    "    pyeeg_svc_acc.append(np.round(acc,2).copy())\n",
    "\n",
    "    \n",
    "    #pyeeg + lgbm\n",
    "    def sparse_float(mat):\n",
    "        return mat.astype('float')\n",
    "    trans_sparse_float = FunctionTransformer(sparse_float, validate = False)\n",
    "\n",
    "    lgbm = LGBMClassifier(n_jobs = -1,\n",
    "                          num_leaves = hyperparameters['pyeeg_lgbm']['num_leaves'],\n",
    "                          max_depth = hyperparameters['pyeeg_lgbm']['max_depth'],\n",
    "                          learning_rate = hyperparameters['pyeeg_lgbm']['learning_rate'],\n",
    "                          n_estimators = hyperparameters['pyeeg_lgbm']['n_estimators'],\n",
    "                          min_split_gain = hyperparameters['pyeeg_lgbm']['min_split_gain'],\n",
    "                          min_child_samples = hyperparameters['pyeeg_lgbm']['min_child_samples'],\n",
    "                          colsample_by_tree = hyperparameters['pyeeg_lgbm']['colsample_bytree'],\n",
    "                          reg_alpha = hyperparameters['pyeeg_lgbm']['reg_alpha'],\n",
    "                          reg_lambda = hyperparameters['pyeeg_lgbm']['reg_lambda'])\n",
    "\n",
    "    clf = make_pipeline(trans_sparse_float, lgbm)\n",
    "    clf.fit(X_train_pyeeg,y_train)\n",
    "    acc = clf.score(X_test_pyeeg,y_test)\n",
    "    pyeeg_lgbm_acc.append(np.round(acc,2).copy())\n",
    "\n",
    "#Save the mean and std of accuracies of the models over the splits\n",
    "scores['pyeeg_lr'] = [np.mean(pyeeg_lr_acc), np.std(pyeeg_lr_acc)]\n",
    "scores['pyeeg_rf'] = [np.mean(pyeeg_rf_acc), np.std(pyeeg_rf_acc)]\n",
    "scores['pyeeg_svc'] = [np.mean(pyeeg_svc_acc), np.std(pyeeg_svc_acc)]\n",
    "scores['pyeeg_lgbm'] = [np.mean(pyeeg_lgbm_acc), np.std(pyeeg_lgbm_acc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-04T20:23:25.834Z"
    }
   },
   "outputs": [],
   "source": [
    "#Salva scores\n",
    "individual_results = pd.DataFrame({'dtw_acc' : dtw_acc,\n",
    "                                   'wm_lr_acc' : accuracies_lr,\n",
    "                                   'wm_rf_acc' : accuracies_rf,\n",
    "                                   'wm_svc_acc' : accuracies_svc,\n",
    "                                   'wm_lgbm_acc' : accuracies_lgbm,\n",
    "                                   'cnn_acc' : accuracies_cnn,\n",
    "                                   'it_acc' : accuracies_it,\n",
    "                                   'pyeeg_lr_acc' : pyeeg_lr_acc,\n",
    "                                   'pyeeg_rf_acc' : pyeeg_rf_acc,\n",
    "                                   'pyeeg_svc_acc' : pyeeg_svc_acc,\n",
    "                                   'pyeeg_lgbm_acc' : pyeeg_lgbm_acc})\n",
    "\n",
    "individual_results.to_csv('results/Individual_Scores_{}.csv'.format(dataset), index = False)\n",
    "\n",
    "results = pd.DataFrame(scores).T.reset_index().rename(columns={0 : 'mean',\n",
    "                                                               1 : 'std',\n",
    "                                                               'index' : 'method'}).sort_values('mean', ascending = False)\n",
    "\n",
    "results.to_csv('results/Scores_{}.csv'.format(dataset), index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeg_survey",
   "language": "python",
   "name": "eeg_survey"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
